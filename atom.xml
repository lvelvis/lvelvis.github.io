<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://lvelvis.github.io</id>
    <title>lvelvis</title>
    <updated>2020-10-27T04:18:28.348Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="http://lvelvis.github.io"/>
    <link rel="self" href="http://lvelvis.github.io/atom.xml"/>
    <subtitle>时光,浓淡相宜;人心,远近相安;这就是最好的生活</subtitle>
    <logo>http://lvelvis.github.io/images/avatar.png</logo>
    <icon>http://lvelvis.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, lvelvis</rights>
    <entry>
        <title type="html"><![CDATA[clickhouse基本操作]]></title>
        <id>http://lvelvis.github.io/post/clickhouse-ji-ben-cao-zuo/</id>
        <link href="http://lvelvis.github.io/post/clickhouse-ji-ben-cao-zuo/">
        </link>
        <updated>2020-10-27T04:09:31.000Z</updated>
        <content type="html"><![CDATA[<h2 id="一-先来说一下clickhouse为啥快">一、先来说一下，ClickHouse为啥快</h2>
<pre><code>ClickHouse有多少CPU，吃多少资源，所以飞快；
ClickHouse不支持事务，不存在隔离级别。这里要额外说一下，有人觉得，你一个数据库都不支持事务，不支持ACID还玩个毛。ClickHouse的定位是分析性数据库，而不是严格的关系型数据库。又有人要问了，数据都不一致，统计个毛。举个例子，汽车的油表是100%准确么？为了获得一个100%准确的值，难道每次测量你都要停车检查么？统计数据的意义在于用大量的数据看规律，看趋势，而不是100%准确。
IO方面，MySQL是行存储，ClickHouse是列存储，后者在count()这类操作天然有优势，同时，在IO方面，MySQL需要大量随机IO，ClickHouse基本是顺序IO。
有人可能觉得上面的数据导入的时候，数据肯定缓存在内存里了，这个的确，但是ClickHouse基本上是顺序IO，用过就知道了，对IO基本没有太高要求，当然，磁盘越快，上层处理越快，但是99%的情况是，CPU先跑满了（数据库里太少见了，大多数都是IO不够用）
</code></pre>
<h2 id="二-创建库">二、创建库</h2>
<p>CREATE/ATTACH DATABASE zabbix ENGINE = Ordinary;<br>
ATTACH 也可以建库，但是metadata目录下不会生成.sql文件，一般用于metadata元数据sql文件被删除后，恢复库表结构使用</p>
<h2 id="三-创建本地表">三、创建本地表</h2>
<p>CREATE TABLE test02( id UInt16,col1 String,col2 String,create_date date ) ENGINE = MergeTree(create_date, (id), 8192);<br>
ENGINE：是表的引擎类型，<br>
MergeTree：最常用的，MergeTree要求有一个日期字段，还有主键。<br>
Log引擎没有这个限制，也是比较常用。<br>
ReplicatedMergeTree：MergeTree的分支，表复制引擎。<br>
Distributed：分布式引擎。<br>
create_date：是表的日期字段，一个表必须要有一个日期字段。<br>
id：是表的主键，主键可以有多个字段，每个字段用逗号分隔。<br>
8192：是索引粒度，用默认值8192即可。</p>
<h2 id="四-创建分布式表">四、创建分布式表</h2>
<p>CREATE TABLE distributed_table AS table ENGINE = Distributed(cluster, db, table, rand());<br>
cluster：配置文件中的群集名称。<br>
db：库名。<br>
table：本地表名。<br>
rand()：分片方式：随机。<br>
intHash64():分片方式：指定字段做hash。<br>
Distribute引擎会选择每个分发到的Shard中的”健康的”副本执行SQL<br>
<img src="http://lvelvis.github.io/post-images/1603771933311.jpg" alt="" loading="lazy"></p>
<h2 id="五-ddl">五、DDL</h2>
<p>如果想按集群操作，需要借助zookeeper，在config.xml中添加配置<br>
&lt;distributed_ddl&gt;<br>
<path>/clickhouse/task_queue/ddl</path><br>
&lt;/distributed_ddl&gt;<br>
一个节点创建表，会同步到各个节点<br>
CREATE TABLE db.table [ON CLUSTER cluster] (...)<br>
添加、删除、修改列<br>
ALTER TABLE [db].table [ON CLUSTER cluster] ADD|DROP|MODIFY COLUMN ...<br>
rename 支持*MergeTree和Distributed<br>
rename table db.table1 to db.table2 [ON CLUSTER cluster]<br>
truncate table db.table;不支持Distributed引擎</p>
<h2 id="六-deleteupdate-不支持distributed引擎">六、delete/update 不支持Distributed引擎</h2>
<p>ALTER TABLE [db.]table DELETE WHERE filter_expr...<br>
ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE ...</p>
<h2 id="七-分区表">七、分区表</h2>
<p>按时间分区：<br>
toYYYYMM(EventDate)：按月分区<br>
toMonday(EventDate)：按周分区<br>
toDate(EventDate)：按天分区<br>
按指定列分区：<br>
PARTITION BY cloumn_name<br>
对分区的操作：<br>
alter table test1 DROP PARTITION [partition] #删除分区<br>
alter table test1 DETACH PARTITION [partition]#下线分区<br>
alter table test1 ATTACH PARTITION [partition]#恢复分区<br>
alter table .test1 FREEZE PARTITION [partition]#备份分区</p>
<h2 id="八-数据同步">八、数据同步</h2>
<ol>
<li>采用remote函数<br>
insert into db.table select * from remote('目标IP',db.table,'user','passwd')</li>
<li>csv文件导入clickhouse<br>
cat test.csv | clickhouse-client -u user --password password --query=&quot;INSERT INTO db.table FORMAT CSV&quot;</li>
<li>同步mysql库中表<br>
CREATE TABLE tmp ENGINE = MergeTree ORDER BY id AS SELECT * FROM mysql('hostip:3306', 'db', 'table', 'user', 'passwd') ;<br>
4） clickhouse-copier 工具</li>
</ol>
<h2 id="九-时间戳转换">九、时间戳转换</h2>
<p>select toUnixTimestamp('2018-11-25 00:00:02');<br>
select toDateTime(1543075202);</p>
<h2 id="十-其他事项">十、其他事项</h2>
<ol>
<li>clickhouse的cluster环境中，每台server的地位是等价的，即不存在master-slave之说，是multi-master模式。</li>
<li>各replicated表的宿主server上要在hosts里配置其他replicated表宿主server的ip和hostname的映射。</li>
<li>上面描述的在不同的server上建立全新的replicated模式的表，如果在某台server上已经存在一张replicated表，并且表中已经有数据，这时在另外的server上执行完replicated建表语句后，已有数据会自动同步到其他server上面。</li>
<li>如果zookeeper挂掉，replicated表会切换成read-only模式，不再进行数据同步，系统会周期性的尝试与zk重新建立连接。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka消费命令]]></title>
        <id>http://lvelvis.github.io/post/kafka-xiao-fei-ming-ling/</id>
        <link href="http://lvelvis.github.io/post/kafka-xiao-fei-ming-ling/">
        </link>
        <updated>2020-09-28T08:12:09.000Z</updated>
        <content type="html"><![CDATA[<p>模拟生产消息：</p>
<pre><code>bin/kafka-console-producer.sh --broker-list 192.168.101.100:9092 --topic flink-test
</code></pre>
<p>模拟消费数据(从开始位置消费)</p>
<pre><code>bin/kafka-console-consumer.sh --bootstrap-server  192.168.101.100:9092  --topic flink-test --from-beginning  |head
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[logstash output file to HDFS]]></title>
        <id>http://lvelvis.github.io/post/logstash-output-file-to-hdfs/</id>
        <link href="http://lvelvis.github.io/post/logstash-output-file-to-hdfs/">
        </link>
        <updated>2020-09-04T02:45:33.000Z</updated>
        <content type="html"><![CDATA[<p>logstash 直接把文件内容写入 hdfs 中， 并支持 hdfs 压缩格式。<br>
logstash 需要安装第三方插件，webhdfs插件，通过hdfs的web接口写入。<br>
即 http://namenode00:50070/webhdfs/v1/ 接口</p>
<p>新版本logstash已默认安装webhdfs插件<br>
官网地址及使用说明：<br>
https://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html<br>
检查hdfs的webhds接口</p>
<pre><code>curl -i  &quot;http://namenode:50070/webhdfs/v1/?user.name=hadoop&amp;op=LISTSTATUS&quot;   
HTTP/1.1 200 OK
Cache-Control: no-cache
Expires: Thu, 13 Jul 2017 04:53:39 GMT
Date: Thu, 13 Jul 2017 04:53:39 GMT
Pragma: no-cache
Expires: Thu, 13 Jul 2017 04:53:39 GMT
Date: Thu, 13 Jul 2017 04:53:39 GMT
Pragma: no-cache
Content-Type: application/json
Set-Cookie: hadoop.auth=&quot;u=hadoop&amp;p=hadoop&amp;t=simple&amp;e=1499957619679&amp;s=KSxdSAtjXAllhn73vh1MAurG9Bk=&quot;; Path=/; Expires=Thu, 13-Jul-2017 14:53:39 GMT; HttpOnly
Transfer-Encoding: chunked
Server: Jetty(6.1.26)
注释： active namenode 返回是200 ，standby namenode 返回是403.
</code></pre>
<p>测试hdfs是否正常通讯：</p>
<pre><code>#通过webhdfs接口创建test.conf
curl -i -X PUT &quot;http://hadoop-master:50070/webhdfs/v1/data/test.conf?user.name=hdfs&amp;op=CREATE&quot;
curl -i -T test.conf &quot;http://hadoop-slave1:50075/webhdfs/v1/data/test.conf?op=CREATE&amp;user.name=hdfs&amp;namenoderpcaddress=hadoop-master:9000&amp;overwrite=false&quot;
</code></pre>
<p>配置<br>
添加 logstash 一个配置文件</p>
<p>vim /home/mtime/logstash-2.3.1/conf/hdfs.conf</p>
<pre><code>input {
    kafka {
        bootstrap_servers =&gt; &quot;192.168.101.22:9092,192.168.101.23:9092,192.168.101.93:9092&quot;
        topics =&gt; &quot;test-logs&quot;
        group_id =&gt; &quot;hdfs-test-logs&quot;
        codec =&gt; json
	    consumer_threads =&gt; 15

    }
}

filter {
    date {
        match =&gt; [&quot;time&quot;,&quot;yyyy-MM-dd HH:mm:ss Z&quot;]
        target =&gt; &quot;@timestamp&quot;
        timezone =&gt; &quot;Asia/Shanghai&quot;
    }
    ruby {
        code =&gt; &quot;event.set('index.date', event.get('@timestamp').time.localtime.strftime('%Y%m%d'))&quot;
    }
    ruby {
        code =&gt; &quot;event.set('index.hour', event.get('@timestamp').time.localtime.strftime('%H'))&quot;
    }
}
output {            
    webhdfs {
           host =&gt; &quot;hadoop-master&quot;
           port =&gt; 50070
           path =&gt; &quot;/data/pt-collect-log/test-logs/%{index.date}/application-%{index.hour}.log&quot;
           user =&gt; &quot;hdfs&quot;
	   codec =&gt; line { format =&gt; &quot;%{message}&quot;}
           flush_size =&gt; 1000
           compression =&gt; &quot;gzip&quot;            
           idle_flush_time =&gt; 10
           retry_interval =&gt; 3
	   retry_times =&gt; 100
       }
}
</code></pre>
<p>关于hdfs部分配置，可以在 plugins-outputs-webhdfs 官网找到<br>
启动 logstart<br>
cd /home/mtime/logstash-2.3.1/bin/<br>
./logstash -f ../conf/hdfs.conf    # 为前台启动<br>
报错处理</p>
<pre><code>[WARN ][logstash.outputs.webhdfs ] Failed to flush outgoing items {:outgoing_count=&gt;160, :exception=&gt;&quot;WebHDFS::IOError&quot;,
我将hdfs端口 由原来的50070 改为 14000 端口，就在不报错了。
官方提供的例子中用的就是50070端口，一直没有尝试14000端口。

还有：
because this file lease is currently owned by DFSClient
hadoop 租约问题，后期正常就没有了。
执行recoverLease来释放文件的锁

$ hdfs debug recoverLease -path /logstash/2017/02/10/go-03.log
还有：
:message=&gt;&quot;webhdfs write caused an exception: {\&quot;RemoteException\&quot;:{\&quot;message\&quot;:\&quot;Failed to APPEND_FILE
当一个进程在读写这个文件的时候，另一个进程应该是不能同时写入的。
我们由原来3个logstash同时消费，改为了1个logstash消费，不在报错了。
这个应该也可以通过有话写入hdfs参数来解决。

还有：
Max write retries reached. Exception: initialize: name or service not known {:level=&gt;:error}
losgstash 需要能解析所有 hadoop 集群所有节点的主机名。
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ceph报错管理]]></title>
        <id>http://lvelvis.github.io/post/ceph-bao-cuo-guan-li/</id>
        <link href="http://lvelvis.github.io/post/ceph-bao-cuo-guan-li/">
        </link>
        <updated>2020-08-25T10:12:33.000Z</updated>
        <content type="html"><![CDATA[<p>使用ceph -s查看集群状态，发现一直有如下报错，且数量一直在增加</p>
<pre><code>daemons have recently crashed
</code></pre>
<p>经查当前系统运行状态正常，判断这里显示的应该是历史故障，处理方式如下：</p>
<p>查看历史crash</p>
<pre><code>ceph crash ls-new
</code></pre>
<p>根据ls出来的id查看详细信息</p>
<pre><code>ceph crash info &lt;crash-id&gt;
</code></pre>
<p>将历史crash信息进行归档，即不再显示</p>
<pre><code>ceph crash archive &lt;crash-id&gt;

</code></pre>
<p>归档所有信息</p>
<pre><code>ceph crash archive-all
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[golang笔记-string、int、int64互相转换]]></title>
        <id>http://lvelvis.github.io/post/golang-bi-ji-stringintint64-hu-xiang-zhuan-huan/</id>
        <link href="http://lvelvis.github.io/post/golang-bi-ji-stringintint64-hu-xiang-zhuan-huan/">
        </link>
        <updated>2020-08-10T08:35:04.000Z</updated>
        <content type="html"><![CDATA[<pre><code>#string到int  
int,err:=strconv.Atoi(string)  
#string到int64  
int64, err := strconv.ParseInt(string, 10, 64)  
#int到string  
string:=strconv.Itoa(int)  
#int64到string  
string:=strconv.FormatInt(int64,10)  ```

同类型之间转换，比如int64到int，直接int(int64)即可；
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[golang笔记-go-restful]]></title>
        <id>http://lvelvis.github.io/post/golang-bi-ji-go-restful/</id>
        <link href="http://lvelvis.github.io/post/golang-bi-ji-go-restful/">
        </link>
        <updated>2020-06-22T06:45:14.000Z</updated>
        <content type="html"><![CDATA[<p>用golang写一个restful api。如果您不知道什么是restful,可以看<a href="http://www.ruanyifeng.com/blog/2014/05/restful_api.html">阮一峰老师的教程</a></p>
<p>首先，我们需要解决的是路由的问题，也就是如何将不同的url映射到不同的处理函数。</p>
<pre><code>    router.GET(&quot;/api/todo/:todoid&quot;, getTodoById)
    router.POST(&quot;/api/todo/&quot;, addTodo)
    router.DELETE(&quot;/api/todo/:todoid&quot;, deleteTodo)
    router.PUT(&quot;/api/todo/:todoid&quot;, modifyTodo)
</code></pre>
<p>作为一个初学者，我马上打开github,找到了<a href="https://github.com/avelino/awesome-go">awesome-go</a>,经过一番调研，我感觉有几个http router的库比较适合：bone, httprouter, mux</p>
<h2 id="基本框架">基本框架</h2>
<p>首先，我们设计了四个路由，分别为根据Id获得todo，增加todo，修改todo，删除todo。这里关于解析路由参数，我们使用了httprouter.Params的ByName函数。</p>
<pre><code>package main

import (
  &quot;fmt&quot;
  &quot;github.com/julienschmidt/httprouter&quot;
  &quot;net/http&quot;
  &quot;log&quot;
  &quot;io&quot;
  &quot;io/ioutil&quot;
)

func getTodoById(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  fmt.Fprintf(w, &quot;getTodo %s\n&quot;, todoid)
}

func addTodo(w http.ResponseWriter, r *http.Request, _ httprouter.Params){
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  fmt.Fprintf(w, &quot;addTodo! %s\n&quot;,body)
}

func deleteTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  fmt.Fprintf(w, &quot;deleteTodo %s\n&quot;, todoid)
}

func modifyTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  fmt.Fprintf(w, &quot;modifyTodo %s to %s\n&quot;, todoid, body)
}

func main() {
    router := httprouter.New()
    router.GET(&quot;/api/todo/:todoid&quot;, getTodoById)
    router.POST(&quot;/api/todo/&quot;, addTodo)
    router.DELETE(&quot;/api/todo/:todoid&quot;, deleteTodo)
    router.PUT(&quot;/api/todo/:todoid&quot;, modifyTodo)
    log.Fatal(http.ListenAndServe(&quot;:8080&quot;, router))
}
</code></pre>
<p>我们可以用curl来测试一下我们的api,以put为例</p>
<pre><code>curl --data &quot;content=shopping&amp;time=tomorrow&quot; http://127.0.0.1:8080/api/todo/123 -X PUT

// modifyTodo 123 to content=shopping&amp;time=tomorrow
</code></pre>
<h2 id="json的解析">json的解析</h2>
<p>我们在使用restful api的时候，常常需要给后台传递数据。从上面可以看到，我们通过http.Request的Body属性可以获得数据</p>
<pre><code>body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
</code></pre>
<p>从上面，我们读出的数据是[]byte，但是我们希望将其解析为对象，那么在这之前，我们需要先定义我们的struct。假设我们的todo只有一个字段，就是Name</p>
<pre><code>type Todo struct {
    Name      string
}
</code></pre>
<p>现在我们可以这样解析</p>
<pre><code>var todo Todo;
json.Unmarshal(body, &amp;todo);
</code></pre>
<h2 id="model层设计">model层设计</h2>
<pre><code>package main

import (
  &quot;gopkg.in/mgo.v2&quot;
  &quot;fmt&quot;
  &quot;log&quot;
  &quot;gopkg.in/mgo.v2/bson&quot;
)

var session *mgo.Session

func init(){
  session,_ = mgo.Dial(&quot;mongodb://127.0.0.1&quot;)
}

type Todo struct {
    Name      string
}

func createTodo(t Todo){
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  c.Insert(&amp;t)
}

func queryTodoById(id string){
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  result := Todo{}

  err := c.Find(bson.M{&quot;_id&quot;: bson.ObjectIdHex(id)}).One(&amp;result)
  if err != nil {
    log.Fatal(err)
  }

  fmt.Println(&quot;Todo:&quot;, result.Name)
}

func removeTodo(id string){
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  err := c.Remove(bson.M{&quot;_id&quot;: bson.ObjectIdHex(id)})
  if err != nil{
    log.Fatal(err)
  }
}

func updateTodo(id string, update interface{}){
  //change := bson.M{&quot;$set&quot;: bson.M{&quot;name&quot;: &quot;hahaha&quot;}}
  c := session.DB(&quot;test&quot;).C(&quot;todo&quot;)
  err := c.Update(bson.M{&quot;_id&quot;: bson.ObjectIdHex(id)}, update)
  if err != nil{
    log.Fatal(err)
  }
}
</code></pre>
<p>我们定义了Todo的struct,并添加了几种函数。</p>
<pre><code>package main

import (
  &quot;fmt&quot;
  &quot;github.com/julienschmidt/httprouter&quot;
  &quot;net/http&quot;
  &quot;log&quot;
  &quot;io&quot;
  &quot;io/ioutil&quot;
  &quot;encoding/json&quot;
  &quot;gopkg.in/mgo.v2/bson&quot;
)

func getTodoById(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  queryTodoById(todoid)
  fmt.Fprintf(w, &quot;getUser %s\n&quot;, todoid)
}

func addTodo(w http.ResponseWriter, r *http.Request, _ httprouter.Params){
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  var todo Todo;
  json.Unmarshal(body, &amp;todo);
  createTodo(todo)
  fmt.Fprintf(w, &quot;addUser! %s\n&quot;,body)
}

func deleteTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  removeTodo(todoid)
  fmt.Fprintf(w, &quot;deleteUser %s\n&quot;, todoid)
}

func modifyTodo(w http.ResponseWriter, r *http.Request, params httprouter.Params){
  todoid := params.ByName(&quot;todoid&quot;)
  body, _ := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
  var todo Todo
  json.Unmarshal(body, &amp;todo);
  change := bson.M{&quot;$set&quot;: bson.M{&quot;name&quot;: todo.Name}}
  updateTodo(todoid,change)
  fmt.Fprintf(w, &quot;modifyUser %s to %s\n&quot;, todoid, body)
}

func main() {
    router := httprouter.New()
    router.GET(&quot;/api/todo/:todoid&quot;, getTodoById)
    router.POST(&quot;/api/todo/&quot;, addTodo)
    router.DELETE(&quot;/api/todo/:todoid&quot;, deleteTodo)
    router.PUT(&quot;/api/todo/:todoid&quot;, modifyTodo)
    log.Fatal(http.ListenAndServe(&quot;:8080&quot;, router))
}</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[jenkins x on kubernetes实践(支持多主)]]></title>
        <id>http://lvelvis.github.io/post/jenkins-x-on-kubernetes-shi-jian-zhi-chi-duo-zhu/</id>
        <link href="http://lvelvis.github.io/post/jenkins-x-on-kubernetes-shi-jian-zhi-chi-duo-zhu/">
        </link>
        <updated>2020-06-18T02:26:17.000Z</updated>
        <content type="html"><![CDATA[<h2 id="jenkins是什么">jenkins是什么？</h2>
<p>Jenkins是一个开源的持续集成工具，可用于自动化的执行与构建，测试和交付或部署软件有关的各种任务,有非常丰富的插件支持。</p>
<h2 id="kubernetes是什么">kubernetes是什么？</h2>
<p>Kubernetes是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。这个视频生动地介绍了k8s</p>
<h2 id="jenkins-on-k8s-有什么好处">jenkins on k8s 有什么好处？</h2>
<p>jenkins通过单Master多个Slave的方式提供服务，Master保存了任务的配置信息，安装的插件等等，而slave主要负责执行任务，在使用中存在以下几个问题：</p>
<ol>
<li>当存在多个slave时，运行slave的机器难以统一管理，每次添加新节点时总要做大量的重复工作。</li>
<li>由于不同业务的构建频率并不相同，在使用会发现有很多slave大多数时间都处于空闲状态，造成资源浪费</li>
<li>jenkins默认采取保守的调度方式，造成某些slave的负载过高，任务不能平均分配</li>
</ol>
<h2 id="jenkins架构">jenkins架构</h2>
<p><img src="http://lvelvis.github.io/post-images/1592447693022.png" alt="" loading="lazy"><br>
使用k8s管理jenkins具有以下优势：</p>
<ol>
<li>使用docker运行jenkins保证环境的一致性，可以根据不同业务选择合适的镜像</li>
<li>k8s对抽象后的资源（pods）进行统一的管理调度，提供资源隔离和共享，使机器计算资源变得弹性可扩展,避免资源浪费。</li>
<li>k8s提供容器的自愈功能，能够保证始终有一定数量的容器是可用的</li>
<li>k8s默认的调度器提供了针对节点当前资源分配容器的调度策略，调度器支持插件化部署便于自定义。</li>
</ol>
<h2 id="一搭建环境">一，搭建环境</h2>
<h3 id="工具准备">工具准备</h3>
<pre><code>kubernetes v1.8.4
docker v1.12.6
jenkins master镜像 jenkins/jenkins:lts（v2.73.3）
slave镜像 jenkinsci/jnlp-slave
Kubernetes plugin (v1.1)
</code></pre>
<h3 id="安装kubernetes集群">安装kubernetes集群</h3>
<p>中文教程：https://www.kubernetes.org.cn/2906.html<br>
省略.....</p>
<h2 id="二创建statefulset">二，创建StatefulSet</h2>
<p>StatefulSet(有状态副本集)：Deployments适用于运行无状态应用，StatefulSet则为有状态的应用提供了支持，可以为应用提供有序的部署和扩展，稳定的持久化存储，我们使用SS来运行jenkins master。</p>
<p>创建完整的Stateful Set需要依次创建一下对象：<br>
1、Persistent Volume<br>
2、Persistent Volume Claim<br>
3、StatefulSet<br>
4、Service</p>
<p>创建PersistentVolume：<br>
为了保存应用运行时的数据需要先创建k8s的卷文件，K8s中存在Volume和PersistentVolume两种类型：</p>
<ol>
<li>Volume：与docker中的volume不同在于Volume生命周期是和pod绑定的，与pod中的container无关。k8s为Volume提供了多种类型文件系统（cephfs,nfs…,简单起见我直接选择了hostPath，使用的node节点本地的存储系统）</li>
<li>PersistentVolume:从名字可以看出来，PV的生命周期独立于使用它的pod，不会像volume随pod消失而消失，而是做为一个集群中的资源存在（像node节点一样），同时PV屏蔽了使用具体存储系统的细节。<br>
k8s中的对象都是通过yaml文件来定义的，首先创建名为jenkins-volume.yml的文件:</li>
</ol>
<p>❣️❣️注意：PV的创建有静态，动态两种方式，动态创建可以减少管理员的操作步骤，需要提供指定的StorageClass。为了测试方便，所以我们直接选择静态创建，manual是一个不存在的storage class</p>
<pre><code>kind: PersistentVolume
apiVersion: v1
metadata:
  name: jenkins-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/tmp/data&quot;
</code></pre>
<p>master节点执行下面的命令，PV就手动创建完了</p>
<pre><code>kubectl create -f jenkins-volume1.yaml
</code></pre>
<p>创建PersistentVolumeClaim：</p>
<pre><code>PersistentVolumeClaim(PVC):
持久化存储卷索取，如果说PV是集群中的资源，PVC就是资源的消费者，PVC可以指定需要的资源大小和访问方式,pod不会和PV直接接触，而是通过PVC来请求资源，PV的生成阶段叫做provision,生成PV后会绑定到PVC对象，然后才能被其他对象使用。
</code></pre>
<p>PV和PVC的生命周期如下图：<br>
<img src="http://lvelvis.github.io/post-images/1592447954632.png" alt="" loading="lazy">pv life<br>
创建文件jenkins-claim.yaml<br>
注意： name必须为jenkins-home-jenkins-0否则会绑定失败</p>
<pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: jenkins-home-jenkins-0
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
</code></pre>
<p>执行命令kubectl create -f jenkins-claim.yaml<br>
然后查看PVC是否创建成功，status为bound说明PVC已经绑定</p>
<pre><code>[root@master ~]# kubectl describe pvc jenkins-home-jenkins-0
Name:          jenkins-home-jenkins-0
Namespace:     kubernetes-plugin
StorageClass:  manual
Status:        Bound
Volume:        jenkins-volume
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed=yes
               pv.kubernetes.io/bound-by-controller=yes
Capacity:      10Gi
Access Modes:  RWO
Events:        &lt;none&gt;
</code></pre>
<p>创建StatefulSet和Service：<br>
从kubernetes-plugin github仓库下载jenkins.yml文件</p>
<pre><code>wget https://raw.githubusercontent.com/jenkinsci/kubernetes-plugin/master/src/main/kubernetes/jenkins.yml
修改jenkins.yml：
去掉87行externalTrafficPolicy: Local（这是GKE使用的）
修改83行type: LoadBalancer改为type: NodePort
</code></pre>
<p>注意：<br>
service type=ClusterIP时只允许从集群内部访问， type设置为NodePort是为了从集群外的机器访问jenkins,请谨慎使用，开启NodePort会在所有节点（含master）的统一固定端口开放服务。</p>
<p>执行命令</p>
<pre><code>[root@master ~]# kubectl create -f jenkins.yml 
statefulset &quot;jenkins&quot; created
service &quot;jenkins&quot; created
访问jenkins master,地址为masterip:32058
</code></pre>
<p>#查看映射的端口</p>
<pre><code>[root@master ~]# kubectl get service jenkins
NAME      TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)                        AGE
jenkins   NodePort   10.96.82.68   &lt;none&gt;        80:32058/TCP,50000:30345/TCP   1m

</code></pre>
<p>查看pod : jenkins-0的容器日志，粘贴下面的密码进入jenkins,jenkins安装完成。</p>
<pre><code>Jenkins initial setup is required. An admin user has been created and a password generated.
Please use the following password to proceed to installation:
70aa7b41ba894855abccd09306625b8a
</code></pre>
<h3 id="问题分析">问题分析</h3>
<p>1.创建stateful set时失败，提示”PersistentVolumeClaim is not bound: “jenkins-home-jenkins-0”：”<br>
因为采用静态创建PV时，StatefulSet会按照固定名称查找PVC，PVC的名字要满足</p>
<p>PVC_name == volumeClaimTemplates_name + “-“ + pod_name</p>
<p>这里的名字就是jenkins-home-jenkins-0</p>
<p>2.pod启动失败，jenkins用户没有目录权限<br>
错误提示”touch: cannot touch ‘/var/jenkins_home/copy_reference_file.log’: Permission denied<br>
Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions?”<br>
要确保节点目录开放权限,在node上执行命令：</p>
<pre><code>sudo chown -R 1000:1000 /var/jenkins_home/
sudo chown -R 1000:1000 /tmp/data
##如果仍然失败，尝试在node上重启docker
systemctl restart docker
</code></pre>
<p>注意pv指定的hostPath权限也要修改，否则是无效的</p>
<h2 id="三-配置jenkins">三 ，配置jenkins</h2>
<p>创建jenkins服务账号</p>
<pre><code>wget https://raw.githubusercontent.com/jenkinsci/kubernetes-plugin/master/src/main/kubernetes/service-account.yml
kubectl create -f service-account.yml
</code></pre>
<p>配置插件<br>
访问http://masterip:32058/pluginManager/,搜索插件Kubernetes plugin安装；<br>
访问 http://masterip:32058/configure<br>
选择新建云–kubernetes,在URl填写api server地址，<br>
执行kubectl describe命令，复制output中的token，填入到 ‘Kubernetes server certificate key’</p>
<p>[root@master ~]# kubectl get secret<br>
NAME                  TYPE                                  DATA      AGE<br>
default-token-4kb54   kubernetes.io/service-account-token   3         1d<br>
jenkins-token-wzbsx   kubernetes.io/service-account-token   3         1d<br>
[root@master ~]# kubectl describe secret/jenkins-token-wzbsx<br>
...<br>
jenkins url,tunnel填写service的CLUSTER-IP即可，结果如图：<br>
<img src="http://lvelvis.github.io/post-images/1592448124617.png" alt="" loading="lazy">peizhi1<br>
选择add pod template，填写下面的内容，retain slave可以设置运行jenkins slave 的container空闲后能存活多久。<br>
<img src="http://lvelvis.github.io/post-images/1592448148171.png" alt="" loading="lazy">content<br>
插件配置完成。</p>
<h2 id="四-测试">四 ，测试</h2>
<ol>
<li>扩容测试<br>
StatefulSet扩容：<br>
首先需要手动创建PV，PVC(见第二步),然后执行扩容命令</li>
</ol>
<pre><code>kubectl scale statefulset/jenkins --replicas=２
</code></pre>
<p>查看StatefulSet,此时已经拥有两个master节点，访问service时会随机将请求发送给后端的master。</p>
<pre><code>[root@master ~]# kubectl get statefulset/jenkins 
NAME      DESIRED   CURRENT   AGE
jenkins   2         2         5d
</code></pre>
<p>虽然通过k8s可以轻松实现jenkins master节点的拓展，但是由于jenkins存储数据的方式通过本地文件存储，master之间的数据同步还是一个麻烦的问题，参考jenkins存储模型。</p>
<p>jenkins master上保存的文件：</p>
<pre><code>ls /temp/data
jenkins.CLI.xml
jenkins.install.InstallUtil.lastExecVersion
jenkins.install.UpgradeWizard.state
jenkins.model.ArtifactManagerConfiguration.xml
jenkins.model.JenkinsLocationConfiguration.xml
jobs
logs
nodeMonitors.xml
nodes
</code></pre>
<ol start="2">
<li>高可用测试<br>
现在stateful set中已经有两个pod,在jenkins-1所在的节点执行docker stop停止运行jenkins-master的容器，同时在命令行查看pod的状态，可以看到jenkins-1异常（Error状态）之后慢慢恢复了运行状态（Running）：</li>
</ol>
<pre><code>[root@master ~]# kubectl get pods -w
NAME        READY     STATUS    RESTARTS   AGE
jenkins-0   1/1       Running   0          1d
jenkins-1   0/1       Running   1         20h
jenkins-1   1/1       Running   1         20h
jenkins-1   0/1       Error     1         20h
jenkins-1   0/1       CrashLoopBackOff   1         20h
jenkins-1   0/1       Running   2         20h
jenkins-1   1/1       Running   2         20h
</code></pre>
<p>kubectl describe pod jenkins-1查看pod的事件日志，k8s通过探针(probe)接口检测到服务停止之后自动执行了拉取镜像，重启container的操作。</p>
<pre><code>Events:
  Type     Reason      Age                From                              Message
  ----     ------      ----               ----                              -------
  Warning  Unhealthy   27m (x2 over 27m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Readiness probe failed: HTTP probe failed with statuscode: 503
  Warning  Unhealthy   27m (x2 over 27m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Liveness probe failed: HTTP probe failed with statuscode: 503
  Warning  Unhealthy   24m                kubelet, iz8pscwd1fv6kprs1zw21kz  Readiness probe failed: Get http://192.168.24.4:8080/login: dial tcp 192.168.24.4:8080: getsockopt: connection refused
  Warning  BackOff     20m (x2 over 20m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Back-off restarting failed container
  Warning  FailedSync  20m (x2 over 20m)  kubelet, iz8pscwd1fv6kprs1zw21kz  Error syncing pod
  Normal   Pulling     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  pulling image &quot;jenkins/jenkins:lts-alpine&quot;
  Normal   Started     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Started container
  Normal   Pulled      19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Successfully pulled image &quot;jenkins/jenkins:lts-alpine&quot;
  Normal   Created     19m (x3 over 20h)  kubelet, iz8pscwd1fv6kprs1zw21kz  Created container
</code></pre>
<ol start="3">
<li>jenkins构建测试<br>
当前集群中使用的jenkins slave镜像只包含一个java运行环境来运行jenkins-slave.jar,在实际使用中需要自定义合适的镜像。选择自定义镜像之后需要修改插件的配置，同样name命名为jnlp替换默认镜像，arguments安装工具提示填写即可。<br>
<img src="http://lvelvis.github.io/post-images/1592448208433.png" alt="" loading="lazy"><br>
创建job，同时开始构建,k8s会在不同节点上创建pod来运行任务</li>
</ol>
<p>jenkins默认调度策略</p>
<ol>
<li>尝试在上次构建的节点上构建，指定某台slave之后会一直使用。</li>
<li>当队列有2个构建时，不会立刻创建两个executor,而是先创建一个executor然后尝试等待executor空闲，目的是保证每个executor被充分利用。<br>
k8s调度策略<br>
使用Pod.spec.nodeSelector根据label为pod选择node<br>
3 .调度器scheduler有Predicates，Priorities两个阶段，分别负责节点过滤和评分排序，各个阶段都有k8s提供的检查项，我们可以自由组合。<br>
（比如PodFitsResources检查cpu内存等资源，PodFitsHostPorts检查端口占用，SelectorSpreadPriority要求一个服务尽量分散分布）自定义schduler参考<br>
资源不足时会发生什么<br>
当前集群中有3个节点，我在node2运行一个CPU占用限制在80%的程序,然后设置jenkins插件ContainerTemplate的request和limit均为cpu 500m,内存500Mi,（500m代表单核CPU的50%）看一下pod会怎么调度<br>
k8s仍然尝试在node2分配节点（为什么其他节点不行），结果POD处于pending状态：</li>
</ol>
<pre><code>{
&quot;phase&quot;: &quot;Pending&quot;,
&quot;conditions&quot;: [
  {
    &quot;type&quot;: &quot;PodScheduled&quot;,
    &quot;status&quot;: &quot;False&quot;,
    &quot;lastProbeTime&quot;: null,
    &quot;lastTransitionTime&quot;: &quot;2017-12-09T08:29:10Z&quot;,
    &quot;reason&quot;: &quot;Unschedulable&quot;,
    &quot;message&quot;: &quot;No nodes are available that match all of the predicates: Insufficient cpu (4), PodToleratesNodeTaints (1).&quot;
  }
],
&quot;qosClass&quot;: &quot;Guaranteed&quot;
</code></pre>
<p>最后pod被删除，而jenkins任务会阻塞一直到有其他空闲的slave出现。</p>
<h2 id="五总结">五，总结</h2>
<p>本文介绍了在k8s集群部署jenkins服务的方式和k8s带来的资源管理便捷，由于我也是刚开始接触k8s,所用的实例只是搭建了用于测试的实验环境，离在实际生产环境中使用还有问题需要验证。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[consul-删除无效服务与节点]]></title>
        <id>http://lvelvis.github.io/post/consul-shan-chu-wu-xiao-fu-wu-yu-jie-dian/</id>
        <link href="http://lvelvis.github.io/post/consul-shan-chu-wu-xiao-fu-wu-yu-jie-dian/">
        </link>
        <updated>2020-06-17T09:43:15.000Z</updated>
        <content type="html"><![CDATA[<p>consul删除无效实例<br>
http://127.0.0.1:8500/v1/agent/service/deregister/test-9c14fa595ddfb8f4c34c673c65b072bb</p>
<p>test-9c14fa595ddfb8f4c34c673c65b072bb : 实例id<br>
method : put</p>
<p>删除无效节点<br>
http://127.0.0.1:8500/v1/v1/agent/force-leave/4b36b27317a0</p>
<p>consul leave #关闭consul并离开集群。也可以使用Ctrl+C或kill -INT来gracefully停止agent，这种体面的离开方式让consule可以有机会通知集群其他成员自己的离开。如果你强制地结束了agent，其他member会检测到这个节点的failed。当成员离开时，它的services和checks都会从catalog中移除。当成员failed时，它的health只是简单的被标记为critical，并不会从catalog中移除。Consul会自动尝试重新连接failed节点，允许它从恶劣的网络环境中恢复，显然离开的nodes不会被重新连接。另外，如果这个节点是server，体面的离开对避免潜在的中断的可能很重要。<br>
为了防止dead nodes的积累，consul会自动把dead nodes移除出catalog。这个过程被称为reaping（收割）。默认是72小时的间隔（不建议更改）</p>
<pre><code>#!/bin/bash
clear 
echo &quot;node_exporter注销工具&quot;
read -p &quot;请输入要踢掉的节点IP,如果有多个IP,请使用英文格式 ',' 隔开: &quot; IP_LIST

for IP in `echo &quot;${IP_LIST}&quot;|awk -F, 'BEGIN{OFS=&quot; &quot;}{$1=$1;printf(&quot;%s&quot;,$0);}'`
do 
   curl -XPUT http://10.100.x.x:8500/v1/agent/service/deregister/node-${IP}
   echo &quot;${IP}节点已剔除!&quot;
done
echo &quot;${IP_LIST}完成剔除&quot;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[docker - 内存使用率差异:cgroup memory.usage_in_bytes与docker容器内的RSS]]></title>
        <id>http://lvelvis.github.io/post/docker-nei-cun-shi-yong-lu-chai-yi-cgroup-memoryusage_in_bytes-yu-docker-rong-qi-nei-de-rss/</id>
        <link href="http://lvelvis.github.io/post/docker-nei-cun-shi-yong-lu-chai-yi-cgroup-memoryusage_in_bytes-yu-docker-rong-qi-nei-de-rss/">
        </link>
        <updated>2020-06-17T03:46:43.000Z</updated>
        <content type="html"><![CDATA[<p>&quot;kubernetes&quot;（v1.10.2）表示我的pod（包含一个容器）使用了大约5GB的内存。在容器内部，RSS说的更像681Mib。任何人都能用以下数据解释如何从681Mib到5GB吗（或者用我省略的另一个命令来描述如何弥补差异，无论是从容器还是从在Kubernetes中运行此容器的Docker主机）？<br>
Kubectl Top Pods表示5GB：</p>
<pre><code>% kubectl top pods -l app=myapp
NAME                             CPU(cores)   MEMORY(bytes)
myapp-56b947bf6d-2lcr7           39m          5039Mi
</code></pre>
<p>Cadvisor报告了类似的数字（可能来自稍有不同的时间，因此请忽略细微的差异）：</p>
<pre><code>container_memory_usage_bytes{pod_name=~&quot;.*myapp.*&quot;}      5309456384

5309456384 / 1024.0 / 1024 ~= 5063 ~= 5039
</code></pre>
<p>在容器中，此文件似乎是cadvisor获取其数据的位置：</p>
<pre><code># kubectl exec -it myapp-56b947bf6d-2lcr7 bash
meme@myapp-56b947bf6d-2lcr7:/app# cat /sys/fs/cgroup/memory/memory.usage_in_bytes
5309456384
容器中的常驻集大小（RSS）不匹配（小于1GB）：
meme@myapp-56b947bf6d-2lcr7:/app# kb=$(ps aux | grep -v grep | grep -v 'ps aux' | grep -v bash | grep -v awk | grep -v RSS | awk '{print $6}' | awk '{s+=$1} END {printf &quot;%.0f&quot;, s}'); mb=$(expr $kb / 1024); printf &quot;Kb: $kb\nMb: $mb\n&quot;
Kb: 698076
Mb: 681
</code></pre>
<pre><code>完整的PS AUX，以防有帮助：
meme@myapp-56b947bf6d-2lcr7:/app# ps aux | grep -v grep | grep -v 'ps aux' | grep -v bash | grep -v awk
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
meme         1  0.0  0.0 151840 10984 ?        Ss   Jun04   0:29 /usr/sbin/apache2 -D FOREGROUND
www-data    10  0.0  0.0 147340  4652 ?        S    Jun04   0:00 /usr/sbin/apache2 -D FOREGROUND
www-data    11  0.0  0.0 148556  4392 ?        S    Jun04   0:16 /usr/sbin/apache2 -D FOREGROUND
www-data    12  0.2  0.0 2080632 11348 ?       Sl   Jun04  31:58 /usr/sbin/apache2 -D FOREGROUND
www-data    13  0.1  0.0 2080384 10980 ?       Sl   Jun04  18:12 /usr/sbin/apache2 -D FOREGROUND
www-data    68  0.3  0.0 349048 94272 ?        Sl   Jun04  47:09 hotapp
www-data   176  0.2  0.0 349624 92888 ?        Sl   Jun04  43:11 hotapp
www-data   179  0.2  0.0 349196 94456 ?        Sl   Jun04  42:20 hotapp
www-data   180  0.3  0.0 349828 95112 ?        Sl   Jun04  44:14 hotapp
www-data   185  0.3  0.0 346644 91948 ?        Sl   Jun04  43:49 hotapp
www-data   186  0.3  0.0 346208 91568 ?        Sl   Jun04  44:27 hotapp
www-data   189  0.2  0.0 350208 95476 ?        Sl   Jun04  41:47 hotapp
</code></pre>
<p>Docker容器统计API中的内存部分：</p>
<pre><code>curl --unix-socket /var/run/docker.sock 'http:/v1.24/containers/a45fc651e7b12f527b677e6a46e2902786bee6620484922016a135e317a42b4e/stats?stream=false' | jq . # yields:

&quot;memory_stats&quot;: {
  &quot;usage&quot;: 5327712256,
  &quot;max_usage&quot;: 5368344576,
  &quot;stats&quot;: {
    &quot;active_anon&quot;: 609095680,
    &quot;active_file&quot;: 74457088,
    &quot;cache&quot;: 109944832,
    &quot;dirty&quot;: 28672,
    &quot;hierarchical_memory_limit&quot;: 5368709120,
    &quot;inactive_anon&quot;: 1687552,
    &quot;inactive_file&quot;: 29974528,
    &quot;mapped_file&quot;: 1675264,
    &quot;pgfault&quot;: 295316278,
    &quot;pgmajfault&quot;: 77,
    &quot;pgpgin&quot;: 85138921,
    &quot;pgpgout&quot;: 84964308,
    &quot;rss&quot;: 605270016,
    &quot;rss_huge&quot;: 0,
    &quot;shmem&quot;: 5513216,
    &quot;total_active_anon&quot;: 609095680,
    &quot;total_active_file&quot;: 74457088,
    &quot;total_cache&quot;: 109944832,
    &quot;total_dirty&quot;: 28672,
    &quot;total_inactive_anon&quot;: 1687552,
    &quot;total_inactive_file&quot;: 29974528,
    &quot;total_mapped_file&quot;: 1675264,
    &quot;total_pgfault&quot;: 295316278,
    &quot;total_pgmajfault&quot;: 77,
    &quot;total_pgpgin&quot;: 85138921,
    &quot;total_pgpgout&quot;: 84964308,
    &quot;total_rss&quot;: 605270016,
    &quot;total_rss_huge&quot;: 0,
    &quot;total_shmem&quot;: 5513216,
    &quot;total_unevictable&quot;: 0,
    &quot;total_writeback&quot;: 0,
    &quot;unevictable&quot;: 0,
    &quot;writeback&quot;: 0
  },
  &quot;limit&quot;: 5368709120
},
</code></pre>
<p>对断言的注释：<br>
总计（memory.usage_in_bytes）=rss+缓存<br>
说：<br>
用法\u字节：为了提高效率，与其他内核组件一样，内存组使用一些优化<br>
避免不必要的缓存线错误共享。使用率受<br>
方法，不显示内存（和交换）使用的“精确”值，这是一个模糊的<br>
有效访问的值。（当然，必要时，它是同步的。）<br>
如果您想知道更精确的内存使用情况，应该使用rss+cache（+swap）<br>
内存中的值。stat（见5.2）。<br>
说：<br>
注意：在Linux上，Docker CLI通过从总内存使用量中减去页面缓存使用量来报告内存使用情况。API不执行这样的计算，而是提供总内存使用量和页面缓存的数量，以便客户机可以根据需要使用数据。<br>
实际上，容器中/sys/fs/cgroup/memory/memory.stat中的大多数内容都出现在上面的docker stats api响应中（与在不同时间采集样本略有不同，抱歉）：</p>
<pre><code>meme@myapp-56b947bf6d-2lcr7:/app# cat /sys/fs/cgroup/memory/memory.stat
cache 119492608
rss 607436800
rss_huge 0
shmem 5525504
mapped_file 1675264
dirty 69632
writeback 0
pgpgin 85573974
pgpgout 85396501
pgfault 296366011
pgmajfault 80
inactive_anon 1687552
active_anon 611213312
inactive_file 32800768
active_file 81166336
unevictable 0
hierarchical_memory_limit 5368709120
total_cache 119492608
total_rss 607436800
total_rss_huge 0
total_shmem 5525504
total_mapped_file 1675264
total_dirty 69632
total_writeback 0
total_pgpgin 85573974
total_pgpgout 85396501
total_pgfault 296366011
total_pgmajfault 80
total_inactive_anon 1687552
total_active_anon 611213312
total_inactive_file 32800768
total_active_file 81166336
total_unevictable 0
</code></pre>
<p>内存信息来自：<br>
Limits:<br>
memory:  5Gi<br>
Requests:<br>
memory:   4Gi</p>
<p>下面是容器内的提示。在这一行程序中，我获取所有进程ID，在它们上运行pmap-x，并从pmap结果中提取kbytes列。总的结果是256兆字节（远小于PS的RSS，我认为部分原因是许多进程没有从PMAP-X返回输出）：</p>
<pre><code>ps aux | awk '{print $2}' | grep -v PID | xargs sudo pmap -x | grep total | grep -v grep | awk '{print $3}' | awk '{s+=$1} END {printf &quot;%.0f&quot;, s}'; echo
256820
</code></pre>
<p>https://github.com/google/cadvisor/issues/638中提到了。它检查kubectl describe pod <pod>和pmap。这里没有照明（同样，它似乎忽略了一些过程）：</p>
<pre><code># python ps_mem.py
Private  +   Shared  =  RAM used    Program

  1.7 MiB +   1.0 MiB =   2.7 MiB   apache2
  2.0 MiB +   1.0 MiB =   3.0 MiB   bash (3)
---------------------------------
                          5.7 MiB
=================================
</code></pre>
<p>最佳答案</p>
<pre><code>有一件事我没看到您检查这里是内核内存。这在memory.usage_in_bytes图中也有说明，但在memory.stat中没有出现。您可以通过查看/sys/fs/cgroup/memory/memory.kmem.usage_in_bytes找到它。
有一次，我看到我们的一个.NET核心应用程序也发生了类似的事情，但我不知道到底发生了什么（可能是.NET核心内存泄漏，因为它是我们的应用程序无法控制的非托管内存）。这将取决于您的应用程序使用是否正常，但就cgroups而言，我相信内核内存使用在默认情况下是不受约束的。
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[golang笔记-go mod与go vendor]]></title>
        <id>http://lvelvis.github.io/post/golang-bi-ji-go-mod-yu-go-vendor/</id>
        <link href="http://lvelvis.github.io/post/golang-bi-ji-go-mod-yu-go-vendor/">
        </link>
        <updated>2020-06-10T08:40:20.000Z</updated>
        <content type="html"><![CDATA[<h1 id="go-mod-使用">go mod 使用</h1>
<p>解决的问题是golang不再依赖gopath的设置，下载下来的包可以直接使用。<br>
go mod init ./<br>
go build main.go 或 go build -mod=vendor main.go<br>
go mod vendor #将包打到vendor文件夹下</p>
<h1 id="go-vendor">go vendor</h1>
<p>管理Golang项目依赖，应该是一个第三方的，但是比较好用。</p>
<p>安装<br>
go get -u github.com/kardianos/govendor</p>
<p>使用一套连招：</p>
<pre><code>govendor init # 创建vendor目录，创建vendor.json文件  
govendor add +external #生成依赖包  
govendor update +vendor # 更新vendor的包命令  
状态	缩写状态	含义
+local	l	本地包，即项目自身的包组织
+external	e	外部包，即被 $GOPATH 管理，但不在 vendor 目录下
+vendor	v	已被 govendor 管理，即在 vendor 目录下
+std	s	标准库中的包
+unused	u	未使用的包，即包在 vendor 目录下，但项目并没有用到
+missing	m	代码引用了依赖包，但该包并没有找到
+program	p	主程序包，意味着可以编译为执行文件
+outside	 	外部包和缺失的包
+all	 	所有的包
命令	功能
init	初始化 vendor 目录
list	列出所有的依赖包
add	添加包到 vendor 目录，如 govendor add +external 添加所有外部包
add PKG_PATH	添加指定的依赖包到 vendor 目录
update	从 $GOPATH 更新依赖包到 vendor 目录
remove	从 vendor 管理中删除依赖
status	列出所有缺失、过期和修改过的包
fetch	添加或更新包到本地 vendor 目录
sync	本地存在 vendor.json 时候拉去依赖包，匹配所记录的版本
get	类似 go get 目录，拉取依赖包到 vendor 目录
</code></pre>
]]></content>
    </entry>
</feed>