<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
<meta name="keywords" content="lvelvis个人博客">
<meta name="description" content="时光,浓淡相宜;人心,远近相安;这就是最好的生活">
<meta name="theme-color" content="#000">
<title>lvelvis</title>
<link rel="shortcut icon" href="/favicon.ico?v=1590472413053">
<link rel="stylesheet" href="/styles/main.css">
<link rel="stylesheet" href="/media/css/mist.css">

<link rel="stylesheet" href="/media/fonts/font-awesome.css">
<link
  href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Rosario:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext"
  rel="stylesheet" type="text/css">

<link href="/media/hljs/styles/androidstudio.css"
  rel="stylesheet">

<script src="/media/hljs/highlight.js"></script>
<script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.0/velocity.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.0/velocity.ui.min.js"></script>

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



</head>

<body>
  <div class="head-top-line"></div>
  <div class="header-box">
    
<div class="mist">
  <header class="header bg-color ">
    <div class="blog-header box-shadow-wrapper  " id="header">
      <div class="nav-toggle" id="nav_toggle">
        <div class="toggle-box">
          <div class="line line-top"></div>
          <div class="line line-center"></div>
          <div class="line line-bottom"></div>
        </div>
      </div>
      <div class="site-meta">       
        <div class="site-title">
          
            <a href="/" class="">
              <span class="logo-line-before">
                <i class=""></i>
              </span>
              <span class="main-title">lvelvis</span>
              <span class="logo-line-after">
                <i class=""></i>
              </span>
            </a>  
          
        </div>
        
      </div>
      <nav class="site-nav" id="site_nav">
        <ul id="nav_ul">
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/" target="_self">
                  <i class="fa fa-home"></i> 首页
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/archives/" target="_self">
                  <i class="fa fa-archive"></i> 归档
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/tags/" target="_self">
                  <i class="fa fa-tags"></i> 标签
                </a>
              
            </li>
          
            
            
              
            
            <li class="nav-item ">
              
              
                <a href="/post/about/" target="_self">
                  <i class="fa fa-user"></i> 关于
                </a>
              
            </li>
          
          
            
              <li class="nav-item ">
                <a href="/friends/" target="_self">
                  
                    <i class="fa fa-address-book"></i> 友情链接
                  
                </a>
              </li>
            
          
        </ul>
      </nav>
    </div>
  </header>
</div>

<script type="text/javascript"> 
 
  let showNav = true;

  let navToggle = document.querySelector('#nav_toggle'),
  siteNav = document.querySelector('#site_nav');
  
  function navClick() {
    let sideBar = document.querySelector('.sidebar');
    let navUl = document.querySelector('#nav_ul');
    navToggle.classList.toggle('nav-toggle-active');
    siteNav.classList.toggle('nav-menu-active');
    if (siteNav.classList.contains('nav-menu-active')) {
      siteNav.style = "height: " + (navUl.children.length * 42) +"px !important";
    } else {
      siteNav.style = "";
    }
  }

  navToggle.addEventListener('click',navClick);  
</script>
  </div>
  <div class="main-continer">
    
    <div
      class="section-layout mist bg-color">
      <div class="section-layout-wrapper">
        <div id="sidebarMeta" class="sidebar">
    
<div class="sidebar-wrapper box-shadow-wrapper ">
  <div class="sidebar-item">
    <img class="site-author-image right-motion" src="/images/avatar.png"/>
    <p class="site-author-name">lvelvis</p>
    
    <div class="site-description right-motion">
      
        <p id="binft">时光,浓淡相宜;人心,远近相安;这就是最好的生活</p>
      
    </div>
    
  </div>
  <div class="sidebar-item side-item-stat right-motion">
    <div class="sidebar-item-box">
      <a href="/archives/">
        
        <span class="site-item-stat-count">10</span>
        <span class="site-item-stat-name">文章</span>
      </a>
    </div>
    <div class="sidebar-item-box">
      <a href="">
        <span class="site-item-stat-count">12</span>
        <span class="site-item-stat-name">分类</span>
      </a>
    </div>
    <div class="sidebar-item-box">
      <a href="/tags/">
        <span class="site-item-stat-count">12</span>
        <span class="site-item-stat-name">标签</span>
      </a>
    </div>
  </div>
  
  
    <div class="sidebar-item sidebar-item-social">
      <div class="social-item">
        
          
            <a href="https://github.com/lvelvis">
              <i class="fa fa-github-alt" title="github"></i>
            </a>
          
            <a href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=421220622&amp;website=www.oicqzone.com&#34;&gt;">
              <i class="fa fa-qq" title="QQ"></i>
            </a>
          
        
        
          
            <a class="social-img" href="#">
              <img src="\media\images\custom-array-imgSocials-1588146527513-socialImg.jpg" />
              <i class="fa fa-wechat" title="weixin" ></i>
            </a>
          
        
      </div>
    </div>
  


</div>
</div>
<script>
  let sidebarMeta = document.querySelector('#sidebarMeta');
  let scheme = 'mist';
  let sidebarWrapper = document.querySelector('.sidebar-wrapper');
  if (sidebarMeta && (scheme === 'pisces' || scheme === 'gemini')) {
    document.addEventListener('scroll', function(e) {
      if (document.scrollingElement.scrollTop > parseInt(sidebarMeta.style.marginTop) + 10) {
        sidebarWrapper.classList.add('home-sidebar-fixed')
      } else {
        sidebarWrapper.classList.remove('home-sidebar-fixed')
      }
    });
  }
  </script>
        <div class="section-box mist box-shadow-wrapper">
          <section class="friends-section bg-color slide-down-in">
            <div class="firends-box">
              <h1 class="firends-title" itemprop="name headline">友情链接</h1>
              
              
              <a href="https://shop238553958.taobao.com/?spm=2013.1.w5001-21656029832.4.af1db473liw0H1&amp;scene=taobao_shop" target="_blank">
                <div class="firends-item">
                  <img src="https://upimage.alexhchu.com/2020/04/21/47eda59424daa.gif" />
                  <div class="right">
                    <div class="top">
                      吃鸡淘宝店
                    </div>
                    <div class="bottom">
                      
                    </div>
                  </div>
                </div>
              </a>
              
              
            </div>
            <hr>
            <div style="text-align:center">
              <span class="with-love">
                <i class="fa fa-heart"></i>
              </span>
              留言添加友链
              <span class="with-love">
                <i class="fa fa-heart"></i>
              </span>
            </div>
            <hr>
            <div class="friends-note">
              <p><strong>友链格式：</strong></p>
              <ul>
                <li>网站名称：枫糖</li>
                <li>网站地址：<a href="https://blog.maplesugar.space" data-pjax-state="">https://blog.maplesugar.space</a>
                </li>
                <li> 网站描述：From rookie to master</li>
                <li> 网站 Logo / 头像：<a href="https://blog.maplesugar.space/uploads/maple-leaf-avatar.jpg"
                    data-pjax-state="">https://blog.maplesugar.space/uploads/maple-leaf-avatar.jpg</a></li>
              </ul>
            </div>
            
  <script src="https://cdn.jsdelivr.net/npm/valine@1.4.4/dist/Valine.min.js"></script>
<div id="vcomments" style="padding: 10px 0px 0px 0px"></div>

<style>
  .v .veditor {
    min-height: 10rem;
    background-image: url('https://upimage.alexhchu.com/2020/04/21/47eda59424daa.gif');
    background-size: contain;
    background-repeat: no-repeat;
    background-position: right;
    background-color: rgba(255, 255, 255, 0);
    resize: none;
  }

  .v .vwrap {
    border: 1px solid #000 !important;
  }

  .v .vbtn {
    padding: .4rem 1.2rem !important;
    border-color: #fff !important;
    background-color: #49b1f5 !important;
    color: #fff !important;
    font-size: .7rem !important;
  }

  .v .vcards .vcard .vh .vmeta .vat {
    padding: 0 .8rem !important;
    border: 1px solid #00c4b6 !important;
    border-radius: 5px !important;
    color: #00c4b6 !important;
  }
</style>
<script>
  new Valine({
    el: '#vcomments',
    appId: 'q6RNtDXsSlHgt7Wml6Krh1My-gzGzoHsz',
    appKey: 'XVXX46bo5q1RnjaGrExJEXOm',
    avatar: 'identicon',
    placeholder: '',
    pageSize: '10',
    lang: 'zh-cn',
    visitor: 'true' === 'true',
    highlight: 'true' === 'true',
    avatarForce: 'true' === 'true',
  });
</script>

          </section>
        </div>
      </div>
    </div>
    <div class="footer-box">
  <footer class="footer">
    <div class="copyright">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | © 2020 Theme By elvis</a>
    </div>
    <div class="poweredby">
      <center><iframe width="280" scrolling="no" height="25" frameborder="0" allowtransparency="true" src="http://i.tianqi.com/index.php?c=code&id=34&icon=1&num=3"></iframe></center><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=100 src="//music.163.com/outchain/player?type=0&id=3135828147&auto=1&height=430"></iframe></center><marquee scrollamount="5" ><h2 class="wow fadeIn animated" data-wow-delay="1250ms" style="visibility: visible; animation-delay: 1250ms; animation-name: fadeIn;"><font face="Orbitron" style="color:#CC0707;text-shadow: 0 0 0.9em #6EC101,0 0 0.9em
Yellow;"><b><?php echo $conf['xbmz']?></b></font></h2></marquee>
  <script type="text/javascript"> 
  document.onkeydown = function(event){
   if ((event.ctrlKey)&&(event.keyCode==115 || event.keyCode==83)){
    event.returnValue=false;
    return;
   } 
  }
 </script>
<script async src="https://api.ly522.com/js/jilei.pure.mini.js"></script>
<span id="jilei_container_site_pv">总访问量<span id="jilei_value_site_pv"></span>次</span>
<span class="post-meta-divider">|</span>
<span id="jilei_container_site_uv">在线人数<span id="jilei_value_site_uv"></span>人</span></p>
    </div>
  </footer>
  
  
    <div class="drawer-box left" id="drawer_box">
      <span class="muse-line muse-line-first"></span>
      <span class="muse-line muse-line-middle"></span>
      <span class="muse-line muse-line-last"></span>
    </div>
  
  <div class="mist back-to-top" id="back_to_top">
    <i class="fa fa-arrow-up"></i>
    
  </div>
  
  
    
<link rel="stylesheet" href="/media/live2d/css/live2d.css" />
<div class="box-scale">
  <div id="landlord" style="left: 5px;bottom: px;"
    data-key="">
    <canvas id="live2d" width="500" height="560" class="live2d"></canvas>
    

      <div class="message" style="opacity:0"></div>
      <div class="live_talk_input_body">
        <div class="live_talk_input_name_body">
          <input name="name" type="text" class="live_talk_name white_input" id="AIuserName" autocomplete="off"
            placeholder="你的名字" />
        </div>
        <div class="live_talk_input_text_body">
          <input name="talk" type="text" class="live_talk_talk white_input" id="AIuserText" autocomplete="off"
            placeholder="要和我聊什么呀？" />
          <button type="button" class="live_talk_send_btn" id="talk_send">发送</button>
        </div>
      </div>
      <input name="live_talk" id="live_talk" value="1" type="hidden" />
      <div class="live_ico_box">
        <div class="live_ico_item type_info" id="showInfoBtn"></div>
        <div class="live_ico_item type_talk" id="showTalkBtn"></div>
        
        <div class="live_ico_item type_music" id="musicButton"></div>
        
        <div class="live_ico_item type_youdu" id="youduButton"></div>
        <div class="live_ico_item type_quit" id="hideButton"></div>
        <input name="live_statu_val" id="live_statu_val" value="0" type="hidden" />
        <audio src="" style="display:none;" id="live2d_bgm" data-bgm="0" preload="none"></audio>
        <input id="duType" value="douqilai" type="hidden">
        
        <input name="live2dBGM" value="" type="hidden">
        
      </div>
    
  </div>
</div>
<div id="open_live2d">召唤看板娘</div>
<script src="https://libs.baidu.com/jquery/2.0.0/jquery.min.js"></script>
<script>
  var message_Path = 'https://cdn.jsdelivr.net/gh/hsxyhao/live2d.github.io@master/';
  let landlord = document.querySelector('#landlord');
  var apiKey = landlord.dataset.key;
</script>
<script type="text/javascript" src="/media/live2d/js/live2d.js"></script>
<script>
	var home_Path = document.location.protocol + '//' + window.document.location.hostname + ":" + window.document.location.port + '/';
	var userAgent = window.navigator.userAgent.toLowerCase();
	var norunAI = ["android", "iphone", "ipod", "ipad", "windows phone", "mqqbrowser", "msie", "trident/7.0"];
	var norunFlag = false;

	for (var i = 0; i < norunAI.length; i++) {
		if (userAgent.indexOf(norunAI[i]) > -1) {
			norunFlag = true;
			break;
		}
	}

	if (!window.WebGLRenderingContext) {
		norunFlag = true;
	}

	if (!norunFlag) {
		var hitFlag = false;
		var AIFadeFlag = false;
		var liveTlakTimer = null;
		var sleepTimer_ = null;
		var AITalkFlag = false;
		var talkNum = 0;
		(function () {
			function renderTip(template, context) {
				var tokenReg = /(\\)?\{([^\{\}\\]+)(\\)?\}/g;
				return template.replace(tokenReg, function (word, slash1, token, slash2) {
					if (slash1 || slash2) {
						return word.replace('\\', '');
					}
					var variables = token.replace(/\s/g, '').split('.');
					var currentObject = context;
					var i, length, variable;
					for (i = 0, length = variables.length; i < length; ++i) {
						variable = variables[i];
						currentObject = currentObject[variable];
						if (currentObject === undefined || currentObject === null) return '';
					}
					return currentObject;
				});
			}

			String.prototype.renderTip = function (context) {
				return renderTip(this, context);
			};

			var re = /x/;
			re.toString = function () {
				showMessage('哈哈，你打开了控制台，是想要看看我的秘密吗？', 5000);
				return '';
			};

			$(document).on('copy', function () {
				showMessage('你都复制了些什么呀，转载要记得加上出处哦~~', 5000);
			});

			function initTips() {
				$.ajax({
					cache: true,
					url: message_Path + 'message.json',
					dataType: "json",
					success: function (result) {
						$.each(result.mouseover, function (index, tips) {
							$(tips.selector).mouseover(function () {
								var text = tips.text;
								if (Array.isArray(tips.text)) text = tips.text[Math.floor(Math.random() * tips.text.length + 1) - 1];
								text = text.renderTip({ text: $(this).text() });
								showMessage(text, 3000);
								talkValTimer();
								clearInterval(liveTlakTimer);
								liveTlakTimer = null;
							});
							$(tips.selector).mouseout(function () {
								showHitokoto();
								if (liveTlakTimer == null) {
									liveTlakTimer = window.setInterval(function () {
										showHitokoto();
									}, 15000);
								};
							});
						});
						$.each(result.click, function (index, tips) {
							$(tips.selector).click(function () {
								if (hitFlag) {
									return false
								}
								hitFlag = true;
								setTimeout(function () {
									hitFlag = false;
								}, 8000);
								var text = tips.text;
								if (Array.isArray(tips.text)) text = tips.text[Math.floor(Math.random() * tips.text.length + 1) - 1];
								text = text.renderTip({ text: $(this).text() });
								showMessage(text, 3000);
							});
							clearInterval(liveTlakTimer);
							liveTlakTimer = null;
							if (liveTlakTimer == null) {
								liveTlakTimer = window.setInterval(function () {
									showHitokoto();
								}, 15000);
							};
						});
					}
				});
			}
			initTips();

			var text;
			if (document.referrer !== '') {
				var referrer = document.createElement('a');
				referrer.href = document.referrer;
				text = '嗨！来自 <span style="color:#0099cc;">' + referrer.hostname + '</span> 的朋友！';
				var domain = referrer.hostname.split('.')[1];
				if (domain == 'baidu') {
					text = '嗨！ 来自 百度搜索 的朋友！<br>欢迎访问<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				} else if (domain == 'so') {
					text = '嗨！ 来自 360搜索 的朋友！<br>欢迎访问<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				} else if (domain == 'google') {
					text = '嗨！ 来自 谷歌搜索 的朋友！<br>欢迎访问<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				}
			} else {
				if (window.location.href == home_Path) { //主页URL判断，需要斜杠结尾
					var now = (new Date()).getHours();
					if (now > 23 || now <= 5) {
						text = '你是夜猫子呀？这么晚还不睡觉，明天起的来嘛？';
					} else if (now > 5 && now <= 7) {
						text = '早上好！一日之计在于晨，美好的一天就要开始了！';
					} else if (now > 7 && now <= 11) {
						text = '上午好！工作顺利嘛，不要久坐，多起来走动走动哦！';
					} else if (now > 11 && now <= 14) {
						text = '中午了，工作了一个上午，现在是午餐时间！';
					} else if (now > 14 && now <= 17) {
						text = '午后很容易犯困呢，今天的运动目标完成了吗？';
					} else if (now > 17 && now <= 19) {
						text = '傍晚了！窗外夕阳的景色很美丽呢，最美不过夕阳红~~';
					} else if (now > 19 && now <= 21) {
						text = '晚上好，今天过得怎么样？';
					} else if (now > 21 && now <= 23) {
						text = '已经这么晚了呀，早点休息吧，晚安~~';
					} else {
						text = '嗨~ 快来逗我玩吧！';
					}
				} else {
					text = '欢迎阅读<span style="color:#0099cc;">「 ' + document.title.split(' - ')[0] + ' 」</span>';
				}
			}
			showMessage(text, 12000);
		})();

		liveTlakTimer = setInterval(function () {
			showHitokoto();
		}, 15000);

		function showHitokoto() {
			if (sessionStorage.getItem("Sleepy") !== "1") {
				if (!AITalkFlag) {
					$.getJSON('https://v1.hitokoto.cn/', function (result) {
						talkValTimer();
						showMessage(result.hitokoto, 0);
					});
				}
			} else {
				hideMessage(0);
				if (sleepTimer_ == null) {
					sleepTimer_ = setInterval(function () {
						checkSleep();
					}, 200);
				}
			}
		}

		function checkSleep() {
			var sleepStatu = sessionStorage.getItem("Sleepy");
			if (sleepStatu !== '1') {
				talkValTimer();
				showMessage('你回来啦~', 0);
				clearInterval(sleepTimer_);
				sleepTimer_ = null;
			}
		}

		function showMessage(text, timeout) {
			if (Array.isArray(text)) text = text[Math.floor(Math.random() * text.length + 1) - 1];
			$('.message').stop();
			$('.message').html(text);
			$('.message').fadeTo(200, 1);
			//if (timeout === null) timeout = 5000;
			//hideMessage(timeout);
		}
		function talkValTimer() {
			$('#live_talk').val('1');
		}

		function hideMessage(timeout) {
			//$('.message').stop().css('opacity',1);
			if (timeout === null) timeout = 5000;
			$('.message').delay(timeout).fadeTo(200, 0);
		}

		function initLive2d() {
			$('#hideButton').on('click', function () {
				if (AIFadeFlag) {
					return false;
				} else {
					AIFadeFlag = true;
					localStorage.setItem("live2dhidden", "0");
					$('#landlord').fadeOut(200);
					$('#open_live2d').delay(200).fadeIn(200);
					setTimeout(function () {
						AIFadeFlag = false;
					}, 300);
				}
			});
			$('#open_live2d').on('click', function () {
				if (AIFadeFlag) {
					return false;
				} else {
					AIFadeFlag = true;
					localStorage.setItem("live2dhidden", "1");
					$('#open_live2d').fadeOut(200);
					$('#landlord').delay(200).fadeIn(200);
					setTimeout(function () {
						AIFadeFlag = false;
					}, 300);
				}
			});
			$('#youduButton').on('click', function () {
				if ($('#youduButton').hasClass('doudong')) {
					var typeIs = $('#youduButton').attr('data-type');
					$('#youduButton').removeClass('doudong');
					$('body').removeClass(typeIs);
					$('#youduButton').attr('data-type', '');
				} else {
					var duType = $('#duType').val();
					var duArr = duType.split(",");
					var dataType = duArr[Math.floor(Math.random() * duArr.length)];

					$('#youduButton').addClass('doudong');
					$('#youduButton').attr('data-type', dataType);
					$('body').addClass(dataType);
				}
			});
			if (apiKey) {
				$('#showInfoBtn').on('click', function () {
					var live_statu = $('#live_statu_val').val();
					if (live_statu == "0") {
						return
					} else {
						$('#live_statu_val').val("0");
						$('.live_talk_input_body').fadeOut(500);
						AITalkFlag = false;
						showHitokoto();
						$('#showTalkBtn').show();
						$('#showInfoBtn').hide();
					}
				});
				$('#showTalkBtn').on('click', function () {
					var live_statu = $('#live_statu_val').val();
					if (live_statu == "1") {
						return
					} else {
						$('#live_statu_val').val("1");
						$('.live_talk_input_body').fadeIn(500);
						AITalkFlag = true;
						$('#showTalkBtn').hide();
						$('#showInfoBtn').show();

					}
				});
				$('#talk_send').on('click', function () {
					var info_ = $('#AIuserText').val();
					var userid_ = $('#AIuserName').val();
					if (info_ == "") {
						showMessage('写点什么吧！', 0);
						return;
					}
					if (userid_ == "") {
						showMessage('聊之前请告诉我你的名字吧！', 0);
						return;
					}
					showMessage('思考中~', 0);
					let protocol = window.location.protocol.indexOf("s") > 0 ? "https" : "http";
					$.ajax({
						type: "get",
						url: `${protocol}://www.tuling123.com/openapi/api?key=${apiKey}&info=${info_}`,
						dataType: "json",
						success: function (res) {
							talkValTimer();
							showMessage(res.text, 0);
							$('#AIuserText').val("");
							sessionStorage.setItem("live2duser", userid_);
						},
						error: function (e) {
							talkValTimer();
							showMessage('似乎有什么错误，请和站长联系！', 0);
						}
					});
				});
			} else {
				$('#showInfoBtn').hide();
				$('#showTalkBtn').hide();
			}
			//获取音乐信息初始化
			var bgmListInfo = $('input[name=live2dBGM]');
			if (bgmListInfo.length == 0) {
				$('#musicButton').hide();
			} else {
				var bgmPlayNow = parseInt($('#live2d_bgm').attr('data-bgm'));
				var bgmPlayTime = 0;
				var live2dBGM_Num = sessionStorage.getItem("live2dBGM_Num");
				var live2dBGM_PlayTime = sessionStorage.getItem("live2dBGM_PlayTime");
				if (live2dBGM_Num) {
					if (live2dBGM_Num <= $('input[name=live2dBGM]').length - 1) {
						bgmPlayNow = parseInt(live2dBGM_Num);
					}
				}
				if (live2dBGM_PlayTime) {
					bgmPlayTime = parseInt(live2dBGM_PlayTime);
				}
				var live2dBGMSrc = bgmListInfo.eq(bgmPlayNow).val();
				$('#live2d_bgm').attr('data-bgm', bgmPlayNow);
				$('#live2d_bgm').attr('src', live2dBGMSrc);
				$('#live2d_bgm')[0].currentTime = bgmPlayTime;
				$('#live2d_bgm')[0].volume = 0.5;
				var live2dBGM_IsPlay = sessionStorage.getItem("live2dBGM_IsPlay");
				var live2dBGM_WindowClose = sessionStorage.getItem("live2dBGM_WindowClose");
				if (live2dBGM_IsPlay == '0' && live2dBGM_WindowClose == '0') {
					$('#live2d_bgm')[0].play();
					$('#musicButton').addClass('play');
				}
				sessionStorage.setItem("live2dBGM_WindowClose", '1');
				$('#musicButton').on('click', function () {
					if ($('#musicButton').hasClass('play')) {
						$('#live2d_bgm')[0].pause();
						$('#musicButton').removeClass('play');
						sessionStorage.setItem("live2dBGM_IsPlay", '1');
					} else {
						$('#live2d_bgm')[0].play();
						$('#musicButton').addClass('play');
						sessionStorage.setItem("live2dBGM_IsPlay", '0');
					}
				});
				window.onbeforeunload = function () {
					sessionStorage.setItem("live2dBGM_WindowClose", '0');
					if ($('#musicButton').hasClass('play')) {
						sessionStorage.setItem("live2dBGM_IsPlay", '0');
					}
				}
				document.getElementById('live2d_bgm').addEventListener("timeupdate", function () {
					var live2dBgmPlayTimeNow = document.getElementById('live2d_bgm').currentTime;
					sessionStorage.setItem("live2dBGM_PlayTime", live2dBgmPlayTimeNow);
				});
				document.getElementById('live2d_bgm').addEventListener("ended", function () {
					var listNow = parseInt($('#live2d_bgm').attr('data-bgm'));
					listNow++;
					if (listNow > $('input[name=live2dBGM]').length - 1) {
						listNow = 0;
					}
					var listNewSrc = $('input[name=live2dBGM]').eq(listNow).val();
					sessionStorage.setItem("live2dBGM_Num", listNow);
					$('#live2d_bgm').attr('src', listNewSrc);
					$('#live2d_bgm')[0].play();
					$('#live2d_bgm').attr('data-bgm', listNow);
				});
				document.getElementById('live2d_bgm').addEventListener("error", function () {
					$('#live2d_bgm')[0].pause();
					$('#musicButton').removeClass('play');
					showMessage('音乐似乎加载不出来了呢！', 0);
				});
			}
			//获取用户名
			var live2dUser = sessionStorage.getItem("live2duser");
			if (live2dUser !== null) {
				$('#AIuserName').val(live2dUser);
			}
			//获取位置
			var landL = sessionStorage.getItem("historywidth");
			var landB = sessionStorage.getItem("historyheight");
			if (landL == null || landB == null) {
				landL = '5px'
				landB = '0px'
			}
			$('#landlord').css('left', landL + 'px');
			$('#landlord').css('bottom', landB + 'px');
			//移动
			function getEvent() {
				return window.event || arguments.callee.caller.arguments[0];
			}
			var smcc = document.getElementById("landlord");
			var moveX = 0;
			var moveY = 0;
			var moveBottom = 0;
			var moveLeft = 0;
			var moveable = false;
			var docMouseMoveEvent = document.onmousemove;
			var docMouseUpEvent = document.onmouseup;
			smcc.onmousedown = function () {
				var ent = getEvent();
				moveable = true;
				moveX = ent.clientX;
				moveY = ent.clientY;
				var obj = smcc;
				moveBottom = parseInt(obj.style.bottom);
				moveLeft = parseInt(obj.style.left);
				if (isFirefox = navigator.userAgent.indexOf("Firefox") > 0) {
					window.getSelection().removeAllRanges();
				}
				document.onmousemove = function () {
					if (moveable) {
						var ent = getEvent();
						var x = moveLeft + ent.clientX - moveX;
						var y = moveBottom + (moveY - ent.clientY);
						obj.style.left = x + "px";
						obj.style.bottom = y + "px";
					}
				};
				document.onmouseup = function () {
					if (moveable) {
						var historywidth = obj.style.left;
						var historyheight = obj.style.bottom;
						historywidth = historywidth.replace('px', '');
						historyheight = historyheight.replace('px', '');
						sessionStorage.setItem("historywidth", historywidth);
						sessionStorage.setItem("historyheight", historyheight);
						document.onmousemove = docMouseMoveEvent;
						document.onmouseup = docMouseUpEvent;
						moveable = false;
						moveX = 0;
						moveY = 0;
						moveBottom = 0;
						moveLeft = 0;
					}
				};
			};
		}
		$(document).ready(function () {
			var AIimgSrc = [];
			let chooseLive2d = 'hijiki'
			if (chooseLive2d === 'histoire') {
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_00.png");
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_01.png");
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_02.png");
				AIimgSrc.push(message_Path + "model/histoire/histoire.1024/texture_03.png");
			} else if (chooseLive2d === 'rem') {
				AIimgSrc.push(message_Path + "model/rem/remu2048/texture_00.png");
			} else if (chooseLive2d === 'Aoba') {
				AIimgSrc.push(message_Path + "model/Aoba/textures/texture_00.png");
			} else if (chooseLive2d === 'hijiki') {
				AIimgSrc.push(message_Path + "model/hijiki/moc/hijiki.2048/texture_00.png");
			} else if (chooseLive2d === 'tororo') {
				AIimgSrc.push(message_Path + "model/tororo/moc/tororo.2048/texture_00.png");
			}
			var images = [];
			var imgLength = AIimgSrc.length;
			var loadingNum = 0;
			for (var i = 0; i < imgLength; i++) {
				images[i] = new Image();
				images[i].src = AIimgSrc[i];
				images[i].onload = function () {
					loadingNum++;
					if (loadingNum === imgLength) {
						var live2dhidden = localStorage.getItem("live2dhidden");
						if (live2dhidden === "0") {
							setTimeout(function () {
								$('#open_live2d').fadeIn(200);
							}, 1300);
						} else {
							setTimeout(function () {
								$('#landlord').fadeIn(200);
							}, 1300);
						}
						let model = '';
						if (chooseLive2d === 'histoire') {
							model = message_Path + "model/histoire/model.json";
						} else if (chooseLive2d === 'rem') {
							model = message_Path + "model/rem/model.json";
						} else if (chooseLive2d === 'Aoba') {
							model = message_Path + "model/Aoba/model.json";
						} else if (chooseLive2d === 'hijiki') {
							model = message_Path + "model/hijiki/hijiki.model.json";
						} else if (chooseLive2d === 'tororo') {
							model = message_Path + "model/tororo/tororo.model.json";
						}
						setTimeout(function () {
							loadlive2d("live2d", model);
						}, 1000);
						initLive2d();
						images = null;
					}
				}
			}
		});
	}
</script>
  
  
</div>
<script>

  let sideBarOpen = 'sidebar-open';
  let body = document.body;
  let back2Top = document.querySelector('#back_to_top'),
  back2TopText = document.querySelector('#back_to_top_text'),
  drawerBox = document.querySelector('#drawer_box'),
  rightSideBar = document.querySelector('.sidebar'),
  viewport = document.querySelector('body');

  function scrollAnimation(currentY, targetY) {
   
    let needScrollTop = targetY - currentY
    let _currentY = currentY
    setTimeout(() => {
      const dist = Math.ceil(needScrollTop / 10)
      _currentY += dist
      window.scrollTo(_currentY, currentY)
      if (needScrollTop > 10 || needScrollTop < -10) {
        scrollAnimation(_currentY, targetY)
      } else {
        window.scrollTo(_currentY, targetY)
      }
    }, 1)
  }

  back2Top.addEventListener("click", function(e) {
    scrollAnimation(document.scrollingElement.scrollTop, 0);
    e.stopPropagation();
    return false;
  });
  
  window.addEventListener('scroll', function(e) {
    let percent = document.scrollingElement.scrollTop / (document.scrollingElement.scrollHeight - document.scrollingElement.clientHeight) * 100;
    if (percent > 1 && !back2Top.classList.contains('back-top-active')) {
      back2Top.classList.add('back-top-active');
    }
    if (percent == 0) {
      back2Top.classList.remove('back-top-active');
    }
    if (back2TopText) {
      back2TopText.textContent = Math.floor(percent);
    }
  });

  
  let hasCacu = false;
  window.onresize = function() {
    if (window.width > 991) {
      calcuHeight();
    } else {
      hasCacu = false;
    }
  }

  function calcuHeight() {
    // 动态调整站点概览高度
    if (!hasCacu && back2Top.classList.contains('pisces') || back2Top.classList.contains('gemini')) {
      let sideBar = document.querySelector('.sidebar');
      let navUl = document.querySelector('#site_nav');
      sideBar.style = 'margin-top:' + (navUl.offsetHeight + navUl.offsetTop + 15) + 'px;';
      hasCacu = true;
    }
  }
  calcuHeight();
  
  let open = false, MOTION_TIME = 300, RIGHT_MOVE_DIS = '320px';

  if (drawerBox) {
    let rightMotions = document.querySelectorAll('.right-motion');
    let right = drawerBox.classList.contains('right');

    let transitionDir = right ? "transition.slideRightIn" : "transition.slideLeftIn";

    let openProp, closeProp;
    if (right) {
      openProp = {
        paddingRight: RIGHT_MOVE_DIS 
      };
      closeProp = {
        paddingRight: '0px'
      };
    } else {
      openProp = {
        paddingLeft: RIGHT_MOVE_DIS 
      };
      closeProp = {
        paddingLeft: '0px'
      };
    }

    drawerBox.onclick = function() {
      open = !open;
      window.Velocity(rightSideBar, 'stop');
      window.Velocity(viewport, 'stop');
      window.Velocity(rightMotions, 'stop');
      if (open) {
        window.Velocity(rightSideBar, {
          width: RIGHT_MOVE_DIS
        }, {
          duration: MOTION_TIME,
          begin: function() {
            window.Velocity(rightMotions, transitionDir,{ });
          }
        })
        window.Velocity(viewport, openProp,{
          duration: MOTION_TIME
        });
      } else {
        window.Velocity(rightSideBar, {
          width: '0px'
        }, {
          duration: MOTION_TIME,
          begin: function() {
            window.Velocity(rightMotions, {
              opacity: 0
            });
          }
        })
        window.Velocity(viewport, closeProp ,{
          duration: MOTION_TIME
        });
      }
      for (let i = 0; i < drawerBox.children.length; i++) {
        drawerBox.children[i].classList.toggle('muse-line');
      }
      drawerBox.classList.toggle(sideBarOpen);
    }
  }

  // 链接跳转
  let newWindow = 'false'
  if (newWindow === 'true') {
    let links = document.querySelectorAll('.post-body a')
    links.forEach(item => {
      if (!item.classList.contains('btn')) {
        item.setAttribute("target","_blank");
      }
    })
  }
  // 代码高亮
  hljs.initHighlightingOnLoad();

</script>
  </div>
</body>
<input hidden id="copy" />
<script>
  //拿来主义(真香)^_^，Clipboard 实现摘自掘金 https://juejin.im/post/5aefeb6e6fb9a07aa43c20af
  window.Clipboard = (function (window, document, navigator) {
    var textArea,
      copy;

    // 判断是不是ios端
    function isOS() {
      return navigator.userAgent.match(/ipad|iphone/i);
    }
    //创建文本元素
    function createTextArea(text) {
      textArea = document.createElement('textArea');
      textArea.value = text;
      textArea.style.width = 0;
      textArea.style.height = 0;
      textArea.clientHeight = 0;
      textArea.clientWidth = 0;
      document.body.appendChild(textArea);
    }
    //选择内容
    function selectText() {
      var range,
        selection;

      if (isOS()) {
        range = document.createRange();
        range.selectNodeContents(textArea);
        selection = window.getSelection();
        selection.removeAllRanges();
        selection.addRange(range);
        textArea.setSelectionRange(0, 999999);
      } else {
        textArea.select();
      }
    }

    //复制到剪贴板
    function copyToClipboard() {
      try {
        document.execCommand("Copy")
      } catch (err) {
        alert("复制错误！请手动复制！")
      }
      document.body.removeChild(textArea);
    }

    copy = function (text) {
      createTextArea(text);
      selectText();
      copyToClipboard();
    };

    return {
      copy: copy
    };
  })(window, document, navigator);

  function copyCode(e) {
    if (e.srcElement.tagName === 'SPAN' && e.srcElement.classList.contains('copy-code')) {
      let code = e.currentTarget.querySelector('code');
      var text = code.innerText;
      if (e.srcElement.textContent === '复制成功') {
        console.log('复制操作频率过高');
        return;
      }
      e.srcElement.textContent = '复制成功';
      (function (elem) {
        setTimeout(() => {
          if (elem.textContent === '复制成功') {
            elem.textContent = '复制代码'
          }
        }, 1000);
      })(e.srcElement)
      Clipboard.copy(text);
    }
  }

  let pres = document.querySelectorAll('pre');
  pres.forEach(pre => {
    let code = pre.querySelector('code');
    let copyElem = document.createElement('span');
    copyElem.classList.add('copy-code');
    copyElem.textContent = '复制代码';
    pre.appendChild(copyElem);
    pre.onclick = copyCode
  })
</script>
<script src="/media/js/motion.js"></script>

<script src="https://cdn.jsdelivr.net/gh/cferdinandi/smooth-scroll/dist/smooth-scroll.polyfills.min.js"></script>
<script>
  var scroll = new SmoothScroll('a[href*="#"]', {
    speed: 500
  });
</script>

<!-- <div class="search-mask" id="search_mask">
  <div class="search-box">
    <div class="search-title">
      <i class="fa fa-search"></i>
      <div class="input-box">
        <input type="text" placeholder="搜索">
      </div>
      <i class="fa fa-times-circle"></i>
    </div>
    <div class="result">
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/golang-bi-ji-shu-zu-qie-pian-map/"" data-c="
          &lt;h1 id=&#34;数组&#34;&gt;数组&lt;/h1&gt;
&lt;p&gt;数组的长度一旦定义了就不能动态增长，并且存储的数据类型必须相同。&lt;/p&gt;
&lt;h2 id=&#34;创建方法&#34;&gt;创建方法：&lt;/h2&gt;
&lt;p&gt;var 数组名 [长度]数据类型&lt;br&gt;
例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;package main
import &amp;quot;fmt&amp;quot;
 
func main(){
    var test [5]int //定义数组名字test，长度为5，数据类型为int的数组
    test[0] = 1    //赋值
    test[1] = 2   
    test[2] = 3
    test[3] = 4
    fmt.Println(test) 
    fmt.Println(test[2])
    fmt.Println(test[1:3]) //输出1到3的数组
    fmt.Println(test[0:]) //0到结尾
    fmt.Println(test[:3])  //0到3
 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##结果##
[1 2 3 4 0]
3
[2 3]
[1 2 3 4 0]
[1 2 3]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;数组的四种初始化方式&#34;&gt;数组的四种初始化方式&lt;/h2&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var s1 [3]int = [3]int{1,2,3}
fmt.Println(&amp;quot;s1&amp;quot;,s1)
var s2 [4]int = [...]int{5,6,7,8} //[...]是固定写法
fmt.Println(&amp;quot;s2&amp;quot;,s2)
var s3 = [2]int{9,10} //第一种的简化
fmt.Println(&amp;quot;s3&amp;quot;,s3)
var s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对
fmt.Println(&amp;quot;s4&amp;quot;,s4)
var s5 = new([5]int)
s5[4] =12
fmt.Println(&amp;quot;s5&amp;quot;,s5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##结果##
s1 [1 2 3]
s2 [5 6 7 8]
s3 [9 10]
s4 [40 41 42 43]
s5 [0 0 0 0 5]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;数组的遍历&#34;&gt;数组的遍历&lt;/h2&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对
fmt.Println(&amp;quot;s4&amp;quot;,s4)
     
for index,value := range s4{
fmt.Println(index,value)
}
 
#结果##
0 40
1 41
2 42
3 43
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;var s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对
for i := 0;i &amp;lt;len(s4);i++{
fmt.Println(i,s4[i])
}
 
#结果##
0 40
1 41
2 42
3 43
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;slice切片&#34;&gt;slice切片&lt;/h1&gt;
&lt;p&gt;1、切片是数组的引用&lt;br&gt;
2、切片的使用类似数组，如遍历&lt;br&gt;
3、切片的长度是可变的&lt;/p&gt;
&lt;h2 id=&#34;创建语法&#34;&gt;创建语法&lt;/h2&gt;
&lt;p&gt;var 切片名 []类型&lt;br&gt;
如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var qiepian []int
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;切片示例&#34;&gt;切片示例:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;###例子一&amp;lt;br&amp;gt;var suzhu [4]int = [...]int{5,6,7,8}
slice := suzhu[1:4] //1到4的值，不包含4
fmt.Println(suzhu)
fmt.Println(slice)
fmt.Println(&amp;quot;切片的容量&amp;quot;,cap(slice))
 
##结果
[5 6 7 8]
[6 7 8]
切片的容量 3&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;
###例子二、使用make创建切片
var slice []int = make([]int,4,10) //类型，大小(长度),容量（可选），容量必须大于长度
slice[0] = 10
slice[1] = 11
fmt.Println(slice)
 
##结果##
[10 11 0 0]
 
 
###例子三
var slice []int = []int {2,4,6}
fmt.Println(slice)
 
##结果##
2 4 6
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;切片的append追加&#34;&gt;切片的append追加&lt;/h2&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var slice []int = []int {2,4,6}
fmt.Println(slice)
//使用append直接追加切片内容（类似python list的append）
slice = append(slice,8,10)
fmt.Println(slice)
slice = append(slice,slice...) //追加切片，...是固定写法
fmt.Println(slice)
 
###结果###
[2 4 6]
[2 4 6 8 10]
[2 4 6 8 10 2 4 6 8 10]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;切片的copy操作&#34;&gt;切片的copy操作&lt;/h2&gt;
&lt;p&gt;使用copy内置函数&lt;br&gt;
例如:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var slice []int = []int {2,4,6}
fmt.Println(slice)
var slice2 []int = make([]int,5)
fmt.Println(slice2)
copy(slice2,slice) //将slice复制给slice2
fmt.Println(slice)
fmt.Println(slice2)
 
##结果##
[2 4 6]
[0 0 0 0 0]
[2 4 6]
[2 4 6 0 0]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;使用切片改变字符串的内容&#34;&gt;使用切片改变字符串的内容&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;var str string = &amp;quot;hello&amp;quot;
fmt.Println(str)
arr := []byte(str)
arr[1] = &#39;a&#39; //转成字符串
arr1 := []rune(str) //中文转换
arr1[0] = &#39;狗&#39;
fmt.Println(arr)
str = string(arr)
fmt.Println(str)
str = string(arr1)
fmt.Println(str)
 
##结果##
hello
[104 97 108 108 111]
hello
狗hello
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;map&#34;&gt;map&lt;/h1&gt;
&lt;p&gt;map是key-value数据结构(类似python的dict)&lt;br&gt;
map是无序存储的&lt;/p&gt;
&lt;p&gt;创建map语法&lt;br&gt;
var map 变量名 map[keytype]valuetype&lt;/p&gt;
&lt;p&gt;如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var m1 map[string]string
var m2 map[string]int
var m3 map[int]string
var m4 map[string]map[string]string
```　　

使用例子：
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;package main&lt;br&gt;
import &amp;quot;fmt&amp;quot;&lt;/p&gt;
&lt;p&gt;func main(){&lt;br&gt;
var m1 map[string]string&lt;br&gt;
//在使用map前,需要先make，make的作用技术给map分配数据空间&lt;br&gt;
m1 = make(map[string]string)&lt;br&gt;
m2 := map[string]string{  //使用方式二&lt;br&gt;
&amp;quot;a1&amp;quot; : &amp;quot;q1&amp;quot;,&lt;br&gt;
&amp;quot;a2&amp;quot; : &amp;quot;a2&amp;quot;,&lt;br&gt;
}&lt;br&gt;
m1[&amp;quot;s1&amp;quot;] = &amp;quot;亚索&amp;quot;&lt;br&gt;
m1[&amp;quot;s2&amp;quot;] = &amp;quot;盖伦&amp;quot;&lt;br&gt;
fmt.Println(m1)&lt;br&gt;
fmt.Println(m1[&amp;quot;s1&amp;quot;])&lt;br&gt;
fmt.Println(m2)&lt;br&gt;
}&lt;/p&gt;
&lt;p&gt;###结果###&lt;br&gt;
map[s1:亚索 s2:盖伦]&lt;br&gt;
亚索&lt;br&gt;
map[a1:q1 a2:a2]&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
 

map的增删改查
增、改
map[key] = value //没有就增加，存在就修改

删
delete(map,key)

查
map[key]   //对应的value，和python的dict一样&lt;/code&gt;&lt;/pre&gt;
">golang笔记-数组、切片、map</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/k8s-hpa-dan-xing-kuo-rong-pei-zhi/"" data-c="
          &lt;pre&gt;&lt;code&gt;HPA全称Horizontal Pod Autoscaling，是K8s实现pod自动水平扩容缩容的特性，这个特性使整个kubernetes集群马上高大上起来了。
要使用HPA也不是这么简单的，HPA api分v1、v2beta1、v2bate2三种，v1只支持通过CPU衡量扩缩容，v2bate1加入针对内存作为度量，v2bate2可以用customer metrics例如网络等，所以v2bate1开始才比较实用。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;要使用HPA必须要开启以下两个特性：&lt;/p&gt;
&lt;p&gt;Aggregation Layer 聚合层，通过与核心的apiserver分离，实现自定义的扩展功能&lt;br&gt;
metrics-server 数据收集，能够收集pod、node等实时运行指标（cpu、内存），给k8s集群使用，例如kubectl top命令、HPA&lt;br&gt;
比较老的版本使用heapster&lt;/p&gt;
&lt;h1 id=&#34;aggregation-layer&#34;&gt;Aggregation Layer&lt;/h1&gt;
&lt;p&gt;要打开Aggregation Layer，需要配置一下apiserver，增加相关认证证书。认证流程是client发起请求到apiserver，apiserver与aggergated apiserver建立tls安全链接，把请求proxy到aggergated apiserver，继续进行–requestheader-*参数的相关认证。&lt;/p&gt;
&lt;p&gt;认证流程&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1589865988979.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;需要生成aggregate使用的证书，参考cfssl生成证书方法，proxy-client-cert-file的CN需要与requestheader-allowed-names匹配。&lt;br&gt;
在apiserver增加如下启动参数&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--requestheader-client-ca-file=/etc/kubernetes/pki/agg-ca.pem
--proxy-client-cert-file=/etc/kubernetes/pki/aggregate.pem
--proxy-client-key-file=/etc/kubernetes/pki/aggregate-key.pem
--requestheader-allowed-names=aggregator
--requestheader-extra-headers-prefix=X-Remote-Extra-
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
#如果kube-proxy没有在Master上面运行，还需要配置
--enable-aggregator-routing=true
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;metrics-server&#34;&gt;metrics server&lt;/h1&gt;
&lt;p&gt;从k8s 1.8开始，集群的资源使用情况都通过metrics api收集，例如容器CPU、内存。这些指标可用于kuberctl top或者k8s的HPA等特性。&lt;br&gt;
metrice server可以在github找到并部署&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/kubernetes-incubator/metrics-server
cd metrics-server
kubectl create -f deploy/1.8+/
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;注1：metrics-server默认使用node的主机名，但是coredns里面没有物理机主机名的解析，一种是部署的时候添加一个参数： –kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP,第二种是使用dnsmasq构建一个上游的dns服务
注2：kubelet 的10250端口使用的是https协议，连接需要验证tls证书。可以在metrics server启动命令添加参数–kubelet-insecure-tls不验证客户端证书
注3：yaml文件中的image地址k8s.gcr.io/metrics-server-amd64:v0.3.3 需要梯子，需要改成中国可以访问的image地址，可以使用aliyun的。这里使用hub.docker.com里的google镜像地址 image: mirrorgooglecontainers/metrics-server-amd64:v0.3.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;成功运行kubectl top命令&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ubuntu@k8s-dev-m1:~/k8sssl/agglayer$ kubectl top nodes
NAME                   CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-dev-node2          103m         5%      2696Mi                 72%       
k8s-dev-node3.bxr.cn   115m      2%     5312Mi                  67%  
k8s-dev-node4          57m          2%       2634Mi                  70%     
k8s-dev-node5          148m         7%       2443Mi                  65%
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;hpa&#34;&gt;HPA&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;有了metrics就可以开始使用HPA特性了。hpa有几个特点
deploy或者rs等需要设置resources才能使用hpa
如果我们创建一个HPA controller，它会每隔15s（可以通过–horizontal-pod-autoscaler-sync-period修改）检测一次hpa定义的资源与实际资源使用情况，如果达到阀值就会调整pod数量。
HPA设置的阀值不是绝对的，允许设置一个浮动范围，–horizontal-pod-autoscaler-tolerance默认是0.1
pod调整算法 desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]
scale有一个窗口期，期间每次变化会记录下来，选择最优的调整建议再进行scale，这样可以保证资源平滑变动，通过–horizontal-pod-autoscaler-downscale-stabilization设定，默认5分钟。
通过hpa调整新增的pod不会马上ready，这时候收集的metrics就不准，为了减少影响，hpa一开始不会收集新pod的metrics。通过–horizontal-pod-autoscaler-initial-readiness-delay（默认30s）和 –horizontal-pod-autoscaler-cpu-initialization-period（默认为 5 分钟）调整
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1589866316410.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
示例hpa.yml:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-test
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization 
        averageUtilization: 160
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面的示例包括cpu和memory指标，averageUtilization这个百分比是根据deployment的resources.requests计算的。例如有deployment限制requests是512Mi，replicas是2，实际pod1用了612Mi，pod2用了598Mi，计算公式是 (612+598)/2/512 = 118%&lt;/p&gt;
&lt;p&gt;查看hpa的情况，targets第一个是memory，第二个是cpu指标，REPLICAS是根据计算后的当前pod数&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ubuntu@k8s-m1:~/k8s/hpa$ kubectl get hpa
NAME       REFERENCE            TARGETS             MINPODS   MAXPODS   REPLICAS   AGE
hpa-test   Deployment/podinfo   120%/160%, 6%/75%   2         4         3          97m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;官方示例还包括packets-per-second、requests-per-second这些指标，需要进一步验证&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: AverageUtilization
        averageUtilization: 50
  - type: Pods
    pods:
      metric:
        name: packets-per-second
      targetAverageValue: 1k
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        name: main-route
      target:
        kind: Value
        value: 10k
status:
  observedGeneration: 1
  lastScaleTime: &amp;lt;some-time&amp;gt;
  currentReplicas: 1
  desiredReplicas: 1
  currentMetrics:
  - type: Resource
    resource:
      name: cpu
    current:
      averageUtilization: 0
      averageValue: 0
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        name: main-route
      current:
        value: 10k

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#create-horizontal-pod-autoscaler&#34;&gt;官方hpa参数&lt;/a&gt;&lt;/p&gt;
">k8s hpa弹性扩容配置</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/beego-jing-tai-wen-jian-jia-zai-lu-jing-xiu-gai/"" data-c="
          &lt;p&gt;Go 语言内部其实已经提供了 http.ServeFile，通过这个函数可以实现静态文件的服务。&lt;/p&gt;
&lt;p&gt;beego 针对这个功能进行了一层封装，通过下面的方式进行静态文件注册：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;beego.SetStaticPath(&amp;quot;/static&amp;quot;,&amp;quot;public&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第一个参数是路径，url 路径信息&lt;br&gt;
第二个参数是静态文件目录（相对应用所在的目录）&lt;br&gt;
beego 支持多个目录的静态文件注册，用户可以注册如下的静态文件目录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;beego.SetStaticPath(&amp;quot;/images&amp;quot;,&amp;quot;images&amp;quot;)
beego.SetStaticPath(&amp;quot;/css&amp;quot;,&amp;quot;css&amp;quot;)
beego.SetStaticPath(&amp;quot;/js&amp;quot;,&amp;quot;js&amp;quot;)
```　　

设置了如上的静态目录之后，用户访问 /images/login/login.png，那么就会访问应用对应的目录下面的 images/login/login.png 文件。

如果是访问 /static/img/logo.png，那么就访问 public/img/logo.png文件。

默认情况下 beego 会判断目录下文件是否存在，不存在直接返回 404 页面，如果请求的是 index.html，那么由于 http.ServeFile 默认是会跳转的，不提供该页面的显示。

因此 beego 可以设置 beego.BConfig.WebConfig.DirectoryIndex=true 这样来使得显示 index.html 页面。而且开启该功能之后，用户访问目录就会显示该目录下所有的文件列表。&lt;/code&gt;&lt;/pre&gt;
">beego静态文件加载路径修改</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/yong-hu-kong-jian-gua-zai-ceph-wen-jian-xi-tong/"" data-c="
          &lt;p&gt;Ceph v0.55 及后续版本默认开启了 cephx 认证。从用户空间（ FUSE ）挂载一 Ceph 文件系统前，确保客户端主机有一份 Ceph 配置副本、和具备 Ceph 元数据服务器能力的密钥环。&lt;/p&gt;
&lt;p&gt;1、 在客户端主机上，把监视器主机上的 Ceph 配置文件拷贝到 /etc/ceph/ 目录下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo mkdir -p /etc/ceph
sudo scp {user}@{server-machine}:/etc/ceph/ceph.conf /etc/ceph/ceph.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2、 在客户端主机上，把监视器主机上的 Ceph 密钥环拷贝到 /etc/ceph 目录下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo scp {user}@{server-machine}:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3、 确保客户端机器上的 Ceph 配置文件和密钥环都有合适的权限位，如 chmod 644 。&lt;br&gt;
cephx 如何配置请参考 CEPHX 配置参考。&lt;br&gt;
要把 Ceph 文件系统挂载为用户空间文件系统，可以用 ceph-fuse 命令，例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo mkdir /home/usernname/cephfs
sudo ceph-fuse -m 192.168.0.1:6789 /home/username/cephfs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;详情见 &lt;a href=&#34;http://docs.ceph.org.cn/man/8/ceph-fuse/&#34;&gt;ceph-fuse&lt;/a&gt;&lt;/p&gt;
">用户空间挂载 CEPH 文件系统</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/linux-ru-he-dao-ru-zi-ding-yi-zheng-shu/"" data-c="
          &lt;h1 id=&#34;问题&#34;&gt;问题&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;curl https://192.168.0.200:8443
提示curl: (60) Peer&#39;s Certificate issuer is not recognized.
curl: (60) Peer&#39;s Certificate issuer is not recognized.
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a &amp;quot;bundle&amp;quot;
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn&#39;t adequate, you can specify an alternate file
 using the --cacert option.
If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
If you&#39;d like to turn off curl&#39;s verification of the certificate, use
 the -k (or --insecure) option.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;mac-os-x&#34;&gt;Mac OS X&lt;/h1&gt;
&lt;h2 id=&#34;添加证书&#34;&gt;添加证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ~/new-root-certificate.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;移除证书&#34;&gt;移除证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sudo security delete-certificate -c &amp;quot;&amp;lt;name of existing certificate&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;windows&#34;&gt;Windows&lt;/h1&gt;
&lt;h2 id=&#34;添加证书-2&#34;&gt;添加证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;certutil -addstore -f &amp;quot;ROOT&amp;quot; new-root-certificate.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;移除证书-2&#34;&gt;移除证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;certutil -delstore &amp;quot;ROOT&amp;quot; serial-number-hex
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;linux-ubuntu-debian&#34;&gt;Linux (Ubuntu, Debian)&lt;/h1&gt;
&lt;h2 id=&#34;添加证书-3&#34;&gt;添加证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;复制 CA 文件到目录： /usr/local/share/ca-certificates/
执行:
sudo cp foo.crt /usr/local/share/ca-certificates/foo.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;更新-ca-证书库&#34;&gt;更新 CA 证书库:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sudo update-ca-certificates
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;移除证书-3&#34;&gt;移除证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Remove your CA.

Update the CA store:

sudo update-ca-certificates --fresh

Restart Kerio Connect to reload the certificates in the 32-bit versions or Debian 7.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;linux-centos-6&#34;&gt;Linux (CentOs 6)&lt;/h1&gt;
&lt;h2 id=&#34;添加证书-4&#34;&gt;添加证书：&lt;/h2&gt;
&lt;p&gt;// root-ca.crt 为ca证书&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;安装 ca-certificates package:

yum install ca-certificates

启用dynamic CA configuration feature:

update-ca-trust force-enable

Add it as a new file to /etc/pki/ca-trust/source/anchors/:
cp root-ca.crt /etc/pki/ca-trust/source/anchors/

执行:

update-ca-trust extract

Restart Kerio Connect to reload the certificates in the 32-bit version.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;linux-centos-5&#34;&gt;Linux (CentOs 5)&lt;/h1&gt;
&lt;h2 id=&#34;添加证书-5&#34;&gt;添加证书：&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Append your trusted certificate to file /etc/pki/tls/certs/ca-bundle.crt

cat foo.crt &amp;gt;&amp;gt; /etc/pki/tls/certs/ca-bundle.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;测试访问&#34;&gt;测试访问&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt; curl -v &amp;quot;https:/gitlab.test.com/micro-lib/server?go-get=1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
">linux如何导入自定义证书</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/istio-ke-hu-duan-yuan-di-zhi-ru-he-xian-shi/"" data-c="
          &lt;h3 id=&#34;前提&#34;&gt;前提&lt;/h3&gt;
&lt;p&gt;由于istio部署到IDC、非云环境无法获取客户端真实IP地址，web应用无法使用该组件，早期版本的istio做了很多测试，最终无法实现获取remote client address；&lt;/p&gt;
&lt;h3 id=&#34;环境&#34;&gt;环境&lt;/h3&gt;
&lt;p&gt;kubernetes版本：k8s-1.16.9&lt;br&gt;
istio版本：1.5&lt;/p&gt;
&lt;h3 id=&#34;方法&#34;&gt;方法&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kubectl -n istio-system edit  deployments. istio-pilot
添加如下：
       env:
       - name: PILOT_SIDECAR_USE_REMOTE_ADDRESS
          value: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以下是github相应的issue&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588232244321.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;其他测试&#34;&gt;其他测试&lt;/h3&gt;
&lt;p&gt;istio-1.5版本回归单体，各个组件优化了很多，后期测试http链接与tcp链接应用&lt;/p&gt;
">istio-客户端源地址如何显示</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/k8s-tong-guo-helm-bu-shu-elk-73/"" data-c="
          &lt;h1 id=&#34;前提&#34;&gt;前提：&lt;/h1&gt;
&lt;p&gt;在kubernetes集群中部署elk组件，es集群部署3个节点，kibana部署一个，容器数据持久化需要storageclass&lt;/p&gt;
&lt;p&gt;依赖：&lt;br&gt;
Helm&lt;br&gt;
Persistent Volumes&lt;/p&gt;
&lt;h1 id=&#34;准备配置&#34;&gt;准备配置&lt;/h1&gt;
&lt;p&gt;由于repo在线安装太慢，建议下载char本地修改参数后安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/elastic/helm-charts.git
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;部署-elk&#34;&gt;部署 ELK&lt;/h1&gt;
&lt;h2 id=&#34;创建elk命名空间&#34;&gt;创建elk命名空间&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#cat elk-ns.yml
apiVersion: v1
kind: Namespace
metadata:
  name: elk
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;部署elasticsearch&#34;&gt;部署elasticsearch&lt;/h2&gt;
&lt;p&gt;cd helm-charts/elasticsearch&lt;/p&gt;
&lt;p&gt;helm install --namespace=elk  --name=elasticsearch .&lt;/p&gt;
&lt;h2 id=&#34;部署-kibana&#34;&gt;部署 Kibana&lt;/h2&gt;
&lt;p&gt;cd helm-charts/kibana&lt;/p&gt;
&lt;p&gt;helm install --namespace=elk --name=kibana .&lt;br&gt;
通过 kubectl get deploy 和 pod 了解部署状态；&lt;/p&gt;
&lt;h1 id=&#34;小知识&#34;&gt;小知识&lt;/h1&gt;
&lt;p&gt;Kibana 直接通过 K8S 内部 DNS 域名 访问 ES。&lt;/p&gt;
&lt;p&gt;查看容器内的配置&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl  exec kibana-kibana-7cbc5db55c-6qct7 -c kibana -- cat /usr/share/kibana/config/kibana.yml

# Default Kibana configuration for docker target
server.name: kibana
server.host: &amp;quot;0&amp;quot;
elasticsearch.hosts: [ &amp;quot;http://elasticsearch:9200&amp;quot; ]
xpack.monitoring.ui.container.elasticsearch.enabled: true
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;kibana-添加-ingress&#34;&gt;Kibana 添加 Ingress&lt;/h1&gt;
&lt;p&gt;通过 Ingress 添加访问入口&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kibana
  namespace: default
spec:
  rules:
  - host: &amp;lt;YourDomain&amp;gt;  ## 访问 Kibana 的域名 
    http:
      paths:
      - backend:
          serviceName: kibana-kibana
          servicePort: 5601
        path: /
 status:
  loadBalancer:
    ingress:
    - ip: &amp;lt;YourLoadBalancerIP&amp;gt;  ## LB 的 IP
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;访问测试&#34;&gt;访问测试&lt;/h1&gt;
&lt;p&gt;访问域名，即可打开 Kibana 7.3 版本；&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588231324831.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;查看集群的运行状态&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588231313286.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;p&gt;也可以通过命令行查看&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~$ curl  -s &amp;lt;YourESHost&amp;gt;/_cluster/health | jq .
{
  &amp;quot;cluster_name&amp;quot;: &amp;quot;elasticsearch&amp;quot;,
  &amp;quot;status&amp;quot;: &amp;quot;yellow&amp;quot;,
  &amp;quot;timed_out&amp;quot;: false,
  &amp;quot;number_of_nodes&amp;quot;: 3,
  &amp;quot;number_of_data_nodes&amp;quot;: 3,
  &amp;quot;active_primary_shards&amp;quot;: 19,
  &amp;quot;active_shards&amp;quot;: 35,
  &amp;quot;relocating_shards&amp;quot;: 0,
  &amp;quot;initializing_shards&amp;quot;: 0,
  &amp;quot;unassigned_shards&amp;quot;: 3,
  &amp;quot;delayed_unassigned_shards&amp;quot;: 0,
  &amp;quot;number_of_pending_tasks&amp;quot;: 0,
  &amp;quot;number_of_in_flight_fetch&amp;quot;: 0,
  &amp;quot;task_max_waiting_in_queue_millis&amp;quot;: 0,
  &amp;quot;active_shards_percent_as_number&amp;quot;: 92.10526315789474
}
&lt;/code&gt;&lt;/pre&gt;
">K8S通过helm 部署 ELK 7.3</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/kubesphere-an-zhuang-shi-yong-ti-yan/"" data-c="
          &lt;p&gt;最近又出来个kubesphere的工具用来管理k8s，今天特意来安装体验下；&lt;br&gt;
github地址：https://github.com/pixiake/ks-installer&lt;/p&gt;
&lt;p&gt;官方使用文档：https://kubesphere.io/docs/advanced-v2.0/zh-CN/installation/all-in-one/&lt;/p&gt;
&lt;p&gt;先放上安装效果图，UI界面还是很清爽的：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588152934567.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;当前环境&#34;&gt;当前环境：&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;k8s集群已经安装完成，用kubesphere管理现有的k8s集群；

k8s版本为1.14  

系统为centos7.6

kubesphere使用要求：

kubernetes version &amp;gt; 1.13.0

helm version &amp;gt; 2.10.0

a default storage class must be in kubernetes cluster
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装完成后默认用户名密码：&lt;/p&gt;
&lt;p&gt;用户名：admin&lt;/p&gt;
&lt;p&gt;密码：P@88w0rd&lt;/p&gt;
&lt;h3 id=&#34;开始安装&#34;&gt;开始安装&lt;/h3&gt;
&lt;p&gt;安装步骤大概记录：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create ns kubesphere-system
kubectl create ns kubesphere-monitoring-system

#访问etcd用到的secret
kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  --from-file=etcd-client-ca.crt=ca.pem  --from-file=etcd-client.crt=etcd-key.pem  --from-file=etcd-client.key=etcd.pem

#管理k8s用到的secret

kubectl -n kubesphere-system create secret generic kubesphere-ca  --from-file=ca.crt=ca.pem --from-file=ca.key=ca-key.pem

#clone好github项目，执行下面的这条命令

cd deploy
kubectl apply -f kubesphere-installer.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;执行完上面的命令，可以通过下面的命令，查看安装过程日志&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l job-name=kubesphere-installer -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -f
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看安装结果，STATUS跟下面保持一致才说明安装成功&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@ks-allinone deploy]# kubectl get pods -n kubesphere-system
NAME                                     READY   STATUS      RESTARTS   AGE
ks-account-6db466d8dc-srrwj              1/1     Running     0          149m
ks-apigateway-7d77cb9495-jzmg6           1/1     Running     0          170m
ks-apiserver-f8469fd47-b58rm             1/1     Running     0          166m
ks-console-54c849bdc9-dfkbf              1/1     Running     0          168m
ks-console-54c849bdc9-z2d5q              1/1     Running     0          168m
ks-controller-manager-569456b4cd-gngm5   1/1     Running     0          170m
ks-docs-6bbdcc9bfb-6jldz                 1/1     Running     0          3h7m
kubesphere-installer-7ph6l               0/1     Completed   1          3h11m
openldap-5c986c5bff-rzqwv                1/1     Running     0          3h25m
redis-76dc4db5dd-lv6kg                   1/1     Running     0          149m
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;安装过程出现的错误&#34;&gt;安装过程出现的错误&lt;/h3&gt;
&lt;p&gt;1.在安装的时提示metrics-server已经安装，导致安装中断；&lt;/p&gt;
&lt;p&gt;解析办法：在kubesphere-installer.yaml的configMap增加配置：metrics_server_enable: False（默认是没有的）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
data:
  ks-config.yaml: |
    kube_apiserver_host: 10.10.5.208:6443
    etcd_tls_enable: True
    etcd_endpoint_ips: 10.10.5.169,10.10.5.183,10.10.5.184
    disableMultiLogin: True
    elk_prefix: logstash
    metrics_server_enable: False
  #  local_registry: 192.168.1.2:5000
kind: ConfigMap
metadata:
  name: kubesphere-config
  namespace: kubesphere-system
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;增加Ingress配置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubesphere
  namespace: kubesphere-system
  annotations:
    #kubernetes.io/ingress.class: traefik
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: ks.staplescn.com
    http:
      paths:
      - path:
        backend:
          serviceName: ks-console
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;访问界面：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588153130667.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
">kubesphere安装使用体验</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian/"" data-c="
          &lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;
&lt;p&gt;我们通常的开发流程是，在本地开发完成应用之后，使用git作为版本管理工具，将本地代码提交到类似Github这样的仓库中做持久化存储，当我们可能来自多个仓库、可能涉及到多个中间件作为底层依赖一起部署到生产环境中时，相信不少在大中型企业工作的小伙伴都知道公司内部通常会有发布系统，那么云原生技术栈中有没有为我们提供类似的发布系统呢？档案是肯定的，而且不乏竞争者，业内知名的有knavtie Build/jekinsX/spinnaker/orgo/tekton，其中，tekon凭借其众多优良特性在一众竞争者中胜出，成为领域内的事实标准，今天我们就来揭开Tekton的神秘面纱。&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588236885419.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;正文&#34;&gt;正文&lt;/h1&gt;
&lt;h3 id=&#34;什么是tekton&#34;&gt;什么是Tekton&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588236902661.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
那Tekton都提供了哪些CRD呢？&lt;br&gt;
•	Task：顾名思义，task表示一个构建任务，task里可以定义一系列的steps，例如编译代码、构建镜像、推送镜像等，每个step实际由一个Pod执行。&lt;br&gt;
•	TaskRun：task只是定义了一个模版，taskRun才真正代表了一次实际的运行，当然你也可以自己手动创建一个taskRun，taskRun创建出来之后，就会自动触发task描述的构建任务。&lt;br&gt;
•	Pipeline：一个或多个task、PipelineResource以及各种定义参数的集合。&lt;br&gt;
•	PipelineRun：类似task和taskRun的关系，pipelineRun也表示某一次实际运行的pipeline，下发一个pipelineRun CRD实例到kubernetes后，同样也会触发一次pipeline的构建。&lt;br&gt;
•	PipelineResource：表示pipeline input资源，比如github上的源码，或者pipeline output资源，例如一个容器镜像或者构建生成的jar包等。&lt;br&gt;
他们大概有如下图所示的关系：&lt;br&gt;
官方介绍：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tekton 是一个功能强大且灵活的 Kubernetes 原生开源框架，用于创建持续集成和交付（CI/CD）系统。通过抽象底层实现细节，用户可以跨多云平台和本地系统进行构建、测试和部署。
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;个人理解：
•	以yaml文件编排应用构建及部署流程
•	knavtive build模块升级版，社区最终采用 Tekton 替代 knavtive Build作为云原生领域的CI/CD 解决方案
•	标准化CI/CD流水线构建、测试及部署流程的工具
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tekton在一众竞争对手的比拼中PK胜出：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588236947460.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;下面就让我们一起来深入详细了解下Tekton到底怎么玩。&lt;br&gt;
Tekton Pipeline中有5类对象，核心理念是通过定义yaml定义构建过程。&lt;br&gt;
•	Task：一个任务的执行模板，用于描述单个任务的构建过程&lt;br&gt;
•	TaskRun：需要通过定义TaskRun任务去运行Task。&lt;br&gt;
•	Pipeline：包含多个Task,并在此基础上定义input和output,input和output以PipelineResource作为交付。&lt;br&gt;
•	PipelineRun：需要定义PipelineRun才会运行Pipeline。&lt;br&gt;
•	PipelineResource：可用于input和output的对象集合。&lt;/p&gt;
&lt;h3 id=&#34;task&#34;&gt;Task&lt;/h3&gt;
&lt;p&gt;Task 就是一个任务执行模板，之所以说 Task 是一个模板是因为 Task 定义中可以包含变量，Task 在真正执行的时候需要给定变量的具体值。如果把 Tekton 的 Task 有点儿类似于定义一个函数，Task 通过 inputs.params 定义需要哪些入参，并且每一个入参还可以指定默认值。Task 的 steps 字段表示当前 Task 是有哪些步骤组成的，每一个步骤具体就是基于镜像启动一个 container 执行一些操作，container 的启动参数可以通过 Task 的入参使用模板语法进行配置。下面是一个Demo：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588236983683.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;taskrun&#34;&gt;TaskRun&lt;/h3&gt;
&lt;p&gt;Task 定义好以后是不能执行的，就像一个函数定义好以后需要调用才能执行一样。所以需要再定义一个 TaskRun 去执行 Task。&lt;br&gt;
TaskRun 主要是负责设置 Task 需要的参数，并通过 taskRef 字段引用要执行的 Task。下面是一个Demo：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237005091.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;但是在实际使用过程中，我们一般很少使用TaskRun，因为它只能给不一个Task 传参，Tekton提供了给多个Task同时传参的解决方案Pipeline和PipelineRun，且看下文详解，这里只是多嘴一下，这个TaskRun很少使用，稍微了解下就可以了。&lt;/p&gt;
&lt;h3 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h3&gt;
&lt;p&gt;一个 TaskRun 只能执行一个 Task，当需要编排多个 Task 的时候就需要 Pipeline 出马了。Pipeline 是一个编排 Task 的模板。Pipeline 的 params 声明了执行时需要的入参。 Pipeline 的 spec.tasks 定义了需要编排的 Task。Tasks 是一个数组，数组中的 task 并不是通过数组声明的顺序去执行的，而是通过 runAfter 来声明 task 执行的顺序。Tekton controller 在解析 CRD 的时候会解析 Task 的顺序，然后根据 runAfter 设置生成的依次树依次去执行。Pipeline 在编排 Task 的时候需要给每一个 Task 传入必须的参数，这些参数的值可以来自 Pipeline 自身的 params 设置。下面是一个Demo：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237033509.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237113343.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pipelinerun&#34;&gt;PipelineRun&lt;/h3&gt;
&lt;p&gt;和 Task 一样 Pipeline 定义完成以后也是不能直接执行的，需要 PipelineRun 才能执行 Pipeline。PipelineRun 的主要作用是给 Pipeline 传入必要的入参，并执行 Pipeline。下面是一个Demo：&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237145814.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237151158.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;pipelineresource&#34;&gt;PipelineResource&lt;/h3&gt;
&lt;p&gt;可能你还想在 Task 之间共享资源，这就是 PipelineResource 的作用。比如我们可以把 git 仓库信息放在 PipelineResource 中。这样所有 Task 就可以共享这些信息了。&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237189249.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237194640.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588237221271.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;实战&#34;&gt;实战&lt;/h1&gt;
&lt;p&gt;关于Tekton的实战，可以参考Github里面的这个完整的Demo，里面是一个go语言吧编写的web服务，接口可以打印&amp;quot;Hello world&amp;quot;。时间有限，就不做演示了，感兴趣的可以在自己的k8s集群上面跑一下感受一下，相关的yaml文件也可以拷贝下来，作为后面改写的模板。&lt;br&gt;
准备 PIpeline 的资源&lt;br&gt;
kubectl apply -f tasks/source-to-image.yaml -f tasks/deploy-using-kubectl.yaml  -f resources/picalc-git.yaml -f image-secret.yaml -f pipeline-account.yaml -f pipeline/build-and-deploy-pipeline.yaml&lt;br&gt;
执行 create 把 pipelieRun 提交到 Kubernetes 集群。之所以这里使用 create 而不是使用 apply 是因为 PIpelineRun 每次都会创建一个新的，kubectl 的 create 指令会基于 generateName 创建新的 PIpelineRun 资源。&lt;br&gt;
kubectl create -f run/picalc-pipeline-run.yaml&lt;/p&gt;
&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;
&lt;p&gt;Tekton以K8S为依托，成为云原生领域CI/CD的事实性标准，帮助我们提高云原生环境下的应用构建和部署效率。&lt;br&gt;
来一张图对全文做一个简单的总结：&lt;/p&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588236724398.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/tekton/&#34;&gt;Tekton官网&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/knative-sample/tekton-knative/tree/b1.0?spm=ata.13261165.0.0.21213a182xyMm5&amp;amp;file=b1.0&#34;&gt;Tekton Demo&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
">kubernetes Tekton-CI/CD 持续集成流水线</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/kubernetes搭建rook-ceph/"" data-c="
          &lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;
&lt;p&gt;Rook官网：https://rook.io&lt;br&gt;
Rook是云原生计算基金会(CNCF)的孵化级项目.&lt;br&gt;
Rook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。&lt;br&gt;
至于CEPH，官网在这：https://ceph.com/&lt;br&gt;
ceph官方提供的helm部署，至今我没成功过，所以转向使用rook提供的方案&lt;br&gt;
有道笔记原文：http://note.youdao.com/noteshare?id=281719f1f0374f787effc90067e0d5ad&amp;amp;sub=0B59EA339D4A4769B55F008D72C1A4C0&lt;/p&gt;
&lt;h1 id=&#34;环境&#34;&gt;环境&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;centos 7.5
kernel 4.18.7-1.el7.elrepo.x86_64
docker 18.06
kubernetes v1.12.2
    kubeadm部署：
        网络: canal
        DNS: coredns
    集群成员：    
    192.168.1.1 kube-master
    192.168.1.2 kube-node1
    192.168.1.3 kube-node2
    192.168.1.4 kube-node3
    192.168.1.5 kube-node4

所有node节点准备一块200G的磁盘：/dev/sdb
kubernetes搭建rook-ceph
&lt;/code&gt;&lt;/pre&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;1&#34;&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233297037.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h1 id=&#34;准备工作&#34;&gt;准备工作&lt;/h1&gt;
&lt;p&gt;所有节点开启ip_forward&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;  /etc/sysctl.d/ceph.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;sysctl -p&lt;/p&gt;
&lt;h1 id=&#34;开始部署operator&#34;&gt;开始部署Operator&lt;/h1&gt;
&lt;h2 id=&#34;部署rook-operator&#34;&gt;部署Rook Operator&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;cd $HOME
git clone https://github.com/rook/rook.git

cd rook
cd cluster/examples/kubernetes/ceph
kubectl apply -f operator.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;figure data-type=&#34;image&#34; tabindex=&#34;2&#34;&gt;&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233371696.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/figure&gt;
&lt;h2 id=&#34;查看operator的状态&#34;&gt;查看Operator的状态&lt;/h2&gt;
&lt;p&gt;执行apply之后稍等一会&lt;br&gt;
operator会在集群内的每个主机创建两个pod:rook-discover,rook-ceph-agent&lt;/p&gt;
&lt;p&gt;kubectl -n rook-ceph-system get pod -o wide&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233523024.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;给节点打标签&#34;&gt;给节点打标签&lt;/h2&gt;
&lt;p&gt;运行ceph-mon的节点打上：ceph-mon=enabled&lt;br&gt;
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-mon=enabled&lt;br&gt;
运行ceph-osd的节点，也就是存储节点，打上：ceph-osd=enabled&lt;br&gt;
kubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-osd=enabled&lt;br&gt;
运行ceph-mgr的节点，打上：ceph-mgr=enabled&lt;br&gt;
mgr只能支持一个节点运行，这是ceph跑k8s里的局限&lt;br&gt;
kubectl label nodes kube-node1 ceph-mgr=enabled&lt;/p&gt;
&lt;h2 id=&#34;配置clusteryaml文件&#34;&gt;配置cluster.yaml文件&lt;/h2&gt;
&lt;p&gt;官方配置文件详解：https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html&lt;br&gt;
文件中有几个地方要注意：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dataDirHostPath: 这个路径是会在宿主机上生成的，保存的是ceph的相关的配置文件，再重新生成*集群的时候要确保这个目录为空，否则mon会无法启动&lt;/li&gt;
&lt;li&gt;useAllDevices: 使用所有的设备，建议为false，否则会把宿主机所有可用的磁盘都干掉&lt;/li&gt;
&lt;li&gt;useAllNodes：使用所有的node节点，建议为false，肯定不会用k8s集群内的所有node来搭建ceph的&lt;/li&gt;
&lt;li&gt;databaseSizeMB和journalSizeMB：当磁盘大于100G的时候，就注释这俩项就行了&lt;br&gt;
本次实验用到的 cluster.yaml 文件内容如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;configmaps&amp;quot;]
  verbs: [ &amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;delete&amp;quot; ]
---
# Allow the operator to create resources in this cluster&#39;s namespace
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster-mgmt
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-cluster-mgmt
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph-system
---
# Allow the pods in this namespace to work with configmaps
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-cluster
subjects:
- kind: ServiceAccount
  name: rook-ceph-cluster
  namespace: rook-ceph
---
apiVersion: ceph.rook.io/v1beta1
kind: Cluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
    # v12 is luminous, v13 is mimic, and v14 is nautilus.
    # RECOMMENDATION: In production, use a specific version tag instead of the general v13 flag, which pulls the latest release and could result in different
    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
    image: ceph/ceph:v13
    # Whether to allow unsupported versions of Ceph. Currently only luminous and mimic are supported.
    # After nautilus is released, Rook will be updated to support nautilus.
    # Do not set to true in production.
    allowUnsupported: false
  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).
  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
  # In Minikube, the &#39;/data&#39; directory is configured to persist across reboots. Use &amp;quot;/data/rook&amp;quot; in Minikube environment.
  dataDirHostPath: /var/lib/rook
  # The service account under which to run the daemon pods in this cluster if the default account is not sufficient (OSDs)
  serviceAccount: rook-ceph-cluster
  # set the amount of mons to be started
  # count可以定义ceph-mon运行的数量，这里默认三个就行了
  mon:
    count: 3
    allowMultiplePerNode: true
  # enable the ceph dashboard for viewing cluster status
  # 开启ceph资源面板
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
  network:
    # toggle to use hostNetwork
    # 使用宿主机的网络进行通讯
    # 使用宿主机的网络貌似可以让集群外的主机挂载ceph
    # 但是我没试过，有兴趣的兄弟可以试试改成true
    # 反正这里只是集群内用，我就不改了
    hostNetwork: false
  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
  # The example under &#39;all&#39; would have all services scheduled on kubernetes nodes labeled with &#39;role=storage-node&#39; and
  # tolerate taints with a key of &#39;storage-node&#39;.
  placement:
#    all:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: role
#              operator: In
#              values:
#              - storage-node
#      podAffinity:
#      podAntiAffinity:
#      tolerations:
#      - key: storage-node
#        operator: Exists
# The above placement information can also be specified for mon, osd, and mgr components
#    mon:
#    osd:
#    mgr:
# nodeAffinity：通过选择标签的方式，可以限制pod被调度到特定的节点上
# 建议限制一下，为了让这几个pod不乱跑
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mon
              operator: In
              values:
              - enabled
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-osd
              operator: In
              values:
              - enabled
    mgr:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-mgr
              operator: In
              values:
              - enabled
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
#    mgr:
#      limits:
#        cpu: &amp;quot;500m&amp;quot;
#        memory: &amp;quot;1024Mi&amp;quot;
#      requests:
#        cpu: &amp;quot;500m&amp;quot;
#        memory: &amp;quot;1024Mi&amp;quot;
# The above example requests/limits can also be added to the mon and osd components
#    mon:
#    osd:
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    deviceFilter:
    location:
    config:
      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.
      # Set the storeType explicitly only if it is required not to use the default.
      # storeType: bluestore
      # databaseSizeMB: &amp;quot;1024&amp;quot; # this value can be removed for environments with normal sized disks (100 GB or larger)
      # journalSizeMB: &amp;quot;1024&amp;quot;  # this value can be removed for environments with normal sized disks (20 GB or larger)
# Cluster level list of directories to use for storage. These values will be set for all nodes that have no `directories` set.
#    directories:
#    - path: /rook/storage-dir
# Individual nodes and their config can be specified as well, but &#39;useAllNodes&#39; above must be set to false. Then, only the named
# nodes below will be used as storage resources.  Each node&#39;s &#39;name&#39; field should match their &#39;kubernetes.io/hostname&#39; label.
#建议磁盘配置方式如下：
#name: 选择一个节点，节点名字为kubernetes.io/hostname的标签，也就是kubectl get nodes看到的名字
#devices: 选择磁盘设置为OSD
# - name: &amp;quot;sdb&amp;quot;:将/dev/sdb设置为osd
    nodes:
    - name: &amp;quot;kube-node1&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
    - name: &amp;quot;kube-node2&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;
    - name: &amp;quot;kube-node3&amp;quot;
      devices:
      - name: &amp;quot;sdb&amp;quot;

#      directories: # specific directories to use for storage can be specified for each node
#      - path: &amp;quot;/rook/storage-dir&amp;quot;
#      resources:
#        limits:
#          cpu: &amp;quot;500m&amp;quot;
#          memory: &amp;quot;1024Mi&amp;quot;
#        requests:
#          cpu: &amp;quot;500m&amp;quot;
#          memory: &amp;quot;1024Mi&amp;quot;
#    - name: &amp;quot;172.17.4.201&amp;quot;
#      devices: # specific devices to use for storage can be specified for each node
#      - name: &amp;quot;sdb&amp;quot;
#      - name: &amp;quot;sdc&amp;quot;
#      config: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: filestore
#    - name: &amp;quot;172.17.4.301&amp;quot;
#      deviceFilter: &amp;quot;^sd.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;开始部署ceph&#34;&gt;开始部署ceph&lt;/h2&gt;
&lt;p&gt;部署ceph&lt;br&gt;
kubectl apply -f cluster.yaml&lt;br&gt;
cluster会在rook-ceph这个namesapce创建资源&lt;br&gt;
看到所有的pod都Running就行了&lt;br&gt;
注意看一下pod分布的宿主机，跟我们打标签的主机是一致的&lt;br&gt;
kubectl -n rook-ceph get pod -o wide&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233756856.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;切换到其他主机看一下磁盘&lt;/p&gt;
&lt;p&gt;切换到kube-node1&lt;br&gt;
lsblk&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233803458.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;配置ceph-dashboard&#34;&gt;配置ceph dashboard&lt;/h2&gt;
&lt;p&gt;看一眼dashboard在哪个service上&lt;br&gt;
kubectl -n rook-ceph get service&lt;br&gt;
可以看到dashboard监听了8443端口&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233853544.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;创建个nodeport类型的service以便集群外部访问&lt;br&gt;
kubectl apply -f dashboard-external-https.yaml&lt;/p&gt;
&lt;p&gt;查看一下nodeport在哪个端口&lt;br&gt;
kubectl -n rook-ceph get service&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233909644.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;找出Dashboard的登陆账号和密码&lt;br&gt;
MGR_POD=&lt;code&gt;kubectl get pod -n rook-ceph | grep mgr | awk &#39;{print $1}&#39;&lt;/code&gt;&lt;br&gt;
kubectl -n rook-ceph logs $MGR_POD | grep password&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588233955731.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;打开浏览器输入任意一个Node的IP+nodeport端口&lt;br&gt;
这里我的就是：https://192.168.1.2:30290&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234024005.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234065656.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;配置ceph为storageclass&#34;&gt;配置ceph为storageclass&lt;/h2&gt;
&lt;p&gt;官方给了一个样本文件：storageclass.yaml&lt;br&gt;
这个文件使用的是 RBD 块存储&lt;br&gt;
pool创建详解：https://rook.io/docs/rook/v0.8/ceph-pool-crd.html&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  #这个name就是创建成ceph pool之后的pool名字
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 1
  # size 池中数据的副本数,1就是不保存任何副本
  failureDomain: osd
  #  failureDomain：数据块的故障域，
  #  值为host时，每个数据块将放置在不同的主机上
  #  值为osd时，每个数据块将放置在不同的osd上
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: ceph
   # StorageClass的名字，pvc调用时填的名字
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use `rook` as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: xfs
# 设置回收策略默认为：Retain
reclaimPolicy: Retain
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;创建storageclass&#34;&gt;创建StorageClass&lt;/h2&gt;
&lt;p&gt;kubectl apply -f storageclass.yaml&lt;br&gt;
kubectl get storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;
kubectl describe storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;
kubernetes搭建rook-ceph&lt;/p&gt;
&lt;p&gt;创建个nginx pod尝试挂载&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; nginx.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: ceph

---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports: 
  - port: 80
    name: nginx-port
    targetPort: 80
    protocol: TCP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /html
          name: http-file
      volumes:
      - name: http-file
        persistentVolumeClaim:
          claimName: nginx-pvc
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;kubectl apply -f nginx.yaml&lt;br&gt;
查看pv,pvc是否创建了&lt;br&gt;
kubectl get pv,pvc&lt;/p&gt;
&lt;p&gt;看一下nginx这个pod也运行了&lt;br&gt;
kubectl get pod&lt;/p&gt;
&lt;p&gt;删除这个pod,看pv是否还存在&lt;br&gt;
kubectl delete -f nginx.yaml&lt;/p&gt;
&lt;p&gt;kubectl get pv,pvc&lt;br&gt;
可以看到，pod和pvc都已经被删除了，但是pv还在！！！&lt;/p&gt;
&lt;h2 id=&#34;添加新节点进入集群&#34;&gt;添加新节点进入集群&lt;/h2&gt;
&lt;p&gt;这次我们要把node4添加进集群，先打标签&lt;br&gt;
kubectl label nodes kube-node4 ceph-osd=enabled&lt;br&gt;
重新编辑cluster.yaml文件&lt;br&gt;
原来的基础上添加node4的信息&lt;/p&gt;
&lt;p&gt;cd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;
vi cluster.yam&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234207475.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;
kubectl apply -f cluster.yaml&lt;/p&gt;
&lt;p&gt;盯着rook-ceph名称空间,集群会自动添加node4进来&lt;br&gt;
kubectl -n rook-ceph get pod -o wide -w&lt;br&gt;
kubectl -n rook-ceph get pod -o wide&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234283568.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234307875.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
去node4节点看一下磁盘&lt;br&gt;
lsblk&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234334204.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;删除一个节点&#34;&gt;删除一个节点&lt;/h2&gt;
&lt;p&gt;去掉node3的标签&lt;br&gt;
kubectl label nodes kube-node3 ceph-osd-&lt;br&gt;
重新编辑cluster.yaml文件&lt;br&gt;
删除node3的信息&lt;br&gt;
cd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;
vi cluster.yam&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234380826.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;
kubectl apply -f cluster.yaml&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234493936.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;/p&gt;
&lt;p&gt;kubectl -n rook-ceph get pod -o wide&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234548709.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
&lt;img src=&#34;http://lvelvis.github.io/post-images/1588234579591.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34;&gt;&lt;br&gt;
最后记得删除宿主机的/var/lib/rook文件夹&lt;/p&gt;
">kubernetes搭建rook-ceph</a>
      </div>
      
      <div class="item">
        <a class="result-title" href="http://lvelvis.github.io/post/about/"" data-c="
          &lt;blockquote&gt;
&lt;p&gt;欢迎来到我的小站呀，很高兴遇见你！🤝&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;关于本站&#34;&gt;🏠 关于本站&lt;/h2&gt;
&lt;p&gt;记录平时运维相关的技术:&lt;br&gt;
1.网络&lt;br&gt;
2.系统&lt;br&gt;
3.虚拟化&lt;br&gt;
4.数据库&lt;br&gt;
5.中间件&lt;br&gt;
...&lt;/p&gt;
&lt;h2 id=&#34;博主是谁&#34;&gt;👨‍💻 博主是谁&lt;/h2&gt;
&lt;p&gt;目前在职于巨人网络&lt;/p&gt;
&lt;h2 id=&#34;兴趣爱好&#34;&gt;⛹ 兴趣爱好&lt;/h2&gt;
&lt;p&gt;番剧 FPS射击游戏 dota&lt;/p&gt;
&lt;h2 id=&#34;联系我呀&#34;&gt;📬 联系我呀&lt;/h2&gt;
">关于</a>
      </div>
      
    </div>
  </div>
</div>
<script>
  // var escape = "[{&#34;content&#34;:&#34;&lt;h1 id=\&#34;数组\&#34;&gt;数组&lt;/h1&gt;\n&lt;p&gt;数组的长度一旦定义了就不能动态增长，并且存储的数据类型必须相同。&lt;/p&gt;\n&lt;h2 id=\&#34;创建方法\&#34;&gt;创建方法：&lt;/h2&gt;\n&lt;p&gt;var 数组名 [长度]数据类型&lt;br&gt;\n例如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;package main\nimport &amp;quot;fmt&amp;quot;\n \nfunc main(){\n    var test [5]int //定义数组名字test，长度为5，数据类型为int的数组\n    test[0] = 1    //赋值\n    test[1] = 2   \n    test[2] = 3\n    test[3] = 4\n    fmt.Println(test) \n    fmt.Println(test[2])\n    fmt.Println(test[1:3]) //输出1到3的数组\n    fmt.Println(test[0:]) //0到结尾\n    fmt.Println(test[:3])  //0到3\n \n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;##结果##\n[1 2 3 4 0]\n3\n[2 3]\n[1 2 3 4 0]\n[1 2 3]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;数组的四种初始化方式\&#34;&gt;数组的四种初始化方式&lt;/h2&gt;\n&lt;p&gt;例如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var s1 [3]int = [3]int{1,2,3}\nfmt.Println(&amp;quot;s1&amp;quot;,s1)\nvar s2 [4]int = [...]int{5,6,7,8} //[...]是固定写法\nfmt.Println(&amp;quot;s2&amp;quot;,s2)\nvar s3 = [2]int{9,10} //第一种的简化\nfmt.Println(&amp;quot;s3&amp;quot;,s3)\nvar s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对\nfmt.Println(&amp;quot;s4&amp;quot;,s4)\nvar s5 = new([5]int)\ns5[4] =12\nfmt.Println(&amp;quot;s5&amp;quot;,s5)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;##结果##\ns1 [1 2 3]\ns2 [5 6 7 8]\ns3 [9 10]\ns4 [40 41 42 43]\ns5 [0 0 0 0 5]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;数组的遍历\&#34;&gt;数组的遍历&lt;/h2&gt;\n&lt;p&gt;例如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对\nfmt.Println(&amp;quot;s4&amp;quot;,s4)\n     \nfor index,value := range s4{\nfmt.Println(index,value)\n}\n \n#结果##\n0 40\n1 41\n2 42\n3 43\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;var s4 = [...]int{3:43,1:41,0:40,2:42} //类似键值对\nfor i := 0;i &amp;lt;len(s4);i++{\nfmt.Println(i,s4[i])\n}\n \n#结果##\n0 40\n1 41\n2 42\n3 43\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;slice切片\&#34;&gt;slice切片&lt;/h1&gt;\n&lt;p&gt;1、切片是数组的引用&lt;br&gt;\n2、切片的使用类似数组，如遍历&lt;br&gt;\n3、切片的长度是可变的&lt;/p&gt;\n&lt;h2 id=\&#34;创建语法\&#34;&gt;创建语法&lt;/h2&gt;\n&lt;p&gt;var 切片名 []类型&lt;br&gt;\n如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var qiepian []int\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;切片示例\&#34;&gt;切片示例:&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;###例子一&amp;lt;br&amp;gt;var suzhu [4]int = [...]int{5,6,7,8}\nslice := suzhu[1:4] //1到4的值，不包含4\nfmt.Println(suzhu)\nfmt.Println(slice)\nfmt.Println(&amp;quot;切片的容量&amp;quot;,cap(slice))\n \n##结果\n[5 6 7 8]\n[6 7 8]\n切片的容量 3&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;\n###例子二、使用make创建切片\nvar slice []int = make([]int,4,10) //类型，大小(长度),容量（可选），容量必须大于长度\nslice[0] = 10\nslice[1] = 11\nfmt.Println(slice)\n \n##结果##\n[10 11 0 0]\n \n \n###例子三\nvar slice []int = []int {2,4,6}\nfmt.Println(slice)\n \n##结果##\n2 4 6\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;切片的append追加\&#34;&gt;切片的append追加&lt;/h2&gt;\n&lt;p&gt;例如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var slice []int = []int {2,4,6}\nfmt.Println(slice)\n//使用append直接追加切片内容（类似python list的append）\nslice = append(slice,8,10)\nfmt.Println(slice)\nslice = append(slice,slice...) //追加切片，...是固定写法\nfmt.Println(slice)\n \n###结果###\n[2 4 6]\n[2 4 6 8 10]\n[2 4 6 8 10 2 4 6 8 10]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;切片的copy操作\&#34;&gt;切片的copy操作&lt;/h2&gt;\n&lt;p&gt;使用copy内置函数&lt;br&gt;\n例如:&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var slice []int = []int {2,4,6}\nfmt.Println(slice)\nvar slice2 []int = make([]int,5)\nfmt.Println(slice2)\ncopy(slice2,slice) //将slice复制给slice2\nfmt.Println(slice)\nfmt.Println(slice2)\n \n##结果##\n[2 4 6]\n[0 0 0 0 0]\n[2 4 6]\n[2 4 6 0 0]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;使用切片改变字符串的内容\&#34;&gt;使用切片改变字符串的内容&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;var str string = &amp;quot;hello&amp;quot;\nfmt.Println(str)\narr := []byte(str)\narr[1] = &#39;a&#39; //转成字符串\narr1 := []rune(str) //中文转换\narr1[0] = &#39;狗&#39;\nfmt.Println(arr)\nstr = string(arr)\nfmt.Println(str)\nstr = string(arr1)\nfmt.Println(str)\n \n##结果##\nhello\n[104 97 108 108 111]\nhello\n狗hello\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;map\&#34;&gt;map&lt;/h1&gt;\n&lt;p&gt;map是key-value数据结构(类似python的dict)&lt;br&gt;\nmap是无序存储的&lt;/p&gt;\n&lt;p&gt;创建map语法&lt;br&gt;\nvar map 变量名 map[keytype]valuetype&lt;/p&gt;\n&lt;p&gt;如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;var m1 map[string]string\nvar m2 map[string]int\nvar m3 map[int]string\nvar m4 map[string]map[string]string\n```　　\n\n使用例子：\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;package main&lt;br&gt;\nimport &amp;quot;fmt&amp;quot;&lt;/p&gt;\n&lt;p&gt;func main(){&lt;br&gt;\nvar m1 map[string]string&lt;br&gt;\n//在使用map前,需要先make，make的作用技术给map分配数据空间&lt;br&gt;\nm1 = make(map[string]string)&lt;br&gt;\nm2 := map[string]string{  //使用方式二&lt;br&gt;\n&amp;quot;a1&amp;quot; : &amp;quot;q1&amp;quot;,&lt;br&gt;\n&amp;quot;a2&amp;quot; : &amp;quot;a2&amp;quot;,&lt;br&gt;\n}&lt;br&gt;\nm1[&amp;quot;s1&amp;quot;] = &amp;quot;亚索&amp;quot;&lt;br&gt;\nm1[&amp;quot;s2&amp;quot;] = &amp;quot;盖伦&amp;quot;&lt;br&gt;\nfmt.Println(m1)&lt;br&gt;\nfmt.Println(m1[&amp;quot;s1&amp;quot;])&lt;br&gt;\nfmt.Println(m2)&lt;br&gt;\n}&lt;/p&gt;\n&lt;p&gt;###结果###&lt;br&gt;\nmap[s1:亚索 s2:盖伦]&lt;br&gt;\n亚索&lt;br&gt;\nmap[a1:q1 a2:a2]&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;\n \n\nmap的增删改查\n增、改\nmap[key] = value //没有就增加，存在就修改\n\n删\ndelete(map,key)\n\n查\nmap[key]   //对应的value，和python的dict一样&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;golang-bi-ji-shu-zu-qie-pian-map&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;golang笔记-数组、切片、map&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;golang&#34;,&#34;slug&#34;:&#34;oHQV9cP9p&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/oHQV9cP9p/&#34;}],&#34;date&#34;:&#34;2020-05-26 13:29:04&#34;,&#34;dateFormat&#34;:&#34;2020-05-26&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/golang-bi-ji-shu-zu-qie-pian-map/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;5 min read&#34;,&#34;time&#34;:290000,&#34;words&#34;:957,&#34;minutes&#34;:5},&#34;description&#34;:&#34;数组\n数组的长度一旦定义了就不能动态增长，并且存储的数据类型必须相同。\n创建方法：\nvar 数组名 [长度]数据类型\n例如：\npackage main\nimport &amp;quot;fmt&amp;quot;\n \nfunc main(){\n    va...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%95%B0%E7%BB%84\&#34;&gt;数组&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%9B%E5%BB%BA%E6%96%B9%E6%B3%95\&#34;&gt;创建方法：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%95%B0%E7%BB%84%E7%9A%84%E5%9B%9B%E7%A7%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E5%BC%8F\&#34;&gt;数组的四种初始化方式&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%95%B0%E7%BB%84%E7%9A%84%E9%81%8D%E5%8E%86\&#34;&gt;数组的遍历&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#slice%E5%88%87%E7%89%87\&#34;&gt;slice切片&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%9B%E5%BB%BA%E8%AF%AD%E6%B3%95\&#34;&gt;创建语法&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%87%E7%89%87%E7%A4%BA%E4%BE%8B\&#34;&gt;切片示例:&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%87%E7%89%87%E7%9A%84append%E8%BF%BD%E5%8A%A0\&#34;&gt;切片的append追加&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%87%E7%89%87%E7%9A%84copy%E6%93%8D%E4%BD%9C\&#34;&gt;切片的copy操作&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E4%BD%BF%E7%94%A8%E5%88%87%E7%89%87%E6%94%B9%E5%8F%98%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%86%85%E5%AE%B9\&#34;&gt;使用切片改变字符串的内容&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#map\&#34;&gt;map&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;pre&gt;&lt;code&gt;HPA全称Horizontal Pod Autoscaling，是K8s实现pod自动水平扩容缩容的特性，这个特性使整个kubernetes集群马上高大上起来了。\n要使用HPA也不是这么简单的，HPA api分v1、v2beta1、v2bate2三种，v1只支持通过CPU衡量扩缩容，v2bate1加入针对内存作为度量，v2bate2可以用customer metrics例如网络等，所以v2bate1开始才比较实用。\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;要使用HPA必须要开启以下两个特性：&lt;/p&gt;\n&lt;p&gt;Aggregation Layer 聚合层，通过与核心的apiserver分离，实现自定义的扩展功能&lt;br&gt;\nmetrics-server 数据收集，能够收集pod、node等实时运行指标（cpu、内存），给k8s集群使用，例如kubectl top命令、HPA&lt;br&gt;\n比较老的版本使用heapster&lt;/p&gt;\n&lt;h1 id=\&#34;aggregation-layer\&#34;&gt;Aggregation Layer&lt;/h1&gt;\n&lt;p&gt;要打开Aggregation Layer，需要配置一下apiserver，增加相关认证证书。认证流程是client发起请求到apiserver，apiserver与aggergated apiserver建立tls安全链接，把请求proxy到aggergated apiserver，继续进行–requestheader-*参数的相关认证。&lt;/p&gt;\n&lt;p&gt;认证流程&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1589865988979.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;需要生成aggregate使用的证书，参考cfssl生成证书方法，proxy-client-cert-file的CN需要与requestheader-allowed-names匹配。&lt;br&gt;\n在apiserver增加如下启动参数&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;--requestheader-client-ca-file=/etc/kubernetes/pki/agg-ca.pem\n--proxy-client-cert-file=/etc/kubernetes/pki/aggregate.pem\n--proxy-client-key-file=/etc/kubernetes/pki/aggregate-key.pem\n--requestheader-allowed-names=aggregator\n--requestheader-extra-headers-prefix=X-Remote-Extra-\n--requestheader-group-headers=X-Remote-Group\n--requestheader-username-headers=X-Remote-User\n#如果kube-proxy没有在Master上面运行，还需要配置\n--enable-aggregator-routing=true\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;metrics-server\&#34;&gt;metrics server&lt;/h1&gt;\n&lt;p&gt;从k8s 1.8开始，集群的资源使用情况都通过metrics api收集，例如容器CPU、内存。这些指标可用于kuberctl top或者k8s的HPA等特性。&lt;br&gt;\nmetrice server可以在github找到并部署&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/kubernetes-incubator/metrics-server\ncd metrics-server\nkubectl create -f deploy/1.8+/\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;注1：metrics-server默认使用node的主机名，但是coredns里面没有物理机主机名的解析，一种是部署的时候添加一个参数： –kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP,第二种是使用dnsmasq构建一个上游的dns服务\n注2：kubelet 的10250端口使用的是https协议，连接需要验证tls证书。可以在metrics server启动命令添加参数–kubelet-insecure-tls不验证客户端证书\n注3：yaml文件中的image地址k8s.gcr.io/metrics-server-amd64:v0.3.3 需要梯子，需要改成中国可以访问的image地址，可以使用aliyun的。这里使用hub.docker.com里的google镜像地址 image: mirrorgooglecontainers/metrics-server-amd64:v0.3.3\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;成功运行kubectl top命令&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;ubuntu@k8s-dev-m1:~/k8sssl/agglayer$ kubectl top nodes\nNAME                   CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nk8s-dev-node2          103m         5%      2696Mi                 72%       \nk8s-dev-node3.bxr.cn   115m      2%     5312Mi                  67%  \nk8s-dev-node4          57m          2%       2634Mi                  70%     \nk8s-dev-node5          148m         7%       2443Mi                  65%\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;hpa\&#34;&gt;HPA&lt;/h1&gt;\n&lt;pre&gt;&lt;code&gt;有了metrics就可以开始使用HPA特性了。hpa有几个特点\ndeploy或者rs等需要设置resources才能使用hpa\n如果我们创建一个HPA controller，它会每隔15s（可以通过–horizontal-pod-autoscaler-sync-period修改）检测一次hpa定义的资源与实际资源使用情况，如果达到阀值就会调整pod数量。\nHPA设置的阀值不是绝对的，允许设置一个浮动范围，–horizontal-pod-autoscaler-tolerance默认是0.1\npod调整算法 desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]\nscale有一个窗口期，期间每次变化会记录下来，选择最优的调整建议再进行scale，这样可以保证资源平滑变动，通过–horizontal-pod-autoscaler-downscale-stabilization设定，默认5分钟。\n通过hpa调整新增的pod不会马上ready，这时候收集的metrics就不准，为了减少影响，hpa一开始不会收集新pod的metrics。通过–horizontal-pod-autoscaler-initial-readiness-delay（默认30s）和 –horizontal-pod-autoscaler-cpu-initialization-period（默认为 5 分钟）调整\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1589866316410.svg\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n示例hpa.yml:&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: hpa-test\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: podinfo\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 75\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization \n        averageUtilization: 160\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;上面的示例包括cpu和memory指标，averageUtilization这个百分比是根据deployment的resources.requests计算的。例如有deployment限制requests是512Mi，replicas是2，实际pod1用了612Mi，pod2用了598Mi，计算公式是 (612+598)/2/512 = 118%&lt;/p&gt;\n&lt;p&gt;查看hpa的情况，targets第一个是memory，第二个是cpu指标，REPLICAS是根据计算后的当前pod数&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;ubuntu@k8s-m1:~/k8s/hpa$ kubectl get hpa\nNAME       REFERENCE            TARGETS             MINPODS   MAXPODS   REPLICAS   AGE\nhpa-test   Deployment/podinfo   120%/160%, 6%/75%   2         4         3          97m\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;官方示例还包括packets-per-second、requests-per-second这些指标，需要进一步验证&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: php-apache\n  namespace: default\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: php-apache\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: AverageUtilization\n        averageUtilization: 50\n  - type: Pods\n    pods:\n      metric:\n        name: packets-per-second\n      targetAverageValue: 1k\n  - type: Object\n    object:\n      metric:\n        name: requests-per-second\n      describedObject:\n        apiVersion: networking.k8s.io/v1beta1\n        kind: Ingress\n        name: main-route\n      target:\n        kind: Value\n        value: 10k\nstatus:\n  observedGeneration: 1\n  lastScaleTime: &amp;lt;some-time&amp;gt;\n  currentReplicas: 1\n  desiredReplicas: 1\n  currentMetrics:\n  - type: Resource\n    resource:\n      name: cpu\n    current:\n      averageUtilization: 0\n      averageValue: 0\n  - type: Object\n    object:\n      metric:\n        name: requests-per-second\n      describedObject:\n        apiVersion: networking.k8s.io/v1beta1\n        kind: Ingress\n        name: main-route\n      current:\n        value: 10k\n\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;&lt;a href=\&#34;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#create-horizontal-pod-autoscaler\&#34;&gt;官方hpa参数&lt;/a&gt;&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;k8s-hpa-dan-xing-kuo-rong-pei-zhi&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;k8s hpa弹性扩容配置&#34;,&#34;tags&#34;:[{&#34;name&#34;:&#34;hpa&#34;,&#34;slug&#34;:&#34;1VCnVUFme&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/1VCnVUFme/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-05-19 13:24:13&#34;,&#34;dateFormat&#34;:&#34;2020-05-19&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/k8s-hpa-dan-xing-kuo-rong-pei-zhi/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;6 min read&#34;,&#34;time&#34;:328000,&#34;words&#34;:1211,&#34;minutes&#34;:6},&#34;description&#34;:&#34;HPA全称Horizontal Pod Autoscaling，是K8s实现pod自动水平扩容缩容的特性，这个特性使整个kubernetes集群马上高大上起来了。\n要使用HPA也不是这么简单的，HPA api分v1、v2beta1、v2ba...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#aggregation-layer\&#34;&gt;Aggregation Layer&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#metrics-server\&#34;&gt;metrics server&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#hpa\&#34;&gt;HPA&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;p&gt;Go 语言内部其实已经提供了 http.ServeFile，通过这个函数可以实现静态文件的服务。&lt;/p&gt;\n&lt;p&gt;beego 针对这个功能进行了一层封装，通过下面的方式进行静态文件注册：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;beego.SetStaticPath(&amp;quot;/static&amp;quot;,&amp;quot;public&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;第一个参数是路径，url 路径信息&lt;br&gt;\n第二个参数是静态文件目录（相对应用所在的目录）&lt;br&gt;\nbeego 支持多个目录的静态文件注册，用户可以注册如下的静态文件目录：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;beego.SetStaticPath(&amp;quot;/images&amp;quot;,&amp;quot;images&amp;quot;)\nbeego.SetStaticPath(&amp;quot;/css&amp;quot;,&amp;quot;css&amp;quot;)\nbeego.SetStaticPath(&amp;quot;/js&amp;quot;,&amp;quot;js&amp;quot;)\n```　　\n\n设置了如上的静态目录之后，用户访问 /images/login/login.png，那么就会访问应用对应的目录下面的 images/login/login.png 文件。\n\n如果是访问 /static/img/logo.png，那么就访问 public/img/logo.png文件。\n\n默认情况下 beego 会判断目录下文件是否存在，不存在直接返回 404 页面，如果请求的是 index.html，那么由于 http.ServeFile 默认是会跳转的，不提供该页面的显示。\n\n因此 beego 可以设置 beego.BConfig.WebConfig.DirectoryIndex=true 这样来使得显示 index.html 页面。而且开启该功能之后，用户访问目录就会显示该目录下所有的文件列表。&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;beego-jing-tai-wen-jian-jia-zai-lu-jing-xiu-gai&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;beego静态文件加载路径修改&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;golang&#34;,&#34;slug&#34;:&#34;oHQV9cP9p&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/oHQV9cP9p/&#34;},{&#34;name&#34;:&#34;beego&#34;,&#34;slug&#34;:&#34;WCSltmB8m&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/WCSltmB8m/&#34;}],&#34;date&#34;:&#34;2020-05-11 12:49:08&#34;,&#34;dateFormat&#34;:&#34;2020-05-11&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/beego-jing-tai-wen-jian-jia-zai-lu-jing-xiu-gai/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;2 min read&#34;,&#34;time&#34;:77000,&#34;words&#34;:328,&#34;minutes&#34;:2},&#34;description&#34;:&#34;Go 语言内部其实已经提供了 http.ServeFile，通过这个函数可以实现静态文件的服务。\nbeego 针对这个功能进行了一层封装，通过下面的方式进行静态文件注册：\nbeego.SetStaticPath(&amp;quot;/static&amp;...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;p&gt;Ceph v0.55 及后续版本默认开启了 cephx 认证。从用户空间（ FUSE ）挂载一 Ceph 文件系统前，确保客户端主机有一份 Ceph 配置副本、和具备 Ceph 元数据服务器能力的密钥环。&lt;/p&gt;\n&lt;p&gt;1、 在客户端主机上，把监视器主机上的 Ceph 配置文件拷贝到 /etc/ceph/ 目录下&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;sudo mkdir -p /etc/ceph\nsudo scp {user}@{server-machine}:/etc/ceph/ceph.conf /etc/ceph/ceph.conf\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;2、 在客户端主机上，把监视器主机上的 Ceph 密钥环拷贝到 /etc/ceph 目录下&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;sudo scp {user}@{server-machine}:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;3、 确保客户端机器上的 Ceph 配置文件和密钥环都有合适的权限位，如 chmod 644 。&lt;br&gt;\ncephx 如何配置请参考 CEPHX 配置参考。&lt;br&gt;\n要把 Ceph 文件系统挂载为用户空间文件系统，可以用 ceph-fuse 命令，例如：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;sudo mkdir /home/usernname/cephfs\nsudo ceph-fuse -m 192.168.0.1:6789 /home/username/cephfs\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;详情见 &lt;a href=\&#34;http://docs.ceph.org.cn/man/8/ceph-fuse/\&#34;&gt;ceph-fuse&lt;/a&gt;&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;yong-hu-kong-jian-gua-zai-ceph-wen-jian-xi-tong&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;用户空间挂载 CEPH 文件系统&#34;,&#34;tags&#34;:[{&#34;name&#34;:&#34;rook-ceph&#34;,&#34;slug&#34;:&#34;hu0mNDfuy&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/hu0mNDfuy/&#34;}],&#34;date&#34;:&#34;2020-05-08 14:03:36&#34;,&#34;dateFormat&#34;:&#34;2020-05-08&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/yong-hu-kong-jian-gua-zai-ceph-wen-jian-xi-tong/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;2 min read&#34;,&#34;time&#34;:61000,&#34;words&#34;:242,&#34;minutes&#34;:2},&#34;description&#34;:&#34;Ceph v0.55 及后续版本默认开启了 cephx 认证。从用户空间（ FUSE ）挂载一 Ceph 文件系统前，确保客户端主机有一份 Ceph 配置副本、和具备 Ceph 元数据服务器能力的密钥环。\n1、 在客户端主机上，把监视器主机...&#34;,&#34;toc&#34;:&#34;&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;问题\&#34;&gt;问题&lt;/h1&gt;\n&lt;pre&gt;&lt;code&gt;curl https://192.168.0.200:8443\n提示curl: (60) Peer&#39;s Certificate issuer is not recognized.\ncurl: (60) Peer&#39;s Certificate issuer is not recognized.\nMore details here: http://curl.haxx.se/docs/sslcerts.html\n\ncurl performs SSL certificate verification by default, using a &amp;quot;bundle&amp;quot;\n of Certificate Authority (CA) public keys (CA certs). If the default\n bundle file isn&#39;t adequate, you can specify an alternate file\n using the --cacert option.\nIf this HTTPS server uses a certificate signed by a CA represented in\n the bundle, the certificate verification probably failed due to a\n problem with the certificate (it might be expired, or the name might\n not match the domain name in the URL).\nIf you&#39;d like to turn off curl&#39;s verification of the certificate, use\n the -k (or --insecure) option.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;mac-os-x\&#34;&gt;Mac OS X&lt;/h1&gt;\n&lt;h2 id=\&#34;添加证书\&#34;&gt;添加证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ~/new-root-certificate.crt\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;移除证书\&#34;&gt;移除证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;sudo security delete-certificate -c &amp;quot;&amp;lt;name of existing certificate&amp;gt;&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;windows\&#34;&gt;Windows&lt;/h1&gt;\n&lt;h2 id=\&#34;添加证书-2\&#34;&gt;添加证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;certutil -addstore -f &amp;quot;ROOT&amp;quot; new-root-certificate.crt\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;移除证书-2\&#34;&gt;移除证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;certutil -delstore &amp;quot;ROOT&amp;quot; serial-number-hex\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;linux-ubuntu-debian\&#34;&gt;Linux (Ubuntu, Debian)&lt;/h1&gt;\n&lt;h2 id=\&#34;添加证书-3\&#34;&gt;添加证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;复制 CA 文件到目录： /usr/local/share/ca-certificates/\n执行:\nsudo cp foo.crt /usr/local/share/ca-certificates/foo.crt\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;更新-ca-证书库\&#34;&gt;更新 CA 证书库:&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;sudo update-ca-certificates\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;移除证书-3\&#34;&gt;移除证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;Remove your CA.\n\nUpdate the CA store:\n\nsudo update-ca-certificates --fresh\n\nRestart Kerio Connect to reload the certificates in the 32-bit versions or Debian 7.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;linux-centos-6\&#34;&gt;Linux (CentOs 6)&lt;/h1&gt;\n&lt;h2 id=\&#34;添加证书-4\&#34;&gt;添加证书：&lt;/h2&gt;\n&lt;p&gt;// root-ca.crt 为ca证书&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;安装 ca-certificates package:\n\nyum install ca-certificates\n\n启用dynamic CA configuration feature:\n\nupdate-ca-trust force-enable\n\nAdd it as a new file to /etc/pki/ca-trust/source/anchors/:\ncp root-ca.crt /etc/pki/ca-trust/source/anchors/\n\n执行:\n\nupdate-ca-trust extract\n\nRestart Kerio Connect to reload the certificates in the 32-bit version.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;linux-centos-5\&#34;&gt;Linux (CentOs 5)&lt;/h1&gt;\n&lt;h2 id=\&#34;添加证书-5\&#34;&gt;添加证书：&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;Append your trusted certificate to file /etc/pki/tls/certs/ca-bundle.crt\n\ncat foo.crt &amp;gt;&amp;gt; /etc/pki/tls/certs/ca-bundle.crt\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;测试访问\&#34;&gt;测试访问&lt;/h1&gt;\n&lt;pre&gt;&lt;code&gt; curl -v &amp;quot;https:/gitlab.test.com/micro-lib/server?go-get=1&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;linux-ru-he-dao-ru-zi-ding-yi-zheng-shu&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;linux如何导入自定义证书&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;linux&#34;,&#34;slug&#34;:&#34;qf8arSPKY&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/qf8arSPKY/&#34;},{&#34;name&#34;:&#34;ssl&#34;,&#34;slug&#34;:&#34;EVZAZcQaN&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/EVZAZcQaN/&#34;}],&#34;date&#34;:&#34;2020-05-06 16:33:15&#34;,&#34;dateFormat&#34;:&#34;2020-05-06&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/linux-ru-he-dao-ru-zi-ding-yi-zheng-shu/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;3 min read&#34;,&#34;time&#34;:141000,&#34;words&#34;:406,&#34;minutes&#34;:3},&#34;description&#34;:&#34;问题\ncurl https://192.168.0.200:8443\n提示curl: (60) Peer&#39;s Certificate issuer is not recognized.\ncurl: (60) Peer&#39;s Certifica...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%97%AE%E9%A2%98\&#34;&gt;问题&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#mac-os-x\&#34;&gt;Mac OS X&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6\&#34;&gt;添加证书：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%A7%BB%E9%99%A4%E8%AF%81%E4%B9%A6\&#34;&gt;移除证书：&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#windows\&#34;&gt;Windows&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6-2\&#34;&gt;添加证书：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%A7%BB%E9%99%A4%E8%AF%81%E4%B9%A6-2\&#34;&gt;移除证书：&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#linux-ubuntu-debian\&#34;&gt;Linux (Ubuntu, Debian)&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6-3\&#34;&gt;添加证书：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%9B%B4%E6%96%B0-ca-%E8%AF%81%E4%B9%A6%E5%BA%93\&#34;&gt;更新 CA 证书库:&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%A7%BB%E9%99%A4%E8%AF%81%E4%B9%A6-3\&#34;&gt;移除证书：&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#linux-centos-6\&#34;&gt;Linux (CentOs 6)&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6-4\&#34;&gt;添加证书：&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#linux-centos-5\&#34;&gt;Linux (CentOs 5)&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6-5\&#34;&gt;添加证书：&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B5%8B%E8%AF%95%E8%AE%BF%E9%97%AE\&#34;&gt;测试访问&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h3 id=\&#34;前提\&#34;&gt;前提&lt;/h3&gt;\n&lt;p&gt;由于istio部署到IDC、非云环境无法获取客户端真实IP地址，web应用无法使用该组件，早期版本的istio做了很多测试，最终无法实现获取remote client address；&lt;/p&gt;\n&lt;h3 id=\&#34;环境\&#34;&gt;环境&lt;/h3&gt;\n&lt;p&gt;kubernetes版本：k8s-1.16.9&lt;br&gt;\nistio版本：1.5&lt;/p&gt;\n&lt;h3 id=\&#34;方法\&#34;&gt;方法&lt;/h3&gt;\n&lt;pre&gt;&lt;code&gt;kubectl -n istio-system edit  deployments. istio-pilot\n添加如下：\n       env:\n       - name: PILOT_SIDECAR_USE_REMOTE_ADDRESS\n          value: &amp;quot;true&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;以下是github相应的issue&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588232244321.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;其他测试\&#34;&gt;其他测试&lt;/h3&gt;\n&lt;p&gt;istio-1.5版本回归单体，各个组件优化了很多，后期测试http链接与tcp链接应用&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;istio-ke-hu-duan-yuan-di-zhi-ru-he-xian-shi&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;istio-客户端源地址如何显示&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;istio&#34;,&#34;slug&#34;:&#34;d8Hk7igEb&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/d8Hk7igEb/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-04-30 15:32:39&#34;,&#34;dateFormat&#34;:&#34;2020-04-30&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/istio-ke-hu-duan-yuan-di-zhi-ru-he-xian-shi/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;1 min read&#34;,&#34;time&#34;:32000,&#34;words&#34;:133,&#34;minutes&#34;:1},&#34;description&#34;:&#34;前提\n由于istio部署到IDC、非云环境无法获取客户端真实IP地址，web应用无法使用该组件，早期版本的istio做了很多测试，最终无法实现获取remote client address；\n环境\nkubernetes版本：k8s-1.16...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%89%8D%E6%8F%90\&#34;&gt;前提&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%8E%AF%E5%A2%83\&#34;&gt;环境&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%96%B9%E6%B3%95\&#34;&gt;方法&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%85%B6%E4%BB%96%E6%B5%8B%E8%AF%95\&#34;&gt;其他测试&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;前提\&#34;&gt;前提：&lt;/h1&gt;\n&lt;p&gt;在kubernetes集群中部署elk组件，es集群部署3个节点，kibana部署一个，容器数据持久化需要storageclass&lt;/p&gt;\n&lt;p&gt;依赖：&lt;br&gt;\nHelm&lt;br&gt;\nPersistent Volumes&lt;/p&gt;\n&lt;h1 id=\&#34;准备配置\&#34;&gt;准备配置&lt;/h1&gt;\n&lt;p&gt;由于repo在线安装太慢，建议下载char本地修改参数后安装&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/elastic/helm-charts.git\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;部署-elk\&#34;&gt;部署 ELK&lt;/h1&gt;\n&lt;h2 id=\&#34;创建elk命名空间\&#34;&gt;创建elk命名空间&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;#cat elk-ns.yml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: elk\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;部署elasticsearch\&#34;&gt;部署elasticsearch&lt;/h2&gt;\n&lt;p&gt;cd helm-charts/elasticsearch&lt;/p&gt;\n&lt;p&gt;helm install --namespace=elk  --name=elasticsearch .&lt;/p&gt;\n&lt;h2 id=\&#34;部署-kibana\&#34;&gt;部署 Kibana&lt;/h2&gt;\n&lt;p&gt;cd helm-charts/kibana&lt;/p&gt;\n&lt;p&gt;helm install --namespace=elk --name=kibana .&lt;br&gt;\n通过 kubectl get deploy 和 pod 了解部署状态；&lt;/p&gt;\n&lt;h1 id=\&#34;小知识\&#34;&gt;小知识&lt;/h1&gt;\n&lt;p&gt;Kibana 直接通过 K8S 内部 DNS 域名 访问 ES。&lt;/p&gt;\n&lt;p&gt;查看容器内的配置&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kubectl  exec kibana-kibana-7cbc5db55c-6qct7 -c kibana -- cat /usr/share/kibana/config/kibana.yml\n\n# Default Kibana configuration for docker target\nserver.name: kibana\nserver.host: &amp;quot;0&amp;quot;\nelasticsearch.hosts: [ &amp;quot;http://elasticsearch:9200&amp;quot; ]\nxpack.monitoring.ui.container.elasticsearch.enabled: true\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;kibana-添加-ingress\&#34;&gt;Kibana 添加 Ingress&lt;/h1&gt;\n&lt;p&gt;通过 Ingress 添加访问入口&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: kibana\n  namespace: default\nspec:\n  rules:\n  - host: &amp;lt;YourDomain&amp;gt;  ## 访问 Kibana 的域名 \n    http:\n      paths:\n      - backend:\n          serviceName: kibana-kibana\n          servicePort: 5601\n        path: /\n status:\n  loadBalancer:\n    ingress:\n    - ip: &amp;lt;YourLoadBalancerIP&amp;gt;  ## LB 的 IP\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h1 id=\&#34;访问测试\&#34;&gt;访问测试&lt;/h1&gt;\n&lt;p&gt;访问域名，即可打开 Kibana 7.3 版本；&lt;/p&gt;\n&lt;figure data-type=\&#34;image\&#34; tabindex=\&#34;1\&#34;&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588231324831.jpg\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/figure&gt;\n&lt;p&gt;查看集群的运行状态&lt;/p&gt;\n&lt;figure data-type=\&#34;image\&#34; tabindex=\&#34;2\&#34;&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588231313286.jpg\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/figure&gt;\n&lt;p&gt;也可以通过命令行查看&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;~$ curl  -s &amp;lt;YourESHost&amp;gt;/_cluster/health | jq .\n{\n  &amp;quot;cluster_name&amp;quot;: &amp;quot;elasticsearch&amp;quot;,\n  &amp;quot;status&amp;quot;: &amp;quot;yellow&amp;quot;,\n  &amp;quot;timed_out&amp;quot;: false,\n  &amp;quot;number_of_nodes&amp;quot;: 3,\n  &amp;quot;number_of_data_nodes&amp;quot;: 3,\n  &amp;quot;active_primary_shards&amp;quot;: 19,\n  &amp;quot;active_shards&amp;quot;: 35,\n  &amp;quot;relocating_shards&amp;quot;: 0,\n  &amp;quot;initializing_shards&amp;quot;: 0,\n  &amp;quot;unassigned_shards&amp;quot;: 3,\n  &amp;quot;delayed_unassigned_shards&amp;quot;: 0,\n  &amp;quot;number_of_pending_tasks&amp;quot;: 0,\n  &amp;quot;number_of_in_flight_fetch&amp;quot;: 0,\n  &amp;quot;task_max_waiting_in_queue_millis&amp;quot;: 0,\n  &amp;quot;active_shards_percent_as_number&amp;quot;: 92.10526315789474\n}\n&lt;/code&gt;&lt;/pre&gt;\n&#34;,&#34;fileName&#34;:&#34;k8s-tong-guo-helm-bu-shu-elk-73&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;K8S通过helm 部署 ELK 7.3&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;elk&#34;,&#34;slug&#34;:&#34;Diytd8lD3&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Diytd8lD3/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-04-30 15:12:33&#34;,&#34;dateFormat&#34;:&#34;2020-04-30&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/k8s-tong-guo-helm-bu-shu-elk-73/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;2 min read&#34;,&#34;time&#34;:111000,&#34;words&#34;:366,&#34;minutes&#34;:2},&#34;description&#34;:&#34;前提：\n在kubernetes集群中部署elk组件，es集群部署3个节点，kibana部署一个，容器数据持久化需要storageclass\n依赖：\nHelm\nPersistent Volumes\n准备配置\n由于repo在线安装太慢，建议下载...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%89%8D%E6%8F%90\&#34;&gt;前提：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%87%86%E5%A4%87%E9%85%8D%E7%BD%AE\&#34;&gt;准备配置&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%83%A8%E7%BD%B2-elk\&#34;&gt;部署 ELK&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%9B%E5%BB%BAelk%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4\&#34;&gt;创建elk命名空间&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%83%A8%E7%BD%B2elasticsearch\&#34;&gt;部署elasticsearch&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%83%A8%E7%BD%B2-kibana\&#34;&gt;部署 Kibana&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%B0%8F%E7%9F%A5%E8%AF%86\&#34;&gt;小知识&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#kibana-%E6%B7%BB%E5%8A%A0-ingress\&#34;&gt;Kibana 添加 Ingress&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E8%AE%BF%E9%97%AE%E6%B5%8B%E8%AF%95\&#34;&gt;访问测试&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;p&gt;最近又出来个kubesphere的工具用来管理k8s，今天特意来安装体验下；&lt;br&gt;\ngithub地址：https://github.com/pixiake/ks-installer&lt;/p&gt;\n&lt;p&gt;官方使用文档：https://kubesphere.io/docs/advanced-v2.0/zh-CN/installation/all-in-one/&lt;/p&gt;\n&lt;p&gt;先放上安装效果图，UI界面还是很清爽的：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588152934567.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;当前环境\&#34;&gt;当前环境：&lt;/h3&gt;\n&lt;pre&gt;&lt;code&gt;k8s集群已经安装完成，用kubesphere管理现有的k8s集群；\n\nk8s版本为1.14  \n\n系统为centos7.6\n\nkubesphere使用要求：\n\nkubernetes version &amp;gt; 1.13.0\n\nhelm version &amp;gt; 2.10.0\n\na default storage class must be in kubernetes cluster\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;安装完成后默认用户名密码：&lt;/p&gt;\n&lt;p&gt;用户名：admin&lt;/p&gt;\n&lt;p&gt;密码：P@88w0rd&lt;/p&gt;\n&lt;h3 id=\&#34;开始安装\&#34;&gt;开始安装&lt;/h3&gt;\n&lt;p&gt;安装步骤大概记录：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kubectl create ns kubesphere-system\nkubectl create ns kubesphere-monitoring-system\n\n#访问etcd用到的secret\nkubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  --from-file=etcd-client-ca.crt=ca.pem  --from-file=etcd-client.crt=etcd-key.pem  --from-file=etcd-client.key=etcd.pem\n\n#管理k8s用到的secret\n\nkubectl -n kubesphere-system create secret generic kubesphere-ca  --from-file=ca.crt=ca.pem --from-file=ca.key=ca-key.pem\n\n#clone好github项目，执行下面的这条命令\n\ncd deploy\nkubectl apply -f kubesphere-installer.yaml\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;执行完上面的命令，可以通过下面的命令，查看安装过程日志&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l job-name=kubesphere-installer -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -f\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;查看安装结果，STATUS跟下面保持一致才说明安装成功&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;[root@ks-allinone deploy]# kubectl get pods -n kubesphere-system\nNAME                                     READY   STATUS      RESTARTS   AGE\nks-account-6db466d8dc-srrwj              1/1     Running     0          149m\nks-apigateway-7d77cb9495-jzmg6           1/1     Running     0          170m\nks-apiserver-f8469fd47-b58rm             1/1     Running     0          166m\nks-console-54c849bdc9-dfkbf              1/1     Running     0          168m\nks-console-54c849bdc9-z2d5q              1/1     Running     0          168m\nks-controller-manager-569456b4cd-gngm5   1/1     Running     0          170m\nks-docs-6bbdcc9bfb-6jldz                 1/1     Running     0          3h7m\nkubesphere-installer-7ph6l               0/1     Completed   1          3h11m\nopenldap-5c986c5bff-rzqwv                1/1     Running     0          3h25m\nredis-76dc4db5dd-lv6kg                   1/1     Running     0          149m\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h3 id=\&#34;安装过程出现的错误\&#34;&gt;安装过程出现的错误&lt;/h3&gt;\n&lt;p&gt;1.在安装的时提示metrics-server已经安装，导致安装中断；&lt;/p&gt;\n&lt;p&gt;解析办法：在kubesphere-installer.yaml的configMap增加配置：metrics_server_enable: False（默认是没有的）&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: v1\ndata:\n  ks-config.yaml: |\n    kube_apiserver_host: 10.10.5.208:6443\n    etcd_tls_enable: True\n    etcd_endpoint_ips: 10.10.5.169,10.10.5.183,10.10.5.184\n    disableMultiLogin: True\n    elk_prefix: logstash\n    metrics_server_enable: False\n  #  local_registry: 192.168.1.2:5000\nkind: ConfigMap\nmetadata:\n  name: kubesphere-config\n  namespace: kubesphere-system\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;增加Ingress配置：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: kubesphere\n  namespace: kubesphere-system\n  annotations:\n    #kubernetes.io/ingress.class: traefik\n    kubernetes.io/ingress.class: nginx\nspec:\n  rules:\n  - host: ks.staplescn.com\n    http:\n      paths:\n      - path:\n        backend:\n          serviceName: ks-console\n          servicePort: 80\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;访问界面：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588153130667.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;kubesphere-an-zhuang-shi-yong-ti-yan&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;kubesphere安装使用体验&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;kubesphere&#34;,&#34;slug&#34;:&#34;BU7sbQs51&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/BU7sbQs51/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-04-29 17:33:05&#34;,&#34;dateFormat&#34;:&#34;2020-04-29&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/kubesphere-an-zhuang-shi-yong-ti-yan/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;3 min read&#34;,&#34;time&#34;:176000,&#34;words&#34;:576,&#34;minutes&#34;:3},&#34;description&#34;:&#34;最近又出来个kubesphere的工具用来管理k8s，今天特意来安装体验下；\ngithub地址：https://github.com/pixiake/ks-installer\n官方使用文档：https://kubesphere.io/doc...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BD%93%E5%89%8D%E7%8E%AF%E5%A2%83\&#34;&gt;当前环境：&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BC%80%E5%A7%8B%E5%AE%89%E8%A3%85\&#34;&gt;开始安装&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E5%87%BA%E7%8E%B0%E7%9A%84%E9%94%99%E8%AF%AF\&#34;&gt;安装过程出现的错误&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;前言\&#34;&gt;前言&lt;/h1&gt;\n&lt;p&gt;我们通常的开发流程是，在本地开发完成应用之后，使用git作为版本管理工具，将本地代码提交到类似Github这样的仓库中做持久化存储，当我们可能来自多个仓库、可能涉及到多个中间件作为底层依赖一起部署到生产环境中时，相信不少在大中型企业工作的小伙伴都知道公司内部通常会有发布系统，那么云原生技术栈中有没有为我们提供类似的发布系统呢？档案是肯定的，而且不乏竞争者，业内知名的有knavtie Build/jekinsX/spinnaker/orgo/tekton，其中，tekon凭借其众多优良特性在一众竞争者中胜出，成为领域内的事实标准，今天我们就来揭开Tekton的神秘面纱。&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588236885419.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h1 id=\&#34;正文\&#34;&gt;正文&lt;/h1&gt;\n&lt;h3 id=\&#34;什么是tekton\&#34;&gt;什么是Tekton&lt;/h3&gt;\n&lt;p&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588236902661.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n那Tekton都提供了哪些CRD呢？&lt;br&gt;\n•\tTask：顾名思义，task表示一个构建任务，task里可以定义一系列的steps，例如编译代码、构建镜像、推送镜像等，每个step实际由一个Pod执行。&lt;br&gt;\n•\tTaskRun：task只是定义了一个模版，taskRun才真正代表了一次实际的运行，当然你也可以自己手动创建一个taskRun，taskRun创建出来之后，就会自动触发task描述的构建任务。&lt;br&gt;\n•\tPipeline：一个或多个task、PipelineResource以及各种定义参数的集合。&lt;br&gt;\n•\tPipelineRun：类似task和taskRun的关系，pipelineRun也表示某一次实际运行的pipeline，下发一个pipelineRun CRD实例到kubernetes后，同样也会触发一次pipeline的构建。&lt;br&gt;\n•\tPipelineResource：表示pipeline input资源，比如github上的源码，或者pipeline output资源，例如一个容器镜像或者构建生成的jar包等。&lt;br&gt;\n他们大概有如下图所示的关系：&lt;br&gt;\n官方介绍：&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;Tekton 是一个功能强大且灵活的 Kubernetes 原生开源框架，用于创建持续集成和交付（CI/CD）系统。通过抽象底层实现细节，用户可以跨多云平台和本地系统进行构建、测试和部署。\n&lt;/code&gt;&lt;/pre&gt;\n&lt;pre&gt;&lt;code&gt;个人理解：\n•\t以yaml文件编排应用构建及部署流程\n•\tknavtive build模块升级版，社区最终采用 Tekton 替代 knavtive Build作为云原生领域的CI/CD 解决方案\n•\t标准化CI/CD流水线构建、测试及部署流程的工具\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;Tekton在一众竞争对手的比拼中PK胜出：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588236947460.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;下面就让我们一起来深入详细了解下Tekton到底怎么玩。&lt;br&gt;\nTekton Pipeline中有5类对象，核心理念是通过定义yaml定义构建过程。&lt;br&gt;\n•\tTask：一个任务的执行模板，用于描述单个任务的构建过程&lt;br&gt;\n•\tTaskRun：需要通过定义TaskRun任务去运行Task。&lt;br&gt;\n•\tPipeline：包含多个Task,并在此基础上定义input和output,input和output以PipelineResource作为交付。&lt;br&gt;\n•\tPipelineRun：需要定义PipelineRun才会运行Pipeline。&lt;br&gt;\n•\tPipelineResource：可用于input和output的对象集合。&lt;/p&gt;\n&lt;h3 id=\&#34;task\&#34;&gt;Task&lt;/h3&gt;\n&lt;p&gt;Task 就是一个任务执行模板，之所以说 Task 是一个模板是因为 Task 定义中可以包含变量，Task 在真正执行的时候需要给定变量的具体值。如果把 Tekton 的 Task 有点儿类似于定义一个函数，Task 通过 inputs.params 定义需要哪些入参，并且每一个入参还可以指定默认值。Task 的 steps 字段表示当前 Task 是有哪些步骤组成的，每一个步骤具体就是基于镜像启动一个 container 执行一些操作，container 的启动参数可以通过 Task 的入参使用模板语法进行配置。下面是一个Demo：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588236983683.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;taskrun\&#34;&gt;TaskRun&lt;/h3&gt;\n&lt;p&gt;Task 定义好以后是不能执行的，就像一个函数定义好以后需要调用才能执行一样。所以需要再定义一个 TaskRun 去执行 Task。&lt;br&gt;\nTaskRun 主要是负责设置 Task 需要的参数，并通过 taskRef 字段引用要执行的 Task。下面是一个Demo：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237005091.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;但是在实际使用过程中，我们一般很少使用TaskRun，因为它只能给不一个Task 传参，Tekton提供了给多个Task同时传参的解决方案Pipeline和PipelineRun，且看下文详解，这里只是多嘴一下，这个TaskRun很少使用，稍微了解下就可以了。&lt;/p&gt;\n&lt;h3 id=\&#34;pipeline\&#34;&gt;Pipeline&lt;/h3&gt;\n&lt;p&gt;一个 TaskRun 只能执行一个 Task，当需要编排多个 Task 的时候就需要 Pipeline 出马了。Pipeline 是一个编排 Task 的模板。Pipeline 的 params 声明了执行时需要的入参。 Pipeline 的 spec.tasks 定义了需要编排的 Task。Tasks 是一个数组，数组中的 task 并不是通过数组声明的顺序去执行的，而是通过 runAfter 来声明 task 执行的顺序。Tekton controller 在解析 CRD 的时候会解析 Task 的顺序，然后根据 runAfter 设置生成的依次树依次去执行。Pipeline 在编排 Task 的时候需要给每一个 Task 传入必须的参数，这些参数的值可以来自 Pipeline 自身的 params 设置。下面是一个Demo：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237033509.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237113343.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;pipelinerun\&#34;&gt;PipelineRun&lt;/h3&gt;\n&lt;p&gt;和 Task 一样 Pipeline 定义完成以后也是不能直接执行的，需要 PipelineRun 才能执行 Pipeline。PipelineRun 的主要作用是给 Pipeline 传入必要的入参，并执行 Pipeline。下面是一个Demo：&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237145814.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237151158.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h3 id=\&#34;pipelineresource\&#34;&gt;PipelineResource&lt;/h3&gt;\n&lt;p&gt;可能你还想在 Task 之间共享资源，这就是 PipelineResource 的作用。比如我们可以把 git 仓库信息放在 PipelineResource 中。这样所有 Task 就可以共享这些信息了。&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237189249.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237194640.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588237221271.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h1 id=\&#34;实战\&#34;&gt;实战&lt;/h1&gt;\n&lt;p&gt;关于Tekton的实战，可以参考Github里面的这个完整的Demo，里面是一个go语言吧编写的web服务，接口可以打印&amp;quot;Hello world&amp;quot;。时间有限，就不做演示了，感兴趣的可以在自己的k8s集群上面跑一下感受一下，相关的yaml文件也可以拷贝下来，作为后面改写的模板。&lt;br&gt;\n准备 PIpeline 的资源&lt;br&gt;\nkubectl apply -f tasks/source-to-image.yaml -f tasks/deploy-using-kubectl.yaml  -f resources/picalc-git.yaml -f image-secret.yaml -f pipeline-account.yaml -f pipeline/build-and-deploy-pipeline.yaml&lt;br&gt;\n执行 create 把 pipelieRun 提交到 Kubernetes 集群。之所以这里使用 create 而不是使用 apply 是因为 PIpelineRun 每次都会创建一个新的，kubectl 的 create 指令会基于 generateName 创建新的 PIpelineRun 资源。&lt;br&gt;\nkubectl create -f run/picalc-pipeline-run.yaml&lt;/p&gt;\n&lt;h1 id=\&#34;总结\&#34;&gt;总结&lt;/h1&gt;\n&lt;p&gt;Tekton以K8S为依托，成为云原生领域CI/CD的事实性标准，帮助我们提高云原生环境下的应用构建和部署效率。&lt;br&gt;\n来一张图对全文做一个简单的总结：&lt;/p&gt;\n&lt;figure data-type=\&#34;image\&#34; tabindex=\&#34;1\&#34;&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588236724398.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/figure&gt;\n&lt;h1 id=\&#34;参考资料\&#34;&gt;参考资料&lt;/h1&gt;\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\&#34;https://cloud.google.com/tekton/\&#34;&gt;Tekton官网&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;https://github.com/knative-sample/tekton-knative/tree/b1.0?spm=ata.13261165.0.0.21213a182xyMm5&amp;amp;file=b1.0\&#34;&gt;Tekton Demo&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&#34;,&#34;fileName&#34;:&#34;kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;kubernetes Tekton-CI/CD 持续集成流水线&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;tekton&#34;,&#34;slug&#34;:&#34;nN9-7w-_h&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/nN9-7w-_h/&#34;},{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;}],&#34;date&#34;:&#34;2020-01-20 16:48:40&#34;,&#34;dateFormat&#34;:&#34;2020-01-20&#34;,&#34;feature&#34;:&#34;http://lvelvis.github.io/post-images/kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian.png&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/kubernetes-tekton-cicd-chi-xu-ji-cheng-liu-shui-xian/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;7 min read&#34;,&#34;time&#34;:363000,&#34;words&#34;:1633,&#34;minutes&#34;:7},&#34;description&#34;:&#34;前言\n我们通常的开发流程是，在本地开发完成应用之后，使用git作为版本管理工具，将本地代码提交到类似Github这样的仓库中做持久化存储，当我们可能来自多个仓库、可能涉及到多个中间件作为底层依赖一起部署到生产环境中时，相信不少在大中型企业工...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%89%8D%E8%A8%80\&#34;&gt;前言&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%AD%A3%E6%96%87\&#34;&gt;正文&lt;/a&gt;&lt;br&gt;\n*\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E4%BB%80%E4%B9%88%E6%98%AFtekton\&#34;&gt;什么是Tekton&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#task\&#34;&gt;Task&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#taskrun\&#34;&gt;TaskRun&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#pipeline\&#34;&gt;Pipeline&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#pipelinerun\&#34;&gt;PipelineRun&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#pipelineresource\&#34;&gt;PipelineResource&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%AE%9E%E6%88%98\&#34;&gt;实战&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%80%BB%E7%BB%93\&#34;&gt;总结&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99\&#34;&gt;参考资料&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;h1 id=\&#34;简介\&#34;&gt;简介&lt;/h1&gt;\n&lt;p&gt;Rook官网：https://rook.io&lt;br&gt;\nRook是云原生计算基金会(CNCF)的孵化级项目.&lt;br&gt;\nRook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。&lt;br&gt;\n至于CEPH，官网在这：https://ceph.com/&lt;br&gt;\nceph官方提供的helm部署，至今我没成功过，所以转向使用rook提供的方案&lt;br&gt;\n有道笔记原文：http://note.youdao.com/noteshare?id=281719f1f0374f787effc90067e0d5ad&amp;amp;sub=0B59EA339D4A4769B55F008D72C1A4C0&lt;/p&gt;\n&lt;h1 id=\&#34;环境\&#34;&gt;环境&lt;/h1&gt;\n&lt;pre&gt;&lt;code&gt;centos 7.5\nkernel 4.18.7-1.el7.elrepo.x86_64\ndocker 18.06\nkubernetes v1.12.2\n    kubeadm部署：\n        网络: canal\n        DNS: coredns\n    集群成员：    \n    192.168.1.1 kube-master\n    192.168.1.2 kube-node1\n    192.168.1.3 kube-node2\n    192.168.1.4 kube-node3\n    192.168.1.5 kube-node4\n\n所有node节点准备一块200G的磁盘：/dev/sdb\nkubernetes搭建rook-ceph\n&lt;/code&gt;&lt;/pre&gt;\n&lt;figure data-type=\&#34;image\&#34; tabindex=\&#34;1\&#34;&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233297037.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/figure&gt;\n&lt;h1 id=\&#34;准备工作\&#34;&gt;准备工作&lt;/h1&gt;\n&lt;p&gt;所有节点开启ip_forward&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;  /etc/sysctl.d/ceph.conf\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;sysctl -p&lt;/p&gt;\n&lt;h1 id=\&#34;开始部署operator\&#34;&gt;开始部署Operator&lt;/h1&gt;\n&lt;h2 id=\&#34;部署rook-operator\&#34;&gt;部署Rook Operator&lt;/h2&gt;\n&lt;pre&gt;&lt;code&gt;cd $HOME\ngit clone https://github.com/rook/rook.git\n\ncd rook\ncd cluster/examples/kubernetes/ceph\nkubectl apply -f operator.yaml\n&lt;/code&gt;&lt;/pre&gt;\n&lt;figure data-type=\&#34;image\&#34; tabindex=\&#34;2\&#34;&gt;&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233371696.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/figure&gt;\n&lt;h2 id=\&#34;查看operator的状态\&#34;&gt;查看Operator的状态&lt;/h2&gt;\n&lt;p&gt;执行apply之后稍等一会&lt;br&gt;\noperator会在集群内的每个主机创建两个pod:rook-discover,rook-ceph-agent&lt;/p&gt;\n&lt;p&gt;kubectl -n rook-ceph-system get pod -o wide&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233523024.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h2 id=\&#34;给节点打标签\&#34;&gt;给节点打标签&lt;/h2&gt;\n&lt;p&gt;运行ceph-mon的节点打上：ceph-mon=enabled&lt;br&gt;\nkubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-mon=enabled&lt;br&gt;\n运行ceph-osd的节点，也就是存储节点，打上：ceph-osd=enabled&lt;br&gt;\nkubectl label nodes {kube-node1,kube-node2,kube-node3} ceph-osd=enabled&lt;br&gt;\n运行ceph-mgr的节点，打上：ceph-mgr=enabled&lt;br&gt;\nmgr只能支持一个节点运行，这是ceph跑k8s里的局限&lt;br&gt;\nkubectl label nodes kube-node1 ceph-mgr=enabled&lt;/p&gt;\n&lt;h2 id=\&#34;配置clusteryaml文件\&#34;&gt;配置cluster.yaml文件&lt;/h2&gt;\n&lt;p&gt;官方配置文件详解：https://rook.io/docs/rook/v0.8/ceph-cluster-crd.html&lt;br&gt;\n文件中有几个地方要注意：&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;dataDirHostPath: 这个路径是会在宿主机上生成的，保存的是ceph的相关的配置文件，再重新生成*集群的时候要确保这个目录为空，否则mon会无法启动&lt;/li&gt;\n&lt;li&gt;useAllDevices: 使用所有的设备，建议为false，否则会把宿主机所有可用的磁盘都干掉&lt;/li&gt;\n&lt;li&gt;useAllNodes：使用所有的node节点，建议为false，肯定不会用k8s集群内的所有node来搭建ceph的&lt;/li&gt;\n&lt;li&gt;databaseSizeMB和journalSizeMB：当磁盘大于100G的时候，就注释这俩项就行了&lt;br&gt;\n本次实验用到的 cluster.yaml 文件内容如下：&lt;/li&gt;\n&lt;/ul&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: v1\nkind: Namespace\nmetadata:\n  name: rook-ceph\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: rook-ceph-cluster\n  namespace: rook-ceph\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: rook-ceph-cluster\n  namespace: rook-ceph\nrules:\n- apiGroups: [&amp;quot;&amp;quot;]\n  resources: [&amp;quot;configmaps&amp;quot;]\n  verbs: [ &amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;delete&amp;quot; ]\n---\n# Allow the operator to create resources in this cluster&#39;s namespace\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: rook-ceph-cluster-mgmt\n  namespace: rook-ceph\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: rook-ceph-cluster-mgmt\nsubjects:\n- kind: ServiceAccount\n  name: rook-ceph-system\n  namespace: rook-ceph-system\n---\n# Allow the pods in this namespace to work with configmaps\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: rook-ceph-cluster\n  namespace: rook-ceph\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: rook-ceph-cluster\nsubjects:\n- kind: ServiceAccount\n  name: rook-ceph-cluster\n  namespace: rook-ceph\n---\napiVersion: ceph.rook.io/v1beta1\nkind: Cluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).\n    # v12 is luminous, v13 is mimic, and v14 is nautilus.\n    # RECOMMENDATION: In production, use a specific version tag instead of the general v13 flag, which pulls the latest release and could result in different\n    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.\n    image: ceph/ceph:v13\n    # Whether to allow unsupported versions of Ceph. Currently only luminous and mimic are supported.\n    # After nautilus is released, Rook will be updated to support nautilus.\n    # Do not set to true in production.\n    allowUnsupported: false\n  # The path on the host where configuration files will be persisted. If not specified, a kubernetes emptyDir will be created (not recommended).\n  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.\n  # In Minikube, the &#39;/data&#39; directory is configured to persist across reboots. Use &amp;quot;/data/rook&amp;quot; in Minikube environment.\n  dataDirHostPath: /var/lib/rook\n  # The service account under which to run the daemon pods in this cluster if the default account is not sufficient (OSDs)\n  serviceAccount: rook-ceph-cluster\n  # set the amount of mons to be started\n  # count可以定义ceph-mon运行的数量，这里默认三个就行了\n  mon:\n    count: 3\n    allowMultiplePerNode: true\n  # enable the ceph dashboard for viewing cluster status\n  # 开启ceph资源面板\n  dashboard:\n    enabled: true\n    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)\n    # urlPrefix: /ceph-dashboard\n  network:\n    # toggle to use hostNetwork\n    # 使用宿主机的网络进行通讯\n    # 使用宿主机的网络貌似可以让集群外的主机挂载ceph\n    # 但是我没试过，有兴趣的兄弟可以试试改成true\n    # 反正这里只是集群内用，我就不改了\n    hostNetwork: false\n  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.\n  # The example under &#39;all&#39; would have all services scheduled on kubernetes nodes labeled with &#39;role=storage-node&#39; and\n  # tolerate taints with a key of &#39;storage-node&#39;.\n  placement:\n#    all:\n#      nodeAffinity:\n#        requiredDuringSchedulingIgnoredDuringExecution:\n#          nodeSelectorTerms:\n#          - matchExpressions:\n#            - key: role\n#              operator: In\n#              values:\n#              - storage-node\n#      podAffinity:\n#      podAntiAffinity:\n#      tolerations:\n#      - key: storage-node\n#        operator: Exists\n# The above placement information can also be specified for mon, osd, and mgr components\n#    mon:\n#    osd:\n#    mgr:\n# nodeAffinity：通过选择标签的方式，可以限制pod被调度到特定的节点上\n# 建议限制一下，为了让这几个pod不乱跑\n    mon:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: ceph-mon\n              operator: In\n              values:\n              - enabled\n    osd:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: ceph-osd\n              operator: In\n              values:\n              - enabled\n    mgr:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n          - matchExpressions:\n            - key: ceph-mgr\n              operator: In\n              values:\n              - enabled\n  resources:\n# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory\n#    mgr:\n#      limits:\n#        cpu: &amp;quot;500m&amp;quot;\n#        memory: &amp;quot;1024Mi&amp;quot;\n#      requests:\n#        cpu: &amp;quot;500m&amp;quot;\n#        memory: &amp;quot;1024Mi&amp;quot;\n# The above example requests/limits can also be added to the mon and osd components\n#    mon:\n#    osd:\n  storage: # cluster level storage configuration and selection\n    useAllNodes: false\n    useAllDevices: false\n    deviceFilter:\n    location:\n    config:\n      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.\n      # Set the storeType explicitly only if it is required not to use the default.\n      # storeType: bluestore\n      # databaseSizeMB: &amp;quot;1024&amp;quot; # this value can be removed for environments with normal sized disks (100 GB or larger)\n      # journalSizeMB: &amp;quot;1024&amp;quot;  # this value can be removed for environments with normal sized disks (20 GB or larger)\n# Cluster level list of directories to use for storage. These values will be set for all nodes that have no `directories` set.\n#    directories:\n#    - path: /rook/storage-dir\n# Individual nodes and their config can be specified as well, but &#39;useAllNodes&#39; above must be set to false. Then, only the named\n# nodes below will be used as storage resources.  Each node&#39;s &#39;name&#39; field should match their &#39;kubernetes.io/hostname&#39; label.\n#建议磁盘配置方式如下：\n#name: 选择一个节点，节点名字为kubernetes.io/hostname的标签，也就是kubectl get nodes看到的名字\n#devices: 选择磁盘设置为OSD\n# - name: &amp;quot;sdb&amp;quot;:将/dev/sdb设置为osd\n    nodes:\n    - name: &amp;quot;kube-node1&amp;quot;\n      devices:\n      - name: &amp;quot;sdb&amp;quot;\n    - name: &amp;quot;kube-node2&amp;quot;\n      devices:\n      - name: &amp;quot;sdb&amp;quot;\n    - name: &amp;quot;kube-node3&amp;quot;\n      devices:\n      - name: &amp;quot;sdb&amp;quot;\n\n#      directories: # specific directories to use for storage can be specified for each node\n#      - path: &amp;quot;/rook/storage-dir&amp;quot;\n#      resources:\n#        limits:\n#          cpu: &amp;quot;500m&amp;quot;\n#          memory: &amp;quot;1024Mi&amp;quot;\n#        requests:\n#          cpu: &amp;quot;500m&amp;quot;\n#          memory: &amp;quot;1024Mi&amp;quot;\n#    - name: &amp;quot;172.17.4.201&amp;quot;\n#      devices: # specific devices to use for storage can be specified for each node\n#      - name: &amp;quot;sdb&amp;quot;\n#      - name: &amp;quot;sdc&amp;quot;\n#      config: # configuration can be specified at the node level which overrides the cluster level config\n#        storeType: filestore\n#    - name: &amp;quot;172.17.4.301&amp;quot;\n#      deviceFilter: &amp;quot;^sd.&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;开始部署ceph\&#34;&gt;开始部署ceph&lt;/h2&gt;\n&lt;p&gt;部署ceph&lt;br&gt;\nkubectl apply -f cluster.yaml&lt;br&gt;\ncluster会在rook-ceph这个namesapce创建资源&lt;br&gt;\n看到所有的pod都Running就行了&lt;br&gt;\n注意看一下pod分布的宿主机，跟我们打标签的主机是一致的&lt;br&gt;\nkubectl -n rook-ceph get pod -o wide&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233756856.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;切换到其他主机看一下磁盘&lt;/p&gt;\n&lt;p&gt;切换到kube-node1&lt;br&gt;\nlsblk&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233803458.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h2 id=\&#34;配置ceph-dashboard\&#34;&gt;配置ceph dashboard&lt;/h2&gt;\n&lt;p&gt;看一眼dashboard在哪个service上&lt;br&gt;\nkubectl -n rook-ceph get service&lt;br&gt;\n可以看到dashboard监听了8443端口&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233853544.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;创建个nodeport类型的service以便集群外部访问&lt;br&gt;\nkubectl apply -f dashboard-external-https.yaml&lt;/p&gt;\n&lt;p&gt;查看一下nodeport在哪个端口&lt;br&gt;\nkubectl -n rook-ceph get service&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233909644.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;找出Dashboard的登陆账号和密码&lt;br&gt;\nMGR_POD=&lt;code&gt;kubectl get pod -n rook-ceph | grep mgr | awk &#39;{print $1}&#39;&lt;/code&gt;&lt;br&gt;\nkubectl -n rook-ceph logs $MGR_POD | grep password&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588233955731.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;打开浏览器输入任意一个Node的IP+nodeport端口&lt;br&gt;\n这里我的就是：https://192.168.1.2:30290&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234024005.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234065656.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h2 id=\&#34;配置ceph为storageclass\&#34;&gt;配置ceph为storageclass&lt;/h2&gt;\n&lt;p&gt;官方给了一个样本文件：storageclass.yaml&lt;br&gt;\n这个文件使用的是 RBD 块存储&lt;br&gt;\npool创建详解：https://rook.io/docs/rook/v0.8/ceph-pool-crd.html&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;apiVersion: ceph.rook.io/v1beta1\nkind: Pool\nmetadata:\n  #这个name就是创建成ceph pool之后的pool名字\n  name: replicapool\n  namespace: rook-ceph\nspec:\n  replicated:\n    size: 1\n  # size 池中数据的副本数,1就是不保存任何副本\n  failureDomain: osd\n  #  failureDomain：数据块的故障域，\n  #  值为host时，每个数据块将放置在不同的主机上\n  #  值为osd时，每个数据块将放置在不同的osd上\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n   name: ceph\n   # StorageClass的名字，pvc调用时填的名字\nprovisioner: ceph.rook.io/block\nparameters:\n  pool: replicapool\n  # Specify the namespace of the rook cluster from which to create volumes.\n  # If not specified, it will use `rook` as the default namespace of the cluster.\n  # This is also the namespace where the cluster will be\n  clusterNamespace: rook-ceph\n  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.\n  fstype: xfs\n# 设置回收策略默认为：Retain\nreclaimPolicy: Retain\n&lt;/code&gt;&lt;/pre&gt;\n&lt;h2 id=\&#34;创建storageclass\&#34;&gt;创建StorageClass&lt;/h2&gt;\n&lt;p&gt;kubectl apply -f storageclass.yaml&lt;br&gt;\nkubectl get storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;\nkubectl describe storageclasses.storage.k8s.io  -n rook-ceph&lt;br&gt;\nkubernetes搭建rook-ceph&lt;/p&gt;\n&lt;p&gt;创建个nginx pod尝试挂载&lt;/p&gt;\n&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; nginx.yaml\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nginx-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: ceph\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  selector:\n    app: nginx\n  ports: \n  - port: 80\n    name: nginx-port\n    targetPort: 80\n    protocol: TCP\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      name: nginx\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /html\n          name: http-file\n      volumes:\n      - name: http-file\n        persistentVolumeClaim:\n          claimName: nginx-pvc\nEOF\n&lt;/code&gt;&lt;/pre&gt;\n&lt;p&gt;kubectl apply -f nginx.yaml&lt;br&gt;\n查看pv,pvc是否创建了&lt;br&gt;\nkubectl get pv,pvc&lt;/p&gt;\n&lt;p&gt;看一下nginx这个pod也运行了&lt;br&gt;\nkubectl get pod&lt;/p&gt;\n&lt;p&gt;删除这个pod,看pv是否还存在&lt;br&gt;\nkubectl delete -f nginx.yaml&lt;/p&gt;\n&lt;p&gt;kubectl get pv,pvc&lt;br&gt;\n可以看到，pod和pvc都已经被删除了，但是pv还在！！！&lt;/p&gt;\n&lt;h2 id=\&#34;添加新节点进入集群\&#34;&gt;添加新节点进入集群&lt;/h2&gt;\n&lt;p&gt;这次我们要把node4添加进集群，先打标签&lt;br&gt;\nkubectl label nodes kube-node4 ceph-osd=enabled&lt;br&gt;\n重新编辑cluster.yaml文件&lt;br&gt;\n原来的基础上添加node4的信息&lt;/p&gt;\n&lt;p&gt;cd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;\nvi cluster.yam&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234207475.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;\nkubectl apply -f cluster.yaml&lt;/p&gt;\n&lt;p&gt;盯着rook-ceph名称空间,集群会自动添加node4进来&lt;br&gt;\nkubectl -n rook-ceph get pod -o wide -w&lt;br&gt;\nkubectl -n rook-ceph get pod -o wide&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234283568.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234307875.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n去node4节点看一下磁盘&lt;br&gt;\nlsblk&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234334204.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;h2 id=\&#34;删除一个节点\&#34;&gt;删除一个节点&lt;/h2&gt;\n&lt;p&gt;去掉node3的标签&lt;br&gt;\nkubectl label nodes kube-node3 ceph-osd-&lt;br&gt;\n重新编辑cluster.yaml文件&lt;br&gt;\n删除node3的信息&lt;br&gt;\ncd $HOME/rook/cluster/examples/kubernetes/ceph/&lt;br&gt;\nvi cluster.yam&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234380826.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;apply一下cluster.yaml文件&lt;br&gt;\nkubectl apply -f cluster.yaml&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234493936.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;/p&gt;\n&lt;p&gt;kubectl -n rook-ceph get pod -o wide&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234548709.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n&lt;img src=\&#34;http://lvelvis.github.io/post-images/1588234579591.png\&#34; alt=\&#34;\&#34; loading=\&#34;lazy\&#34;&gt;&lt;br&gt;\n最后记得删除宿主机的/var/lib/rook文件夹&lt;/p&gt;\n&#34;,&#34;fileName&#34;:&#34;kubernetes搭建rook-ceph&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;kubernetes搭建rook-ceph&#34;,&#34;tags&#34;:[{&#34;index&#34;:-1,&#34;name&#34;:&#34;k8s&#34;,&#34;slug&#34;:&#34;Q617Y3Kh2&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/Q617Y3Kh2/&#34;},{&#34;name&#34;:&#34;rook-ceph&#34;,&#34;slug&#34;:&#34;hu0mNDfuy&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/hu0mNDfuy/&#34;},{&#34;name&#34;:&#34;ceph&#34;,&#34;slug&#34;:&#34;MkN4-Vurh-&#34;,&#34;used&#34;:true,&#34;link&#34;:&#34;http://lvelvis.github.io/tag/MkN4-Vurh-/&#34;}],&#34;date&#34;:&#34;2019-12-12 00:00:00&#34;,&#34;dateFormat&#34;:&#34;2019-12-12&#34;,&#34;feature&#34;:&#34;http://lvelvis.github.io/post-images/kubernetes搭建rook-ceph.png&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/kubernetes搭建rook-ceph/&#34;,&#34;hideInList&#34;:false,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;13 min read&#34;,&#34;time&#34;:779000,&#34;words&#34;:2524,&#34;minutes&#34;:13},&#34;description&#34;:&#34;简介\nRook官网：https://rook.io\nRook是云原生计算基金会(CNCF)的孵化级项目.\nRook是Kubernetes的开源云本地存储协调器，为各种存储解决方案提供平台，框架和支持，以便与云原生环境本地集成。\n至于CEPH...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%AE%80%E4%BB%8B\&#34;&gt;简介&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%8E%AF%E5%A2%83\&#34;&gt;环境&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C\&#34;&gt;准备工作&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2operator\&#34;&gt;开始部署Operator&lt;/a&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%83%A8%E7%BD%B2rook-operator\&#34;&gt;部署Rook Operator&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%9F%A5%E7%9C%8Boperator%E7%9A%84%E7%8A%B6%E6%80%81\&#34;&gt;查看Operator的状态&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E7%BB%99%E8%8A%82%E7%82%B9%E6%89%93%E6%A0%87%E7%AD%BE\&#34;&gt;给节点打标签&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%85%8D%E7%BD%AEclusteryaml%E6%96%87%E4%BB%B6\&#34;&gt;配置cluster.yaml文件&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2ceph\&#34;&gt;开始部署ceph&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%85%8D%E7%BD%AEceph-dashboard\&#34;&gt;配置ceph dashboard&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E9%85%8D%E7%BD%AEceph%E4%B8%BAstorageclass\&#34;&gt;配置ceph为storageclass&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%9B%E5%BB%BAstorageclass\&#34;&gt;创建StorageClass&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E6%B7%BB%E5%8A%A0%E6%96%B0%E8%8A%82%E7%82%B9%E8%BF%9B%E5%85%A5%E9%9B%86%E7%BE%A4\&#34;&gt;添加新节点进入集群&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%88%A0%E9%99%A4%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9\&#34;&gt;删除一个节点&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;},{&#34;content&#34;:&#34;&lt;blockquote&gt;\n&lt;p&gt;欢迎来到我的小站呀，很高兴遇见你！🤝&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;h2 id=\&#34;关于本站\&#34;&gt;🏠 关于本站&lt;/h2&gt;\n&lt;p&gt;记录平时运维相关的技术:&lt;br&gt;\n1.网络&lt;br&gt;\n2.系统&lt;br&gt;\n3.虚拟化&lt;br&gt;\n4.数据库&lt;br&gt;\n5.中间件&lt;br&gt;\n...&lt;/p&gt;\n&lt;h2 id=\&#34;博主是谁\&#34;&gt;👨‍💻 博主是谁&lt;/h2&gt;\n&lt;p&gt;目前在职于巨人网络&lt;/p&gt;\n&lt;h2 id=\&#34;兴趣爱好\&#34;&gt;⛹ 兴趣爱好&lt;/h2&gt;\n&lt;p&gt;番剧 FPS射击游戏 dota&lt;/p&gt;\n&lt;h2 id=\&#34;联系我呀\&#34;&gt;📬 联系我呀&lt;/h2&gt;\n&#34;,&#34;fileName&#34;:&#34;about&#34;,&#34;abstract&#34;:&#34;&#34;,&#34;title&#34;:&#34;关于&#34;,&#34;tags&#34;:[],&#34;date&#34;:&#34;2019-01-25 19:09:48&#34;,&#34;dateFormat&#34;:&#34;2019-01-25&#34;,&#34;feature&#34;:&#34;&#34;,&#34;link&#34;:&#34;http://lvelvis.github.io/post/about/&#34;,&#34;hideInList&#34;:true,&#34;isTop&#34;:false,&#34;stats&#34;:{&#34;text&#34;:&#34;1 min read&#34;,&#34;time&#34;:16000,&#34;words&#34;:77,&#34;minutes&#34;:1},&#34;description&#34;:&#34;\n欢迎来到我的小站呀，很高兴遇见你！🤝\n\n🏠 关于本站\n记录平时运维相关的技术:\n1.网络\n2.系统\n3.虚拟化\n4.数据库\n5.中间件\n...\n👨‍💻 博主是谁\n目前在职于巨人网络\n⛹ 兴趣爱好\n番剧 FPS射击游戏 dota\n\ud83d...&#34;,&#34;toc&#34;:&#34;&lt;ul class=\&#34;markdownIt-TOC\&#34;&gt;\n&lt;li&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%85%B3%E4%BA%8E%E6%9C%AC%E7%AB%99\&#34;&gt;🏠 关于本站&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%8D%9A%E4%B8%BB%E6%98%AF%E8%B0%81\&#34;&gt;👨‍💻 博主是谁&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E5%85%B4%E8%B6%A3%E7%88%B1%E5%A5%BD\&#34;&gt;⛹ 兴趣爱好&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\&#34;#%E8%81%94%E7%B3%BB%E6%88%91%E5%91%80\&#34;&gt;📬 联系我呀&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&#34;}]";
  // var json = escape.substr(1, escape.length - 2);
  // var datas = json.split(',');
  // for (let i=0; i < datas.length; i++) {
  //   let item = datas[i];
  //   let attrs = item.split('34;:&#34')
  //   debugger
  //   console.log(datas[i])
  // }
  let escapeMap = new Map();
  escapeMap.set('&#34;', '"');
  escapeMap.set('&gt;', '>');
  escapeMap.set('&#39;', "'");
  escapeMap.set('&lt;', '<');
  escapeMap.set('&quot;', '"');
  escapeMap.set('&amp;', '&');
</script> -->

<script src="/media/js/mouse/peace.js"></script>


<script src=" /media/js/cool.js"></script>


</html>